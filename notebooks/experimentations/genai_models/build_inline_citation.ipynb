{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4d184a-9a96-4607-86ed-c7d6249432cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:22:31.437857Z",
     "iopub.status.busy": "2025-06-20T02:22:31.437542Z",
     "iopub.status.idle": "2025-06-20T02:22:31.451243Z",
     "shell.execute_reply": "2025-06-20T02:22:31.449575Z",
     "shell.execute_reply.started": "2025-06-20T02:22:31.437831Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s -- l.%(lineno)d: %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa0064-5798-434a-a37c-3141d346448f",
   "metadata": {},
   "source": [
    "# Build summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "493057c8-4c31-49a0-a601-9a9e300fe9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:22:32.320086Z",
     "iopub.status.busy": "2025-06-20T02:22:32.319678Z",
     "iopub.status.idle": "2025-06-20T02:23:08.554868Z",
     "shell.execute_reply": "2025-06-20T02:23:08.553732Z",
     "shell.execute_reply.started": "2025-06-20T02:22:32.320020Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:22:34,444 - httpx - INFO -- l.1025: HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n",
      "2025-06-19 22:22:38,772 - googleapiclient.discovery_cache - INFO -- l.49: file_cache is only supported with oauth2client<4.0.0\n",
      "2025-06-19 22:22:38,777 - src.newsletters.main - INFO -- l.32: Query emails\n",
      "2025-06-19 22:22:40,172 - src.newsletters.main - INFO -- l.38: Using 6 sender(s): dict_keys(['TLDR AI <dan@tldrnewsletter.com>', 'AlphaSignal <news@alphasignal.ai>', 'TLDR <dan@tldrnewsletter.com>', 'TLDR Product <dan@tldrnewsletter.com>', 'Bloomberg Technology <noreply@news.bloomberg.com>', 'TechCrunch <newsletters@techcrunch.com>']).\n",
      "2025-06-19 22:22:40,174 - src.newsletters.main - INFO -- l.40: Found 1 emails from sender TLDR AI <dan@tldrnewsletter.com>\n",
      "2025-06-19 22:22:40,175 - src.newsletters.main - INFO -- l.40: Found 0 emails from sender AlphaSignal <news@alphasignal.ai>\n",
      "2025-06-19 22:22:40,176 - src.newsletters.main - INFO -- l.40: Found 1 emails from sender TLDR <dan@tldrnewsletter.com>\n",
      "2025-06-19 22:22:40,177 - src.newsletters.main - INFO -- l.40: Found 0 emails from sender TLDR Product <dan@tldrnewsletter.com>\n",
      "2025-06-19 22:22:40,179 - src.newsletters.main - INFO -- l.40: Found 0 emails from sender Bloomberg Technology <noreply@news.bloomberg.com>\n",
      "2025-06-19 22:22:40,180 - src.newsletters.main - INFO -- l.40: Found 0 emails from sender TechCrunch <newsletters@techcrunch.com>\n",
      "2025-06-19 22:22:40,181 - src.newsletters.main - INFO -- l.43: Parsing emails to extract news stories\n",
      "2025-06-19 22:22:40,182 - src.newsletters.parser.tldr - INFO -- l.26: Parsing email 'Elon Musk vs. OpenAI ‚öîÔ∏è, Perplexity hardware ü§ñ, INTELLECT-1 Release 1Ô∏è‚É£'\n",
      "2025-06-19 22:22:42,708 - src.newsletters.parser.tldr - INFO -- l.26: Parsing email 'Tesla's next-gen self driving üöó, US killed Meta crypto ü™ô, the rise of PIPs üíº'\n",
      "2025-06-19 22:22:46,291 - src.newsletters.main - INFO -- l.57: Newsletters block complete. Found 29 news stories in total.\n",
      "2025-06-19 22:22:46,293 - src.scoring.main - INFO -- l.24: Converting all news stories to a structured format.\n",
      "2025-06-19 22:22:47,089 - src.scoring.main - INFO -- l.28: Querying the metadata tags from Google Sheets.\n",
      "2025-06-19 22:22:47,092 - googleapiclient.discovery_cache - INFO -- l.49: file_cache is only supported with oauth2client<4.0.0\n",
      "2025-06-19 22:23:08,541 - src.reporting.report - INFO -- l.66: Will generate report for range 2024-12-01 to 2024-12-02\n",
      "2025-06-19 22:23:08,543 - src.reporting.report - INFO -- l.294: Target fields were successfully verified.\n",
      "2025-06-19 22:23:08,546 - src.reporting.report - INFO -- l.254: We'll use the news stories provided by the dataframe.\n",
      "2025-06-19 22:23:08,547 - src.reporting.report - INFO -- l.80: Found 29 candidate news stories for this report.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.newsletters.main import runner as runner_newsletters\n",
    "from src.scoring.main import runner as runner_scoring\n",
    "from src.reporting.report import Report\n",
    "\n",
    "report_date_range = ['2024-12-01', '2024-12-02']\n",
    "news_stories, parser_error = runner_newsletters(after=report_date_range[0], before=report_date_range[1])\n",
    "df_news_stories, target_fields = runner_scoring(news_stories)\n",
    "report = Report(\n",
    "    df_scored_news_stories=df_news_stories, \n",
    "    target_fields=target_fields, \n",
    "    report_date_range=report_date_range,\n",
    "    score_col=\"score_category_count\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b7a84b-400e-41a1-868f-2c6defb20d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T21:04:00.181039Z",
     "iopub.status.busy": "2025-06-19T21:04:00.180613Z",
     "iopub.status.idle": "2025-06-19T21:04:00.193882Z",
     "shell.execute_reply": "2025-06-19T21:04:00.192632Z",
     "shell.execute_reply.started": "2025-06-19T21:04:00.181014Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:04:00,186 - src.genai_model.genai_model - INFO -- l.106: Found 13 models for model_type large\n",
      "2025-06-19 17:04:00,188 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-exp-1206', 'openrouter/google/gemini-exp-1206:free', 'gemini/gemini-exp-1121', 'openrouter/google/gemini-exp-1121:free', 'gemini/gemini-exp-1114', 'openrouter/google/gemini-exp-1114:free', 'gemini/gemini-1.5-pro-002', 'gemini/gemini-1.5-pro-exp-0827', 'openrouter/meta-llama/llama-3.1-405b-instruct:free', 'gemini/gemini-1.5-pro-001', 'mistral/mistral-large-2411', 'mistral/mistral-large-2407', 'mistral/mistral-large-2402']\n"
     ]
    }
   ],
   "source": [
    "from src.reporting.report import Categories\n",
    "from src.genai_model.summarizer import SummarizerMarketIntel\n",
    "\n",
    "df_ns = report.df_ns\n",
    "all_tags = df_ns[Categories.MARKET.value].explode().dropna().unique()\n",
    "summarizer = SummarizerMarketIntel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3330d6f-71a3-4248-bf9c-11b5e6e370c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T21:04:00.195842Z",
     "iopub.status.busy": "2025-06-19T21:04:00.195515Z",
     "iopub.status.idle": "2025-06-19T21:04:00.213461Z",
     "shell.execute_reply": "2025-06-19T21:04:00.211288Z",
     "shell.execute_reply.started": "2025-06-19T21:04:00.195813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OpenAI', 'Twitter', 'Perplexity', 'Google', 'NVIDIA', 'Anthropic',\n",
       "       'Meta'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2d9ef3-88f9-4059-8ef4-fdeb9b33196e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T21:04:00.215616Z",
     "iopub.status.busy": "2025-06-19T21:04:00.215241Z",
     "iopub.status.idle": "2025-06-19T21:04:06.389616Z",
     "shell.execute_reply": "2025-06-19T21:04:06.388138Z",
     "shell.execute_reply.started": "2025-06-19T21:04:00.215579Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m17:04:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 17:04:00,243 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 17:04:00,497 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 17:04:00,516 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m17:04:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 17:04:00,519 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:04:00,946 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 17:04:00,963 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m17:04:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 17:04:00,964 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 17:04:01,143 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:04:01,158 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m17:04:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 17:04:01,159 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 17:04:01,550 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 17:04:01,566 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m17:04:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 17:04:01,568 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 17:04:01,742 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 17:04:01,754 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m17:04:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 17:04:01,755 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:04:02,146 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 17:04:02,167 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m17:04:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 17:04:02,169 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:04:02,422 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 17:04:02,439 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m17:04:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 17:04:02,440 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 17:04:02,600 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 17:04:02,615 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m17:04:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 17:04:02,617 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:04:03,012 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 17:04:03,028 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m17:04:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 17:04:03,030 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 17:04:03,204 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 17:04:03,221 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m17:04:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:04:03,222 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 17:04:06,364 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m17:04:06 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 17:04:06,382 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n"
     ]
    }
   ],
   "source": [
    "tag = all_tags[0]\n",
    "df_tag = report._get_df_from_tag(df=df_ns, tag=tag)\n",
    "tag_report = report._build_tag_report(df_in=df_tag, summarizer=summarizer)\n",
    "market_intel_report = f\"## {tag}\\n\\n{tag_report}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0cc5ef7-84c5-4c55-bed5-bc77e9f75d82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T21:04:28.969221Z",
     "iopub.status.busy": "2025-06-19T21:04:28.968767Z",
     "iopub.status.idle": "2025-06-19T21:04:28.975837Z",
     "shell.execute_reply": "2025-06-19T21:04:28.974454Z",
     "shell.execute_reply.started": "2025-06-19T21:04:28.969191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## OpenAI\n",
      "\n",
      "OpenAI faces legal challenges from Elon Musk over its for-profit conversion, citing antitrust violations and financial risks. The company also confronts industry structural challenges in building large language models (LLMs), including NVIDIA's dominance in chip supply and intense competition. Meanwhile, OpenAI recently terminated access to its Sora video generation platform for some testers due to a public leak. The AI sector, while advancing technologically, grapples with profitability issues, similar to historical tech firms. (summary provided by mistral/mistral-large-2411)\n",
      "\n",
      "Sources:\n",
      "\n",
      "1. [ELON MUSK SEEKS TO BLOCK OPENAI'S FOR-PROFIT CONVERSION AGAIN](https://www.theverge.com/2024/11/30/24309697/elon-musk-openai-lawsuit-for-profit-transition-preliminary-injunction)\n",
      "2. [BUILDING LLMS IS PROBABLY NOT GOING BE A BRILLIANT BUSINESS](https://calpaterson.com/porter.html)\n",
      "3. [OPENAI IS AT WAR WITH ITS OWN SORA VIDEO TESTERS FOLLOWING BRIEF\n",
      "PUBLIC LEAK](https://arstechnica.com/ai/2024/11/openai-is-at-war-with-its-own-sora-video-testers-following-brief-public-leak/)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(market_intel_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac93d9a-3d90-445b-ba93-8bef8eb54799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T21:05:27.134704Z",
     "iopub.status.busy": "2025-06-19T21:05:27.134121Z",
     "iopub.status.idle": "2025-06-19T21:05:32.876115Z",
     "shell.execute_reply": "2025-06-19T21:05:32.874844Z",
     "shell.execute_reply.started": "2025-06-19T21:05:27.134669Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m17:05:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 17:05:27,140 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 17:05:27,332 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 17:05:27,349 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m17:05:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 17:05:27,352 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:05:27,852 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 17:05:27,870 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m17:05:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 17:05:27,872 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 17:05:28,059 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:05:28,074 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m17:05:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 17:05:28,077 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 17:05:28,420 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 17:05:28,433 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m17:05:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 17:05:28,434 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 17:05:28,596 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 17:05:28,615 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m17:05:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 17:05:28,619 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:05:29,046 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 17:05:29,063 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m17:05:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 17:05:29,064 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 17:05:29,226 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 17:05:29,242 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m17:05:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 17:05:29,244 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:05:29,407 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 17:05:29,421 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m17:05:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 17:05:29,422 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:05:29,819 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 17:05:29,849 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m17:05:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 17:05:29,853 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:05:30,026 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 17:05:30,045 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m17:05:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 17:05:30,048 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:05:32,864 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m17:05:32 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 17:05:32,868 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n"
     ]
    }
   ],
   "source": [
    "all_citations, summary, df_cite = report._create_citations(df_in=df_tag, summarizer=summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "232767c3-5fe8-43a5-bb34-1841a334e4bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T21:05:33.736054Z",
     "iopub.status.busy": "2025-06-19T21:05:33.735622Z",
     "iopub.status.idle": "2025-06-19T21:05:33.748377Z",
     "shell.execute_reply": "2025-06-19T21:05:33.746341Z",
     "shell.execute_reply.started": "2025-06-19T21:05:33.736025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. [ELON MUSK SEEKS TO BLOCK OPENAI'S FOR-PROFIT CONVERSION AGAIN](https://www.theverge.com/2024/11/30/24309697/elon-musk-openai-lawsuit-for-profit-transition-preliminary-injunction)\\n2. [BUILDING LLMS IS PROBABLY NOT GOING BE A BRILLIANT BUSINESS](https://calpaterson.com/porter.html)\\n3. [OPENAI IS AT WAR WITH ITS OWN SORA VIDEO TESTERS FOLLOWING BRIEF\\r\\nPUBLIC LEAK](https://arstechnica.com/ai/2024/11/openai-is-at-war-with-its-own-sora-video-testers-following-brief-public-leak/)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b06a422-52a4-4852-9175-f23696708e99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T21:05:37.032082Z",
     "iopub.status.busy": "2025-06-19T21:05:37.031619Z",
     "iopub.status.idle": "2025-06-19T21:05:37.042118Z",
     "shell.execute_reply": "2025-06-19T21:05:37.040814Z",
     "shell.execute_reply.started": "2025-06-19T21:05:37.032051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OpenAI faces legal challenges from Elon Musk over its for-profit transition, citing antitrust concerns and financial risks. The company also confronts industry structural challenges, including NVIDIA's dominance in chip supply and intense competition, which may hinder profitability despite significant capital raised. Meanwhile, OpenAI recently terminated testing access to its Sora video generation platform following a public leak by a group of artist testers. The AI sector, while advancing technologically, may see more success in leveraging existing models than in developing new ones. (summary provided by mistral/mistral-large-2411)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8669d007-3ad9-4f58-b6a2-9eecb1d1383a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T21:06:13.726695Z",
     "iopub.status.busy": "2025-06-19T21:06:13.726272Z",
     "iopub.status.idle": "2025-06-19T21:06:13.757196Z",
     "shell.execute_reply": "2025-06-19T21:06:13.755857Z",
     "shell.execute_reply.started": "2025-06-19T21:06:13.726668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>news_provider</th>\n",
       "      <th>source_of_the_news</th>\n",
       "      <th>text</th>\n",
       "      <th>news_summary</th>\n",
       "      <th>date_source</th>\n",
       "      <th>date_source_time_zone</th>\n",
       "      <th>version</th>\n",
       "      <th>competitive_intelligence</th>\n",
       "      <th>themes</th>\n",
       "      <th>market_intelligence</th>\n",
       "      <th>personalities</th>\n",
       "      <th>score_category_count</th>\n",
       "      <th>nb_citation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ELON MUSK SEEKS TO BLOCK OPENAI'S FOR-PROFIT C...</td>\n",
       "      <td>https://www.theverge.com/2024/11/30/24309697/e...</td>\n",
       "      <td>www.theverge.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Elon Musk's legal team has filed a motion to p...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Legal]</td>\n",
       "      <td>[OpenAI, Twitter]</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BUILDING LLMS IS PROBABLY NOT GOING BE A BRILL...</td>\n",
       "      <td>https://calpaterson.com/porter.html</td>\n",
       "      <td>calpaterson.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>LLM makers like OpenAI face significant challe...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Model]</td>\n",
       "      <td>[OpenAI, NVIDIA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>OPENAI IS AT WAR WITH ITS OWN SORA VIDEO TESTE...</td>\n",
       "      <td>https://arstechnica.com/ai/2024/11/openai-is-a...</td>\n",
       "      <td>arstechnica.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>OpenAI cut off testing access to its Sora vide...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Multimodal]</td>\n",
       "      <td>[OpenAI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   ELON MUSK SEEKS TO BLOCK OPENAI'S FOR-PROFIT C...   \n",
       "10  BUILDING LLMS IS PROBABLY NOT GOING BE A BRILL...   \n",
       "23  OPENAI IS AT WAR WITH ITS OWN SORA VIDEO TESTE...   \n",
       "\n",
       "                                                  url     news_provider  \\\n",
       "0   https://www.theverge.com/2024/11/30/24309697/e...  www.theverge.com   \n",
       "10                https://calpaterson.com/porter.html   calpaterson.com   \n",
       "23  https://arstechnica.com/ai/2024/11/openai-is-a...   arstechnica.com   \n",
       "\n",
       "                  source_of_the_news text  \\\n",
       "0   TLDR AI <dan@tldrnewsletter.com>        \n",
       "10  TLDR AI <dan@tldrnewsletter.com>        \n",
       "23     TLDR <dan@tldrnewsletter.com>        \n",
       "\n",
       "                                         news_summary  \\\n",
       "0   Elon Musk's legal team has filed a motion to p...   \n",
       "10  LLM makers like OpenAI face significant challe...   \n",
       "23  OpenAI cut off testing access to its Sora vide...   \n",
       "\n",
       "                 date_source date_source_time_zone version  \\\n",
       "0  2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "10 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "23 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "\n",
       "   competitive_intelligence             themes market_intelligence  \\\n",
       "0                        []            [Legal]   [OpenAI, Twitter]   \n",
       "10                       []  [AI&GenAI, Model]    [OpenAI, NVIDIA]   \n",
       "23                       []       [Multimodal]            [OpenAI]   \n",
       "\n",
       "   personalities  score_category_count  nb_citation  \n",
       "0             []                     3            1  \n",
       "10            []                     4            2  \n",
       "23            []                     2            3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "308481ba-3301-44ee-b7b6-f9d1e0a89076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T22:26:02.716615Z",
     "iopub.status.busy": "2025-06-19T22:26:02.714899Z",
     "iopub.status.idle": "2025-06-19T22:26:02.806069Z",
     "shell.execute_reply": "2025-06-19T22:26:02.804841Z",
     "shell.execute_reply.started": "2025-06-19T22:26:02.715411Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.genai_model.genai_model import GenAIModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a99607c2-d62f-4742-a46d-8d8f5dd4612f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T22:38:06.739892Z",
     "iopub.status.busy": "2025-06-19T22:38:06.739361Z",
     "iopub.status.idle": "2025-06-19T22:38:08.782147Z",
     "shell.execute_reply": "2025-06-19T22:38:08.780234Z",
     "shell.execute_reply.started": "2025-06-19T22:38:06.739863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OpenAI faces legal challenges from Elon Musk over its for-profit transition, citing antitrust concerns and financial risks.', \"The company also confronts industry structural challenges, including NVIDIA's dominance in chip supply and intense competition, which may hinder profitability despite significant capital raised.\", 'Meanwhile, OpenAI recently terminated testing access to its Sora video generation platform following a public leak by a group of artist testers.', 'The AI sector, while advancing technologically, may see more success in leveraging existing models than in developing new ones.', '(summary provided by mistral/mistral-large-2411)']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(summary)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f24ea6a-ce97-4f1e-be4f-e6c1119e7ce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T22:49:51.426293Z",
     "iopub.status.busy": "2025-06-19T22:49:51.425004Z",
     "iopub.status.idle": "2025-06-19T22:49:51.433528Z",
     "shell.execute_reply": "2025-06-19T22:49:51.431861Z",
     "shell.execute_reply.started": "2025-06-19T22:49:51.426259Z"
    }
   },
   "outputs": [],
   "source": [
    "def define_user_prompt(sentence: str, article_summary: str) -> str:\n",
    "    user_prompt = \\\n",
    "    f\"\"\"\n",
    "    I wrote a text using the content of multiple news articles. \n",
    "    I will provide you with one sentence from that text that I wrote [SENTENCE], and with a summary of one of the articles that I used to write the entire text [ARTICLE_SUMMARY]. \n",
    "    I want you to tell me whether that specific article was used to write that specific sentence. \n",
    "    The input will be formatted as follows:\n",
    "    [SENTENCE]: <a single sentence from the text that I wrote>\n",
    "    [ARTICLE SUMMARY]: <the summary of an article that I used to write that text>\n",
    "    \n",
    "    You response can only be one of two words: \"YES\" or \"NO\". Do not answer anything else.\n",
    "    \"YES\" means this article was used to write that sentence\n",
    "    \"NO\" means this article was not used to write that summary.\n",
    "    \n",
    "    Below are the actual sentence and article summary that I want you to use:\n",
    "    [SENTENCE]: {sentence}\n",
    "    [ARTICLE SUMMARY]: {article_summary}\n",
    "    \"\"\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f74fd0b-b86b-4b98-addf-9c87e7e3e44f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T22:49:52.607470Z",
     "iopub.status.busy": "2025-06-19T22:49:52.606892Z",
     "iopub.status.idle": "2025-06-19T22:49:52.616539Z",
     "shell.execute_reply": "2025-06-19T22:49:52.615115Z",
     "shell.execute_reply.started": "2025-06-19T22:49:52.607438Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \\\n",
    "\"\"\"\n",
    "You are an expert technical writer, specialized in the field of Artificial Intelligence.\n",
    "You are tasked with identifying whether a given article was used to write a sentence taken from a longer text.\n",
    "You need to be very precise and very careful before giving your answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a08de64-36f3-4a7f-86a3-2560e0c95f69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T22:49:53.448895Z",
     "iopub.status.busy": "2025-06-19T22:49:53.448525Z",
     "iopub.status.idle": "2025-06-19T22:49:53.459206Z",
     "shell.execute_reply": "2025-06-19T22:49:53.457859Z",
     "shell.execute_reply.started": "2025-06-19T22:49:53.448870Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 18:49:53,450 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 18:49:53,452 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n"
     ]
    }
   ],
   "source": [
    "annotator = GenAIModel(model_type=\"medium\", system_promt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1408879-e262-46b5-98f3-f2e0a48cfaa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T23:14:11.243180Z",
     "iopub.status.busy": "2025-06-19T23:14:11.242014Z",
     "iopub.status.idle": "2025-06-19T23:14:12.962141Z",
     "shell.execute_reply": "2025-06-19T23:14:12.961017Z",
     "shell.execute_reply.started": "2025-06-19T23:14:11.243145Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:14:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 19:14:11,248 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence to match: OpenAI faces legal challenges from Elon Musk over its for-profit transition, citing antitrust concerns and financial risks.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 19:14:11,822 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m19:14:11 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 19:14:11,831 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:14:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 19:14:11,834 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article title: ELON MUSK SEEKS TO BLOCK OPENAI'S FOR-PROFIT CONVERSION AGAIN -- response: YES\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 19:14:12,269 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m19:14:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 19:14:12,273 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m19:14:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 19:14:12,276 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article title: BUILDING LLMS IS PROBABLY NOT GOING BE A BRILLIANT BUSINESS -- response: NO\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 19:14:12,949 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m19:14:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 19:14:12,952 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article title: OPENAI IS AT WAR WITH ITS OWN SORA VIDEO TESTERS FOLLOWING BRIEF\n",
      "PUBLIC LEAK -- response: NO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "sent = sentences[idx]\n",
    "print(f\"Sentence to match: {sent}\\n\")\n",
    "for _dd in df_cite.index:\n",
    "    article_summary = df_cite.loc[_dd, \"news_summary\"]\n",
    "    article_title = df_cite.loc[_dd, \"title\"]\n",
    "    response = annotator.completion_str(\n",
    "        user_prompt=define_user_prompt(sentence=sent, article_summary=article_summary), \n",
    "        parameters={\"temperature\": 0.2, \"top_p\": 0.5},\n",
    "    )\n",
    "    print(f\"Article title: {article_title} -- response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "757a6726-4f86-45c1-9d8c-b3259eea5e44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T01:52:40.772404Z",
     "iopub.status.busy": "2025-06-20T01:52:40.771461Z",
     "iopub.status.idle": "2025-06-20T01:52:40.783725Z",
     "shell.execute_reply": "2025-06-20T01:52:40.782800Z",
     "shell.execute_reply.started": "2025-06-20T01:52:40.772360Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def add_inline_citations(sentences: List[str], df_cite: pd.DataFrame):\n",
    "    inline_citations = []\n",
    "    for sent in sentences:\n",
    "        _local_cite = []\n",
    "        for _dd in df_cite.index:\n",
    "            article_summary = df_cite.loc[_dd, \"news_summary\"]\n",
    "            response = annotator.completion_str(\n",
    "                user_prompt=define_user_prompt(sentence=sent, article_summary=article_summary), \n",
    "                parameters={\"temperature\": 0.2, \"top_p\": 0.5},\n",
    "            )\n",
    "            if \"YES\" in response:\n",
    "                _local_cite.append(int(df_cite.loc[_dd, \"nb_citation\"]))\n",
    "        if len(_local_cite) == 0:\n",
    "            _local_cite = \"\"\n",
    "        inline_citations.append(_local_cite)\n",
    "    return inline_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d8f8c23d-9451-48b1-9964-f428c710fa9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T01:52:41.917102Z",
     "iopub.status.busy": "2025-06-20T01:52:41.916422Z",
     "iopub.status.idle": "2025-06-20T01:55:01.973846Z",
     "shell.execute_reply": "2025-06-20T01:55:01.972593Z",
     "shell.execute_reply.started": "2025-06-20T01:52:41.917072Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m21:52:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:41,923 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:42,627 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:52:42 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:52:42,632 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:52:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:42,638 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:43,291 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:52:43 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:52:43,294 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:52:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:43,297 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:43,750 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:52:43 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:52:43,753 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:52:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:43,757 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:44,243 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:52:44 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:52:44,248 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:52:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:44,253 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:44,725 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:52:44 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:52:44,729 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:52:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:44,732 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:45,183 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:52:45 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:52:45,186 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:52:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:45,190 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:45,751 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:52:45 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:52:45,755 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:52:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:45,758 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:46,173 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:52:46 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:52:46,176 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:52:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:46,180 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:46,598 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:52:46 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:52:46,602 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:52:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:46,607 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:47,048 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:52:47 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:52:47,052 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:52:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:47,057 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:47,513 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:52:47 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:52:47,516 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:52:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:47,519 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:47,701 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 21:52:47,720 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m21:52:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 21:52:47,722 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 21:52:48,620 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 21:52:48,636 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m21:52:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 21:52:48,638 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 21:52:49,058 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:52:49 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:52:49,062 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:52:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:49,066 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:52:49,238 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 21:52:49,260 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m21:52:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 21:52:49,263 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 21:52:50,156 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 21:52:50,172 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m21:52:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 21:52:50,175 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 21:55:01,134 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:55:01 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:55:01,136 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:55:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:55:01,139 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:55:01,558 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:55:01 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:55:01,565 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m21:55:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:55:01,570 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 21:55:01,960 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m21:55:01 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 21:55:01,964 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [2], '']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inline_citations=add_inline_citations(sentences, df_cite)\n",
    "inline_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f7309abc-4649-4abc-8024-31517d864426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T01:55:07.057721Z",
     "iopub.status.busy": "2025-06-20T01:55:07.056905Z",
     "iopub.status.idle": "2025-06-20T01:55:07.063108Z",
     "shell.execute_reply": "2025-06-20T01:55:07.061838Z",
     "shell.execute_reply.started": "2025-06-20T01:55:07.057683Z"
    }
   },
   "outputs": [],
   "source": [
    "def join_cite(sent, in_cite):\n",
    "    return sent[:-1] + \" \" + str(in_cite) + sent[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5526f46c-b2f6-415e-aa22-b0c6046fd75a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T01:55:07.600737Z",
     "iopub.status.busy": "2025-06-20T01:55:07.599696Z",
     "iopub.status.idle": "2025-06-20T01:55:07.609761Z",
     "shell.execute_reply": "2025-06-20T01:55:07.607954Z",
     "shell.execute_reply.started": "2025-06-20T01:55:07.600704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OpenAI faces legal challenges from Elon Musk over its for-profit transition, citing antitrust concerns and financial risks [1]. The company also confronts industry structural challenges, including NVIDIA's dominance in chip supply and intense competition, which may hinder profitability despite significant capital raised [2]. Meanwhile, OpenAI recently terminated testing access to its Sora video generation platform following a public leak by a group of artist testers [3]. The AI sector, while advancing technologically, may see more success in leveraging existing models than in developing new ones [2]. (summary provided by mistral/mistral-large-2411 )\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\" \").join([join_cite(a,b) for a, b in zip(sentences, inline_citations)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670557c9-ed89-415c-a2b3-ad9d18bc640f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
