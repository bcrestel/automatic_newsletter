{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a70f2a9-719c-45a8-9df9-043eb3cf9cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:44:35.813435Z",
     "iopub.status.busy": "2025-02-10T04:44:35.812897Z",
     "iopub.status.idle": "2025-02-10T04:44:38.246839Z",
     "shell.execute_reply": "2025-02-10T04:44:38.246141Z",
     "shell.execute_reply.started": "2025-02-10T04:44:35.813404Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.gmail import Gmail\n",
    "from src.newsletters.config import NEWSLETTER_AND_PARSER\n",
    "from src.newsletters.parser.tldr import tldr_parser\n",
    "from src.newsletters.parser.alpha_signal import alpha_signal_parser\n",
    "from src.config import SCOPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348555ad-1e0e-4b8e-8e34-e3054add7402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:44:38.249890Z",
     "iopub.status.busy": "2025-02-10T04:44:38.248147Z",
     "iopub.status.idle": "2025-02-10T04:44:38.257371Z",
     "shell.execute_reply": "2025-02-10T04:44:38.255300Z",
     "shell.execute_reply.started": "2025-02-10T04:44:38.249857Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s -- l.%(lineno)d: %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6024b25-c28e-480f-8297-0c6f43361e99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:44:38.259912Z",
     "iopub.status.busy": "2025-02-10T04:44:38.259231Z",
     "iopub.status.idle": "2025-02-10T04:44:38.277105Z",
     "shell.execute_reply": "2025-02-10T04:44:38.275044Z",
     "shell.execute_reply.started": "2025-02-10T04:44:38.259858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['TLDR AI <dan@tldrnewsletter.com>', 'AlphaSignal <news@alphasignal.ai>', 'TLDR <dan@tldrnewsletter.com>', 'TLDR Product <dan@tldrnewsletter.com>'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEWSLETTER_AND_PARSER.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93ef2c90-9422-44da-840d-f51dc7522f28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:44:38.281390Z",
     "iopub.status.busy": "2025-02-10T04:44:38.280452Z",
     "iopub.status.idle": "2025-02-10T04:44:38.303416Z",
     "shell.execute_reply": "2025-02-10T04:44:38.301762Z",
     "shell.execute_reply.started": "2025-02-10T04:44:38.281229Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 23:44:38,292 - googleapiclient.discovery_cache - INFO -- l.49: file_cache is only supported with oauth2client<4.0.0\n"
     ]
    }
   ],
   "source": [
    "gmail = Gmail(\n",
    "    path_to_token=\"/home/secrets_vault/token.json\",\n",
    "    path_to_credentials=\"/home/secrets_vault/credentials.json\",\n",
    "    scopes=SCOPES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a26028d-0127-4144-a086-3e6ffafe6ea9",
   "metadata": {},
   "source": [
    "# ALPHA SIGNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c39b7a28-6382-4560-84f4-3a0dfec15627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:44:38.308031Z",
     "iopub.status.busy": "2025-02-10T04:44:38.307606Z",
     "iopub.status.idle": "2025-02-10T04:44:38.813840Z",
     "shell.execute_reply": "2025-02-10T04:44:38.812760Z",
     "shell.execute_reply.started": "2025-02-10T04:44:38.308004Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 23:44:38,309 - src.gmail - DEBUG -- l.44: Fetch email for sender AlphaSignal <news@alphasignal.ai> from 2025-02-07 until 2025-02-07.\n",
      "2025-02-09 23:44:38,321 - googleapiclient.discovery - DEBUG -- l.1258: URL being requested: GET https://gmail.googleapis.com/gmail/v1/users/me/messages?q=after%3A1738904400+before%3A1738990799+from%3AAlphaSignal+%3Cnews%40alphasignal.ai%3E&alt=json\n",
      "2025-02-09 23:44:38,324 - google_auth_httplib2 - DEBUG -- l.118: Making request: POST https://oauth2.googleapis.com/token\n",
      "2025-02-09 23:44:38,676 - googleapiclient.discovery - DEBUG -- l.1258: URL being requested: GET https://gmail.googleapis.com/gmail/v1/users/me/messages/194e1dae7e6bbcc6?alt=json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, list)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails = gmail.fetch_emails(sender='AlphaSignal <news@alphasignal.ai>', after='2025-02-07', before='2025-02-07')\n",
    "len(emails), type(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eaec6a7-d237-4d53-82ca-b67e016eb887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:44:38.814732Z",
     "iopub.status.busy": "2025-02-10T04:44:38.814471Z",
     "iopub.status.idle": "2025-02-10T04:44:44.783287Z",
     "shell.execute_reply": "2025-02-10T04:44:44.782337Z",
     "shell.execute_reply.started": "2025-02-10T04:44:38.814710Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 23:44:38,815 - src.newsletters.parser.alpha_signal - INFO -- l.34: Parsing email '🔥Mistral AI updates Chatbot, Faster than ChatGPT & Claude'\n",
      "2025-02-09 23:44:38,932 - src.genai_model.genai_model - INFO -- l.106: Found 12 models for model_type small\n",
      "2025-02-09 23:44:38,933 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-1.5-flash-8b-latest', 'gemini/gemini-1.5-flash-8b-001', 'gemini/gemini-1.5-flash-8b-exp-0924', 'gemini/gemini-1.5-flash-8b-exp-0827', 'openrouter/google/gemini-flash-1.5-8b-exp', 'groq/gemma2-9b-it', 'openrouter/google/gemma-2-9b-it:free', 'mistal/ministral-8b-2410', 'groq/llama-3.1-8b-instant', 'openrouter/meta-llama/llama-3.1-8b-instruct:free', 'groq/llama-3-8b-8192', 'openrouter/meta-llama/llama-3-8b-instruct:free']\n",
      "\u001b[92m23:44:38 - LiteLLM:DEBUG\u001b[0m: utils.py:299 - \n",
      "\n",
      "2025-02-09 23:44:38,934 - LiteLLM - DEBUG -- l.299: \n",
      "\n",
      "\u001b[92m23:44:38 - LiteLLM:DEBUG\u001b[0m: utils.py:299 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "2025-02-09 23:44:38,937 - LiteLLM - DEBUG -- l.299: \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m23:44:38 - LiteLLM:DEBUG\u001b[0m: utils.py:299 - \u001b[92mlitellm.completion(model='gemini/gemini-1.5-flash-8b-latest', messages=[{'role': 'system', 'content': '\\nYou are an expert technical writer, specialized in the field of Artificial Intelligence.\\nYour speciality is to summarize multiple news articles into professional news digests.\\nYou audience consists of senior business leaders who are interested in \\n1. understanding the technological trends in AI, \\n2. monitoring the fundraising activities in the ecosystem, and \\n3. tracking the main AI companies.\\n        '}, {'role': 'user', 'content': '\\nI will provide you with a list of news articles, and I want you to summarize all those articles into a single paragraph. Here are a few additional requirements:\\n- The summary should be as short as possible.\\n- The core content of all the news stories should be contained in the summary. \\n- If several news stories contain similar or identical material, this should not be repeated in the summary.\\n\\nTo produce the summaries, I want you to follow these steps:\\n1. Organize the news articles into a small number of categories.\\n2. For each category, summarize all news articles of that category into a concise paragraph.\\n3. Combine the summaries of all categories together.\\n\\nYou must only return the output from step 3, not the intermediate steps (steps 1 and 2).\\n\\nEach news article follows the same format:\\n<TITLE> : <MAIN TEXT>\\nHere is the list of news articles that I want you to use to create your newsletter:\\nMistral AI brings Le Chat to iOS, Android, and enterprise : French AI lab, Mistral AI updates its chatbot\\xa0LE CHAT and makes it\\r\\navailable on iOS, Android, and enterprise infrastructure. It\\r\\nintroduces FLASH ANSWERS, A BUILT-IN CODE INTERPRETER, and REAL-TIME\\r\\nSEARCH, competing with ChatGPT, Claude, and DeepSeek.\\r\\n\\r\\nCORE FUNCTIONALITIES\\r\\n\\r\\nLe Chat provides tools for both general use and technical\\r\\napplications, offering:\\r\\n\\r\\n \\t* OCR ENGINE: Processes PDFs, spreadsheets, and complex or\\r\\nlow-quality files.\\r\\n\\r\\n \\t* CODE INTERPRETER: Runs scripts, handles data analysis, and creates\\r\\nvisualizations.\\r\\n\\r\\n \\t* IMAGE GENERATION: Uses BLACK FOREST LABS’ FLUX ULTRA to generate\\r\\nphotorealistic visuals.\\r\\n\\r\\n \\t* REAL-TIME INFORMATION RETRIEVAL: Combines static pre-trained\\r\\nknowledge with updated data from web searches and news outlets.\\r\\n\\r\\n \\t* Its MULTI-STEP AGENT FRAMEWORK lets you AUTOMATE WORKFLOWS by\\r\\nintegrating with messaging systems and databases.\\r\\n\\r\\nPERFORMANCE\\r\\n\\r\\nLe Chat runs on Mistral’s latest models and introduces FLASH\\r\\nANSWERS, a feature enabling RESPONSE SPEEDS OF UP TO 1,000 WORDS PER\\r\\nSECOND. It supports real-time web search with data from journalism\\r\\nsources and social platforms. The assistant processes text, images,\\r\\nand structured data files without external dependencies.\\r\\n\\r\\nCOMPARISON WITH OTHER CHATBOTS\\r\\n\\r\\n \\t* Faster than ChatGPT and Claude in response generation.\\r\\n\\r\\n \\t* Stronger OCR and document parsing than Claude.\\r\\n\\r\\n \\t* Code execution similar to ChatGPT Plus.\\r\\n\\r\\n \\t* Web search supports social media and journalism sources.\\r\\n\\r\\n \\t* Claude is known\\xa0\\xa0for token efficiency, but Le Chat provides\\r\\ncustom enterprise deployments and flexible infrastructure setups.\\r\\n\\r\\n \\t* Le Chat adheres to EU privacy standards (GDPR).\\r\\n\\r\\nENTERPRISE DEPLOYMENTS AND CUSTOMIZATION\\r\\nLe Chat targets enterprises with options for SAAS, ON-PREMISE, and VPC\\r\\nDEPLOYMENT. Teams can customize models and ensure on-site data\\r\\nsecurity, making it a viable alternative to U.S. and Chinese\\r\\nproviders.\\r\\n\\r\\nACCESS AND AVAILABILITY\\r\\n\\r\\nYou can find Le Chat available on web, iOS, and Android. It offers\\r\\nthree tiers:\\r\\n\\r\\n \\t* FREE PLAN: Limited usage with access to core features.\\r\\n\\r\\n \\t* PRO PLAN: Faster responses, priority access, and extended usage\\r\\nlimits.\\r\\n\\r\\n \\t* ENTERPRISE: Custom deployment on ON-PREMISE OR VPC INFRASTRUCTURE,\\r\\nmodel fine-tuning, and API rate customization.\\n        '}], temperature=0.2, top_p=0.5)\u001b[0m\n",
      "2025-02-09 23:44:38,940 - LiteLLM - DEBUG -- l.299: \u001b[92mlitellm.completion(model='gemini/gemini-1.5-flash-8b-latest', messages=[{'role': 'system', 'content': '\\nYou are an expert technical writer, specialized in the field of Artificial Intelligence.\\nYour speciality is to summarize multiple news articles into professional news digests.\\nYou audience consists of senior business leaders who are interested in \\n1. understanding the technological trends in AI, \\n2. monitoring the fundraising activities in the ecosystem, and \\n3. tracking the main AI companies.\\n        '}, {'role': 'user', 'content': '\\nI will provide you with a list of news articles, and I want you to summarize all those articles into a single paragraph. Here are a few additional requirements:\\n- The summary should be as short as possible.\\n- The core content of all the news stories should be contained in the summary. \\n- If several news stories contain similar or identical material, this should not be repeated in the summary.\\n\\nTo produce the summaries, I want you to follow these steps:\\n1. Organize the news articles into a small number of categories.\\n2. For each category, summarize all news articles of that category into a concise paragraph.\\n3. Combine the summaries of all categories together.\\n\\nYou must only return the output from step 3, not the intermediate steps (steps 1 and 2).\\n\\nEach news article follows the same format:\\n<TITLE> : <MAIN TEXT>\\nHere is the list of news articles that I want you to use to create your newsletter:\\nMistral AI brings Le Chat to iOS, Android, and enterprise : French AI lab, Mistral AI updates its chatbot\\xa0LE CHAT and makes it\\r\\navailable on iOS, Android, and enterprise infrastructure. It\\r\\nintroduces FLASH ANSWERS, A BUILT-IN CODE INTERPRETER, and REAL-TIME\\r\\nSEARCH, competing with ChatGPT, Claude, and DeepSeek.\\r\\n\\r\\nCORE FUNCTIONALITIES\\r\\n\\r\\nLe Chat provides tools for both general use and technical\\r\\napplications, offering:\\r\\n\\r\\n \\t* OCR ENGINE: Processes PDFs, spreadsheets, and complex or\\r\\nlow-quality files.\\r\\n\\r\\n \\t* CODE INTERPRETER: Runs scripts, handles data analysis, and creates\\r\\nvisualizations.\\r\\n\\r\\n \\t* IMAGE GENERATION: Uses BLACK FOREST LABS’ FLUX ULTRA to generate\\r\\nphotorealistic visuals.\\r\\n\\r\\n \\t* REAL-TIME INFORMATION RETRIEVAL: Combines static pre-trained\\r\\nknowledge with updated data from web searches and news outlets.\\r\\n\\r\\n \\t* Its MULTI-STEP AGENT FRAMEWORK lets you AUTOMATE WORKFLOWS by\\r\\nintegrating with messaging systems and databases.\\r\\n\\r\\nPERFORMANCE\\r\\n\\r\\nLe Chat runs on Mistral’s latest models and introduces FLASH\\r\\nANSWERS, a feature enabling RESPONSE SPEEDS OF UP TO 1,000 WORDS PER\\r\\nSECOND. It supports real-time web search with data from journalism\\r\\nsources and social platforms. The assistant processes text, images,\\r\\nand structured data files without external dependencies.\\r\\n\\r\\nCOMPARISON WITH OTHER CHATBOTS\\r\\n\\r\\n \\t* Faster than ChatGPT and Claude in response generation.\\r\\n\\r\\n \\t* Stronger OCR and document parsing than Claude.\\r\\n\\r\\n \\t* Code execution similar to ChatGPT Plus.\\r\\n\\r\\n \\t* Web search supports social media and journalism sources.\\r\\n\\r\\n \\t* Claude is known\\xa0\\xa0for token efficiency, but Le Chat provides\\r\\ncustom enterprise deployments and flexible infrastructure setups.\\r\\n\\r\\n \\t* Le Chat adheres to EU privacy standards (GDPR).\\r\\n\\r\\nENTERPRISE DEPLOYMENTS AND CUSTOMIZATION\\r\\nLe Chat targets enterprises with options for SAAS, ON-PREMISE, and VPC\\r\\nDEPLOYMENT. Teams can customize models and ensure on-site data\\r\\nsecurity, making it a viable alternative to U.S. and Chinese\\r\\nproviders.\\r\\n\\r\\nACCESS AND AVAILABILITY\\r\\n\\r\\nYou can find Le Chat available on web, iOS, and Android. It offers\\r\\nthree tiers:\\r\\n\\r\\n \\t* FREE PLAN: Limited usage with access to core features.\\r\\n\\r\\n \\t* PRO PLAN: Faster responses, priority access, and extended usage\\r\\nlimits.\\r\\n\\r\\n \\t* ENTERPRISE: Custom deployment on ON-PREMISE OR VPC INFRASTRUCTURE,\\r\\nmodel fine-tuning, and API rate customization.\\n        '}], temperature=0.2, top_p=0.5)\u001b[0m\n",
      "\u001b[92m23:44:38 - LiteLLM:DEBUG\u001b[0m: utils.py:299 - \n",
      "\n",
      "2025-02-09 23:44:38,945 - LiteLLM - DEBUG -- l.299: \n",
      "\n",
      "\u001b[92m23:44:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:377 - self.optional_params: {}\n",
      "2025-02-09 23:44:38,950 - LiteLLM - DEBUG -- l.377: self.optional_params: {}\n",
      "\u001b[92m23:44:38 - LiteLLM:DEBUG\u001b[0m: utils.py:299 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "2025-02-09 23:44:38,952 - LiteLLM - DEBUG -- l.299: SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m23:44:38 - LiteLLM:DEBUG\u001b[0m: transformation.py:126 - Translating developer role to system role for non-OpenAI providers.\n",
      "2025-02-09 23:44:38,964 - LiteLLM - DEBUG -- l.126: Translating developer role to system role for non-OpenAI providers.\n",
      "\u001b[92m23:44:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b-latest; provider = gemini\n",
      "2025-02-09 23:44:38,967 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b-latest; provider = gemini\n",
      "\u001b[92m23:44:38 - LiteLLM:DEBUG\u001b[0m: utils.py:2912 - \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-1.5-flash-8b-latest', 'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': 0.5, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert technical writer, specialized in the field of Artificial Intelligence.\\nYour speciality is to summarize multiple news articles into professional news digests.\\nYou audience consists of senior business leaders who are interested in \\n1. understanding the technological trends in AI, \\n2. monitoring the fundraising activities in the ecosystem, and \\n3. tracking the main AI companies.\\n        '}, {'role': 'user', 'content': '\\nI will provide you with a list of news articles, and I want you to summarize all those articles into a single paragraph. Here are a few additional requirements:\\n- The summary should be as short as possible.\\n- The core content of all the news stories should be contained in the summary. \\n- If several news stories contain similar or identical material, this should not be repeated in the summary.\\n\\nTo produce the summaries, I want you to follow these steps:\\n1. Organize the news articles into a small number of categories.\\n2. For each category, summarize all news articles of that category into a concise paragraph.\\n3. Combine the summaries of all categories together.\\n\\nYou must only return the output from step 3, not the intermediate steps (steps 1 and 2).\\n\\nEach news article follows the same format:\\n<TITLE> : <MAIN TEXT>\\nHere is the list of news articles that I want you to use to create your newsletter:\\nMistral AI brings Le Chat to iOS, Android, and enterprise : French AI lab, Mistral AI updates its chatbot\\xa0LE CHAT and makes it\\r\\navailable on iOS, Android, and enterprise infrastructure. It\\r\\nintroduces FLASH ANSWERS, A BUILT-IN CODE INTERPRETER, and REAL-TIME\\r\\nSEARCH, competing with ChatGPT, Claude, and DeepSeek.\\r\\n\\r\\nCORE FUNCTIONALITIES\\r\\n\\r\\nLe Chat provides tools for both general use and technical\\r\\napplications, offering:\\r\\n\\r\\n \\t* OCR ENGINE: Processes PDFs, spreadsheets, and complex or\\r\\nlow-quality files.\\r\\n\\r\\n \\t* CODE INTERPRETER: Runs scripts, handles data analysis, and creates\\r\\nvisualizations.\\r\\n\\r\\n \\t* IMAGE GENERATION: Uses BLACK FOREST LABS’ FLUX ULTRA to generate\\r\\nphotorealistic visuals.\\r\\n\\r\\n \\t* REAL-TIME INFORMATION RETRIEVAL: Combines static pre-trained\\r\\nknowledge with updated data from web searches and news outlets.\\r\\n\\r\\n \\t* Its MULTI-STEP AGENT FRAMEWORK lets you AUTOMATE WORKFLOWS by\\r\\nintegrating with messaging systems and databases.\\r\\n\\r\\nPERFORMANCE\\r\\n\\r\\nLe Chat runs on Mistral’s latest models and introduces FLASH\\r\\nANSWERS, a feature enabling RESPONSE SPEEDS OF UP TO 1,000 WORDS PER\\r\\nSECOND. It supports real-time web search with data from journalism\\r\\nsources and social platforms. The assistant processes text, images,\\r\\nand structured data files without external dependencies.\\r\\n\\r\\nCOMPARISON WITH OTHER CHATBOTS\\r\\n\\r\\n \\t* Faster than ChatGPT and Claude in response generation.\\r\\n\\r\\n \\t* Stronger OCR and document parsing than Claude.\\r\\n\\r\\n \\t* Code execution similar to ChatGPT Plus.\\r\\n\\r\\n \\t* Web search supports social media and journalism sources.\\r\\n\\r\\n \\t* Claude is known\\xa0\\xa0for token efficiency, but Le Chat provides\\r\\ncustom enterprise deployments and flexible infrastructure setups.\\r\\n\\r\\n \\t* Le Chat adheres to EU privacy standards (GDPR).\\r\\n\\r\\nENTERPRISE DEPLOYMENTS AND CUSTOMIZATION\\r\\nLe Chat targets enterprises with options for SAAS, ON-PREMISE, and VPC\\r\\nDEPLOYMENT. Teams can customize models and ensure on-site data\\r\\nsecurity, making it a viable alternative to U.S. and Chinese\\r\\nproviders.\\r\\n\\r\\nACCESS AND AVAILABILITY\\r\\n\\r\\nYou can find Le Chat available on web, iOS, and Android. It offers\\r\\nthree tiers:\\r\\n\\r\\n \\t* FREE PLAN: Limited usage with access to core features.\\r\\n\\r\\n \\t* PRO PLAN: Faster responses, priority access, and extended usage\\r\\nlimits.\\r\\n\\r\\n \\t* ENTERPRISE: Custom deployment on ON-PREMISE OR VPC INFRASTRUCTURE,\\r\\nmodel fine-tuning, and API rate customization.\\n        '}]}\n",
      "2025-02-09 23:44:38,973 - LiteLLM - DEBUG -- l.2912: \n",
      "LiteLLM: Params passed to completion() {'model': 'gemini-1.5-flash-8b-latest', 'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': 0.5, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert technical writer, specialized in the field of Artificial Intelligence.\\nYour speciality is to summarize multiple news articles into professional news digests.\\nYou audience consists of senior business leaders who are interested in \\n1. understanding the technological trends in AI, \\n2. monitoring the fundraising activities in the ecosystem, and \\n3. tracking the main AI companies.\\n        '}, {'role': 'user', 'content': '\\nI will provide you with a list of news articles, and I want you to summarize all those articles into a single paragraph. Here are a few additional requirements:\\n- The summary should be as short as possible.\\n- The core content of all the news stories should be contained in the summary. \\n- If several news stories contain similar or identical material, this should not be repeated in the summary.\\n\\nTo produce the summaries, I want you to follow these steps:\\n1. Organize the news articles into a small number of categories.\\n2. For each category, summarize all news articles of that category into a concise paragraph.\\n3. Combine the summaries of all categories together.\\n\\nYou must only return the output from step 3, not the intermediate steps (steps 1 and 2).\\n\\nEach news article follows the same format:\\n<TITLE> : <MAIN TEXT>\\nHere is the list of news articles that I want you to use to create your newsletter:\\nMistral AI brings Le Chat to iOS, Android, and enterprise : French AI lab, Mistral AI updates its chatbot\\xa0LE CHAT and makes it\\r\\navailable on iOS, Android, and enterprise infrastructure. It\\r\\nintroduces FLASH ANSWERS, A BUILT-IN CODE INTERPRETER, and REAL-TIME\\r\\nSEARCH, competing with ChatGPT, Claude, and DeepSeek.\\r\\n\\r\\nCORE FUNCTIONALITIES\\r\\n\\r\\nLe Chat provides tools for both general use and technical\\r\\napplications, offering:\\r\\n\\r\\n \\t* OCR ENGINE: Processes PDFs, spreadsheets, and complex or\\r\\nlow-quality files.\\r\\n\\r\\n \\t* CODE INTERPRETER: Runs scripts, handles data analysis, and creates\\r\\nvisualizations.\\r\\n\\r\\n \\t* IMAGE GENERATION: Uses BLACK FOREST LABS’ FLUX ULTRA to generate\\r\\nphotorealistic visuals.\\r\\n\\r\\n \\t* REAL-TIME INFORMATION RETRIEVAL: Combines static pre-trained\\r\\nknowledge with updated data from web searches and news outlets.\\r\\n\\r\\n \\t* Its MULTI-STEP AGENT FRAMEWORK lets you AUTOMATE WORKFLOWS by\\r\\nintegrating with messaging systems and databases.\\r\\n\\r\\nPERFORMANCE\\r\\n\\r\\nLe Chat runs on Mistral’s latest models and introduces FLASH\\r\\nANSWERS, a feature enabling RESPONSE SPEEDS OF UP TO 1,000 WORDS PER\\r\\nSECOND. It supports real-time web search with data from journalism\\r\\nsources and social platforms. The assistant processes text, images,\\r\\nand structured data files without external dependencies.\\r\\n\\r\\nCOMPARISON WITH OTHER CHATBOTS\\r\\n\\r\\n \\t* Faster than ChatGPT and Claude in response generation.\\r\\n\\r\\n \\t* Stronger OCR and document parsing than Claude.\\r\\n\\r\\n \\t* Code execution similar to ChatGPT Plus.\\r\\n\\r\\n \\t* Web search supports social media and journalism sources.\\r\\n\\r\\n \\t* Claude is known\\xa0\\xa0for token efficiency, but Le Chat provides\\r\\ncustom enterprise deployments and flexible infrastructure setups.\\r\\n\\r\\n \\t* Le Chat adheres to EU privacy standards (GDPR).\\r\\n\\r\\nENTERPRISE DEPLOYMENTS AND CUSTOMIZATION\\r\\nLe Chat targets enterprises with options for SAAS, ON-PREMISE, and VPC\\r\\nDEPLOYMENT. Teams can customize models and ensure on-site data\\r\\nsecurity, making it a viable alternative to U.S. and Chinese\\r\\nproviders.\\r\\n\\r\\nACCESS AND AVAILABILITY\\r\\n\\r\\nYou can find Le Chat available on web, iOS, and Android. It offers\\r\\nthree tiers:\\r\\n\\r\\n \\t* FREE PLAN: Limited usage with access to core features.\\r\\n\\r\\n \\t* PRO PLAN: Faster responses, priority access, and extended usage\\r\\nlimits.\\r\\n\\r\\n \\t* ENTERPRISE: Custom deployment on ON-PREMISE OR VPC INFRASTRUCTURE,\\r\\nmodel fine-tuning, and API rate customization.\\n        '}]}\n",
      "\u001b[92m23:44:38 - LiteLLM:DEBUG\u001b[0m: utils.py:2915 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'top_p': 0.5}\n",
      "2025-02-09 23:44:38,977 - LiteLLM - DEBUG -- l.2915: \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'top_p': 0.5}\n",
      "\u001b[92m23:44:38 - LiteLLM:DEBUG\u001b[0m: utils.py:299 - Final returned optional params: {'temperature': 0.2, 'top_p': 0.5}\n",
      "2025-02-09 23:44:38,982 - LiteLLM - DEBUG -- l.299: Final returned optional params: {'temperature': 0.2, 'top_p': 0.5}\n",
      "\u001b[92m23:44:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:377 - self.optional_params: {'temperature': 0.2, 'top_p': 0.5}\n",
      "2025-02-09 23:44:38,984 - LiteLLM - DEBUG -- l.377: self.optional_params: {'temperature': 0.2, 'top_p': 0.5}\n",
      "\u001b[92m23:44:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4179 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash-8b-latest', 'combined_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'stripped_model_name': 'gemini-1.5-flash-8b-latest', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'custom_llm_provider': 'gemini'}\n",
      "2025-02-09 23:44:39,015 - LiteLLM - DEBUG -- l.4179: checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash-8b-latest', 'combined_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'stripped_model_name': 'gemini-1.5-flash-8b-latest', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m23:44:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4370 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "2025-02-09 23:44:39,017 - LiteLLM - DEBUG -- l.4370: Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "\u001b[92m23:44:39 - LiteLLM:DEBUG\u001b[0m: utils.py:1981 - Model not found or error in checking supports_system_messages support. You passed model=gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Error: This model isn't mapped yet. model=gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n",
      "2025-02-09 23:44:39,019 - LiteLLM - DEBUG -- l.1981: Model not found or error in checking supports_system_messages support. You passed model=gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Error: This model isn't mapped yet. model=gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n",
      "\u001b[92m23:44:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:634 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b-latest:generateContent?key=AIzaSyCTCGul8Epux8G_pMEdksxzJOBYwrrqUXY \\\n",
      "-H 'Content-Type: *****' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': '\\nYou are an expert technical writer, specialized in the field of Artificial Intelligence.\\nYour speciality is to summarize multiple news articles into professional news digests.\\nYou audience consists of senior business leaders who are interested in \\n1. understanding the technological trends in AI, \\n2. monitoring the fundraising activities in the ecosystem, and \\n3. tracking the main AI companies.\\n        '}, {'text': '\\nI will provide you with a list of news articles, and I want you to summarize all those articles into a single paragraph. Here are a few additional requirements:\\n- The summary should be as short as possible.\\n- The core content of all the news stories should be contained in the summary. \\n- If several news stories contain similar or identical material, this should not be repeated in the summary.\\n\\nTo produce the summaries, I want you to follow these steps:\\n1. Organize the news articles into a small number of categories.\\n2. For each category, summarize all news articles of that category into a concise paragraph.\\n3. Combine the summaries of all categories together.\\n\\nYou must only return the output from step 3, not the intermediate steps (steps 1 and 2).\\n\\nEach news article follows the same format:\\n<TITLE> : <MAIN TEXT>\\nHere is the list of news articles that I want you to use to create your newsletter:\\nMistral AI brings Le Chat to iOS, Android, and enterprise : French AI lab, Mistral AI updates its chatbot\\xa0LE CHAT and makes it\\r\\navailable on iOS, Android, and enterprise infrastructure. It\\r\\nintroduces FLASH ANSWERS, A BUILT-IN CODE INTERPRETER, and REAL-TIME\\r\\nSEARCH, competing with ChatGPT, Claude, and DeepSeek.\\r\\n\\r\\nCORE FUNCTIONALITIES\\r\\n\\r\\nLe Chat provides tools for both general use and technical\\r\\napplications, offering:\\r\\n\\r\\n \\t* OCR ENGINE: Processes PDFs, spreadsheets, and complex or\\r\\nlow-quality files.\\r\\n\\r\\n \\t* CODE INTERPRETER: Runs scripts, handles data analysis, and creates\\r\\nvisualizations.\\r\\n\\r\\n \\t* IMAGE GENERATION: Uses BLACK FOREST LABS’ FLUX ULTRA to generate\\r\\nphotorealistic visuals.\\r\\n\\r\\n \\t* REAL-TIME INFORMATION RETRIEVAL: Combines static pre-trained\\r\\nknowledge with updated data from web searches and news outlets.\\r\\n\\r\\n \\t* Its MULTI-STEP AGENT FRAMEWORK lets you AUTOMATE WORKFLOWS by\\r\\nintegrating with messaging systems and databases.\\r\\n\\r\\nPERFORMANCE\\r\\n\\r\\nLe Chat runs on Mistral’s latest models and introduces FLASH\\r\\nANSWERS, a feature enabling RESPONSE SPEEDS OF UP TO 1,000 WORDS PER\\r\\nSECOND. It supports real-time web search with data from journalism\\r\\nsources and social platforms. The assistant processes text, images,\\r\\nand structured data files without external dependencies.\\r\\n\\r\\nCOMPARISON WITH OTHER CHATBOTS\\r\\n\\r\\n \\t* Faster than ChatGPT and Claude in response generation.\\r\\n\\r\\n \\t* Stronger OCR and document parsing than Claude.\\r\\n\\r\\n \\t* Code execution similar to ChatGPT Plus.\\r\\n\\r\\n \\t* Web search supports social media and journalism sources.\\r\\n\\r\\n \\t* Claude is known\\xa0\\xa0for token efficiency, but Le Chat provides\\r\\ncustom enterprise deployments and flexible infrastructure setups.\\r\\n\\r\\n \\t* Le Chat adheres to EU privacy standards (GDPR).\\r\\n\\r\\nENTERPRISE DEPLOYMENTS AND CUSTOMIZATION\\r\\nLe Chat targets enterprises with options for SAAS, ON-PREMISE, and VPC\\r\\nDEPLOYMENT. Teams can customize models and ensure on-site data\\r\\nsecurity, making it a viable alternative to U.S. and Chinese\\r\\nproviders.\\r\\n\\r\\nACCESS AND AVAILABILITY\\r\\n\\r\\nYou can find Le Chat available on web, iOS, and Android. It offers\\r\\nthree tiers:\\r\\n\\r\\n \\t* FREE PLAN: Limited usage with access to core features.\\r\\n\\r\\n \\t* PRO PLAN: Faster responses, priority access, and extended usage\\r\\nlimits.\\r\\n\\r\\n \\t* ENTERPRISE: Custom deployment on ON-PREMISE OR VPC INFRASTRUCTURE,\\r\\nmodel fine-tuning, and API rate customization.\\n        '}]}], 'generationConfig': {'temperature': 0.2, 'top_p': 0.5}}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-02-09 23:44:39,023 - LiteLLM - DEBUG -- l.634: \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b-latest:generateContent?key=AIzaSyCTCGul8Epux8G_pMEdksxzJOBYwrrqUXY \\\n",
      "-H 'Content-Type: *****' \\\n",
      "-d '{'contents': [{'role': 'user', 'parts': [{'text': '\\nYou are an expert technical writer, specialized in the field of Artificial Intelligence.\\nYour speciality is to summarize multiple news articles into professional news digests.\\nYou audience consists of senior business leaders who are interested in \\n1. understanding the technological trends in AI, \\n2. monitoring the fundraising activities in the ecosystem, and \\n3. tracking the main AI companies.\\n        '}, {'text': '\\nI will provide you with a list of news articles, and I want you to summarize all those articles into a single paragraph. Here are a few additional requirements:\\n- The summary should be as short as possible.\\n- The core content of all the news stories should be contained in the summary. \\n- If several news stories contain similar or identical material, this should not be repeated in the summary.\\n\\nTo produce the summaries, I want you to follow these steps:\\n1. Organize the news articles into a small number of categories.\\n2. For each category, summarize all news articles of that category into a concise paragraph.\\n3. Combine the summaries of all categories together.\\n\\nYou must only return the output from step 3, not the intermediate steps (steps 1 and 2).\\n\\nEach news article follows the same format:\\n<TITLE> : <MAIN TEXT>\\nHere is the list of news articles that I want you to use to create your newsletter:\\nMistral AI brings Le Chat to iOS, Android, and enterprise : French AI lab, Mistral AI updates its chatbot\\xa0LE CHAT and makes it\\r\\navailable on iOS, Android, and enterprise infrastructure. It\\r\\nintroduces FLASH ANSWERS, A BUILT-IN CODE INTERPRETER, and REAL-TIME\\r\\nSEARCH, competing with ChatGPT, Claude, and DeepSeek.\\r\\n\\r\\nCORE FUNCTIONALITIES\\r\\n\\r\\nLe Chat provides tools for both general use and technical\\r\\napplications, offering:\\r\\n\\r\\n \\t* OCR ENGINE: Processes PDFs, spreadsheets, and complex or\\r\\nlow-quality files.\\r\\n\\r\\n \\t* CODE INTERPRETER: Runs scripts, handles data analysis, and creates\\r\\nvisualizations.\\r\\n\\r\\n \\t* IMAGE GENERATION: Uses BLACK FOREST LABS’ FLUX ULTRA to generate\\r\\nphotorealistic visuals.\\r\\n\\r\\n \\t* REAL-TIME INFORMATION RETRIEVAL: Combines static pre-trained\\r\\nknowledge with updated data from web searches and news outlets.\\r\\n\\r\\n \\t* Its MULTI-STEP AGENT FRAMEWORK lets you AUTOMATE WORKFLOWS by\\r\\nintegrating with messaging systems and databases.\\r\\n\\r\\nPERFORMANCE\\r\\n\\r\\nLe Chat runs on Mistral’s latest models and introduces FLASH\\r\\nANSWERS, a feature enabling RESPONSE SPEEDS OF UP TO 1,000 WORDS PER\\r\\nSECOND. It supports real-time web search with data from journalism\\r\\nsources and social platforms. The assistant processes text, images,\\r\\nand structured data files without external dependencies.\\r\\n\\r\\nCOMPARISON WITH OTHER CHATBOTS\\r\\n\\r\\n \\t* Faster than ChatGPT and Claude in response generation.\\r\\n\\r\\n \\t* Stronger OCR and document parsing than Claude.\\r\\n\\r\\n \\t* Code execution similar to ChatGPT Plus.\\r\\n\\r\\n \\t* Web search supports social media and journalism sources.\\r\\n\\r\\n \\t* Claude is known\\xa0\\xa0for token efficiency, but Le Chat provides\\r\\ncustom enterprise deployments and flexible infrastructure setups.\\r\\n\\r\\n \\t* Le Chat adheres to EU privacy standards (GDPR).\\r\\n\\r\\nENTERPRISE DEPLOYMENTS AND CUSTOMIZATION\\r\\nLe Chat targets enterprises with options for SAAS, ON-PREMISE, and VPC\\r\\nDEPLOYMENT. Teams can customize models and ensure on-site data\\r\\nsecurity, making it a viable alternative to U.S. and Chinese\\r\\nproviders.\\r\\n\\r\\nACCESS AND AVAILABILITY\\r\\n\\r\\nYou can find Le Chat available on web, iOS, and Android. It offers\\r\\nthree tiers:\\r\\n\\r\\n \\t* FREE PLAN: Limited usage with access to core features.\\r\\n\\r\\n \\t* PRO PLAN: Faster responses, priority access, and extended usage\\r\\nlimits.\\r\\n\\r\\n \\t* ENTERPRISE: Custom deployment on ON-PREMISE OR VPC INFRASTRUCTURE,\\r\\nmodel fine-tuning, and API rate customization.\\n        '}]}], 'generationConfig': {'temperature': 0.2, 'top_p': 0.5}}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-02-09 23:44:39,039 - httpcore.connection - DEBUG -- l.47: connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "2025-02-09 23:44:39,067 - httpcore.connection - DEBUG -- l.47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fda96820810>\n",
      "2025-02-09 23:44:39,067 - httpcore.connection - DEBUG -- l.47: start_tls.started ssl_context=<ssl.SSLContext object at 0x7fda97ed2570> server_hostname='generativelanguage.googleapis.com' timeout=600.0\n",
      "2025-02-09 23:44:39,101 - httpcore.connection - DEBUG -- l.47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fda92f4e650>\n",
      "2025-02-09 23:44:39,102 - httpcore.http11 - DEBUG -- l.47: send_request_headers.started request=<Request [b'POST']>\n",
      "2025-02-09 23:44:39,105 - httpcore.http11 - DEBUG -- l.47: send_request_headers.complete\n",
      "2025-02-09 23:44:39,107 - httpcore.http11 - DEBUG -- l.47: send_request_body.started request=<Request [b'POST']>\n",
      "2025-02-09 23:44:39,111 - httpcore.http11 - DEBUG -- l.47: send_request_body.complete\n",
      "2025-02-09 23:44:39,113 - httpcore.http11 - DEBUG -- l.47: receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-02-09 23:44:40,911 - httpcore.http11 - DEBUG -- l.47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 10 Feb 2025 04:44:40 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1754'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "2025-02-09 23:44:40,913 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b-latest:generateContent?key=AIzaSyCTCGul8Epux8G_pMEdksxzJOBYwrrqUXY \"HTTP/1.1 200 OK\"\n",
      "2025-02-09 23:44:40,914 - httpcore.http11 - DEBUG -- l.47: receive_response_body.started request=<Request [b'POST']>\n",
      "2025-02-09 23:44:40,917 - httpcore.http11 - DEBUG -- l.47: receive_response_body.complete\n",
      "2025-02-09 23:44:40,918 - httpcore.http11 - DEBUG -- l.47: response_closed.started\n",
      "2025-02-09 23:44:40,919 - httpcore.http11 - DEBUG -- l.47: response_closed.complete\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: utils.py:299 - RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"Mistral AI's Le Chat chatbot is now available on iOS, Android, and enterprise platforms.  Featuring a built-in code interpreter, real-time search, and image generation capabilities, Le Chat competes with ChatGPT, Claude, and DeepSeek.  The chatbot boasts fast response speeds, robust OCR and document parsing, and code execution comparable to ChatGPT Plus.  Le Chat supports enterprise deployments with options for SaaS, on-premise, and VPC deployments, allowing for model customization and on-site data security.  The chatbot is available in free, pro, and enterprise tiers.\\n\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"safetyRatings\": [\n",
      "        {\n",
      "          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "          \"probability\": \"NEGLIGIBLE\"\n",
      "        },\n",
      "        {\n",
      "          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "          \"probability\": \"NEGLIGIBLE\"\n",
      "        },\n",
      "        {\n",
      "          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "          \"probability\": \"NEGLIGIBLE\"\n",
      "        },\n",
      "        {\n",
      "          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "          \"probability\": \"NEGLIGIBLE\"\n",
      "        }\n",
      "      ],\n",
      "      \"avgLogprobs\": -0.18060569439904164\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 888,\n",
      "    \"candidatesTokenCount\": 118,\n",
      "    \"totalTokenCount\": 1006,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 888\n",
      "      }\n",
      "    ],\n",
      "    \"candidatesTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 118\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-1.5-flash-8b-001\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "2025-02-09 23:44:40,921 - LiteLLM - DEBUG -- l.299: RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"Mistral AI's Le Chat chatbot is now available on iOS, Android, and enterprise platforms.  Featuring a built-in code interpreter, real-time search, and image generation capabilities, Le Chat competes with ChatGPT, Claude, and DeepSeek.  The chatbot boasts fast response speeds, robust OCR and document parsing, and code execution comparable to ChatGPT Plus.  Le Chat supports enterprise deployments with options for SaaS, on-premise, and VPC deployments, allowing for model customization and on-site data security.  The chatbot is available in free, pro, and enterprise tiers.\\n\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"safetyRatings\": [\n",
      "        {\n",
      "          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "          \"probability\": \"NEGLIGIBLE\"\n",
      "        },\n",
      "        {\n",
      "          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "          \"probability\": \"NEGLIGIBLE\"\n",
      "        },\n",
      "        {\n",
      "          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "          \"probability\": \"NEGLIGIBLE\"\n",
      "        },\n",
      "        {\n",
      "          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "          \"probability\": \"NEGLIGIBLE\"\n",
      "        }\n",
      "      ],\n",
      "      \"avgLogprobs\": -0.18060569439904164\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 888,\n",
      "    \"candidatesTokenCount\": 118,\n",
      "    \"totalTokenCount\": 1006,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 888\n",
      "      }\n",
      "    ],\n",
      "    \"candidatesTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 118\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-1.5-flash-8b-001\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "2025-02-09 23:44:40,924 - httpcore.connection - DEBUG -- l.47: close.started\n",
      "2025-02-09 23:44:40,925 - httpcore.connection - DEBUG -- l.47: close.complete\n",
      "\u001b[92m23:44:40 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-09 23:44:40,926 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "2025-02-09 23:44:40,929 - LiteLLM - DEBUG -- l.1014: Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:563 - completion_response _select_model_name_for_cost_calc: gemini/gemini-1.5-flash-8b-latest\n",
      "2025-02-09 23:44:40,930 - LiteLLM - DEBUG -- l.563: completion_response _select_model_name_for_cost_calc: gemini/gemini-1.5-flash-8b-latest\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:563 - completion_response _select_model_name_for_cost_calc: gemini/gemini-1.5-flash-8b-latest\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4179 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash-8b-latest', 'combined_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'stripped_model_name': 'gemini-1.5-flash-8b-latest', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'custom_llm_provider': 'gemini'}\n",
      "2025-02-09 23:44:40,931 - LiteLLM - DEBUG -- l.563: completion_response _select_model_name_for_cost_calc: gemini/gemini-1.5-flash-8b-latest\n",
      "2025-02-09 23:44:40,934 - LiteLLM - DEBUG -- l.4179: checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash-8b-latest', 'combined_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'stripped_model_name': 'gemini-1.5-flash-8b-latest', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4179 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash-8b-latest', 'combined_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'stripped_model_name': 'gemini-1.5-flash-8b-latest', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'custom_llm_provider': 'gemini'}\n",
      "2025-02-09 23:44:40,935 - LiteLLM - DEBUG -- l.4179: checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash-8b-latest', 'combined_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'stripped_model_name': 'gemini-1.5-flash-8b-latest', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4370 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "2025-02-09 23:44:40,939 - LiteLLM - DEBUG -- l.4370: Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4370 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "2025-02-09 23:44:40,942 - LiteLLM - DEBUG -- l.4370: Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:861 - response_cost_failure_debug_information: {'error_str': \"This model isn't mapped yet. model=gemini/gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\", 'traceback_str': 'Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4269, in _get_model_info_helper\\n    raise ValueError(\\nValueError: This model isn\\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 843, in _response_cost_calculator\\n    response_cost = litellm.response_cost_calculator(\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 833, in response_cost_calculator\\n    raise e\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 821, in response_cost_calculator\\n    response_cost = completion_cost(\\n                    ^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 771, in completion_cost\\n    raise e\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 750, in completion_cost\\n    ) = cost_per_token(\\n        ^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 283, in cost_per_token\\n    return gemini_cost_per_token(model=model, usage=usage_block)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/gemini/cost_calculator.py\", line 19, in cost_per_token\\n    return generic_cost_per_token(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py\", line 140, in generic_cost_per_token\\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4452, in get_model_info\\n    _model_info = _get_model_info_helper(\\n                  ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4373, in _get_model_info_helper\\n    raise Exception(\\nException: This model isn\\'t mapped yet. model=gemini/gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\\n', 'model': 'gemini-1.5-flash-8b-latest', 'cache_hit': None, 'custom_llm_provider': 'gemini', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}\n",
      "2025-02-09 23:44:40,959 - LiteLLM - DEBUG -- l.861: response_cost_failure_debug_information: {'error_str': \"This model isn't mapped yet. model=gemini/gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\", 'traceback_str': 'Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4269, in _get_model_info_helper\\n    raise ValueError(\\nValueError: This model isn\\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 843, in _response_cost_calculator\\n    response_cost = litellm.response_cost_calculator(\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 833, in response_cost_calculator\\n    raise e\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 821, in response_cost_calculator\\n    response_cost = completion_cost(\\n                    ^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 771, in completion_cost\\n    raise e\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 750, in completion_cost\\n    ) = cost_per_token(\\n        ^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 283, in cost_per_token\\n    return gemini_cost_per_token(model=model, usage=usage_block)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/gemini/cost_calculator.py\", line 19, in cost_per_token\\n    return generic_cost_per_token(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py\", line 140, in generic_cost_per_token\\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4452, in get_model_info\\n    _model_info = _get_model_info_helper(\\n                  ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4373, in _get_model_info_helper\\n    raise Exception(\\nException: This model isn\\'t mapped yet. model=gemini/gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\\n', 'model': 'gemini-1.5-flash-8b-latest', 'cache_hit': None, 'custom_llm_provider': 'gemini', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:861 - response_cost_failure_debug_information: {'error_str': \"This model isn't mapped yet. model=gemini/gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\", 'traceback_str': 'Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4269, in _get_model_info_helper\\n    raise ValueError(\\nValueError: This model isn\\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 843, in _response_cost_calculator\\n    response_cost = litellm.response_cost_calculator(\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 833, in response_cost_calculator\\n    raise e\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 821, in response_cost_calculator\\n    response_cost = completion_cost(\\n                    ^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 771, in completion_cost\\n    raise e\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 750, in completion_cost\\n    ) = cost_per_token(\\n        ^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 283, in cost_per_token\\n    return gemini_cost_per_token(model=model, usage=usage_block)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/gemini/cost_calculator.py\", line 19, in cost_per_token\\n    return generic_cost_per_token(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py\", line 140, in generic_cost_per_token\\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4452, in get_model_info\\n    _model_info = _get_model_info_helper(\\n                  ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4373, in _get_model_info_helper\\n    raise Exception(\\nException: This model isn\\'t mapped yet. model=gemini/gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\\n', 'model': 'gemini-1.5-flash-8b-latest', 'cache_hit': False, 'custom_llm_provider': 'gemini', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}\n",
      "2025-02-09 23:44:40,963 - LiteLLM - DEBUG -- l.861: response_cost_failure_debug_information: {'error_str': \"This model isn't mapped yet. model=gemini/gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\", 'traceback_str': 'Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4269, in _get_model_info_helper\\n    raise ValueError(\\nValueError: This model isn\\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 843, in _response_cost_calculator\\n    response_cost = litellm.response_cost_calculator(\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 833, in response_cost_calculator\\n    raise e\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 821, in response_cost_calculator\\n    response_cost = completion_cost(\\n                    ^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 771, in completion_cost\\n    raise e\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 750, in completion_cost\\n    ) = cost_per_token(\\n        ^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 283, in cost_per_token\\n    return gemini_cost_per_token(model=model, usage=usage_block)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/gemini/cost_calculator.py\", line 19, in cost_per_token\\n    return generic_cost_per_token(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py\", line 140, in generic_cost_per_token\\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4452, in get_model_info\\n    _model_info = _get_model_info_helper(\\n                  ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4373, in _get_model_info_helper\\n    raise Exception(\\nException: This model isn\\'t mapped yet. model=gemini/gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\\n', 'model': 'gemini-1.5-flash-8b-latest', 'cache_hit': False, 'custom_llm_provider': 'gemini', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4179 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash-8b-latest', 'combined_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'stripped_model_name': 'gemini-1.5-flash-8b-latest', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'custom_llm_provider': 'gemini'}\n",
      "2025-02-09 23:44:40,970 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): link.alphasignal.ai:443\n",
      "2025-02-09 23:44:40,964 - LiteLLM - DEBUG -- l.4179: checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash-8b-latest', 'combined_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'stripped_model_name': 'gemini-1.5-flash-8b-latest', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4370 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "2025-02-09 23:44:40,972 - LiteLLM - DEBUG -- l.4370: Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2976 - Model=gemini-1.5-flash-8b-latest is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload\n",
      "2025-02-09 23:44:40,979 - LiteLLM - DEBUG -- l.2976: Model=gemini-1.5-flash-8b-latest is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1040 - Logging Details LiteLLM-Success Call streaming complete\n",
      "2025-02-09 23:44:40,984 - LiteLLM - DEBUG -- l.1040: Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:563 - completion_response _select_model_name_for_cost_calc: gemini/gemini-1.5-flash-8b-latest\n",
      "2025-02-09 23:44:40,987 - LiteLLM - DEBUG -- l.563: completion_response _select_model_name_for_cost_calc: gemini/gemini-1.5-flash-8b-latest\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4179 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash-8b-latest', 'combined_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'stripped_model_name': 'gemini-1.5-flash-8b-latest', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'custom_llm_provider': 'gemini'}\n",
      "2025-02-09 23:44:40,990 - LiteLLM - DEBUG -- l.4179: checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash-8b-latest', 'combined_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'stripped_model_name': 'gemini-1.5-flash-8b-latest', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4370 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "2025-02-09 23:44:40,993 - LiteLLM - DEBUG -- l.4370: Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "\u001b[92m23:44:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:861 - response_cost_failure_debug_information: {'error_str': \"This model isn't mapped yet. model=gemini/gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\", 'traceback_str': 'Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4269, in _get_model_info_helper\\n    raise ValueError(\\nValueError: This model isn\\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 843, in _response_cost_calculator\\n    response_cost = litellm.response_cost_calculator(\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 833, in response_cost_calculator\\n    raise e\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 821, in response_cost_calculator\\n    response_cost = completion_cost(\\n                    ^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 771, in completion_cost\\n    raise e\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 750, in completion_cost\\n    ) = cost_per_token(\\n        ^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 283, in cost_per_token\\n    return gemini_cost_per_token(model=model, usage=usage_block)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/gemini/cost_calculator.py\", line 19, in cost_per_token\\n    return generic_cost_per_token(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py\", line 140, in generic_cost_per_token\\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4452, in get_model_info\\n    _model_info = _get_model_info_helper(\\n                  ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4373, in _get_model_info_helper\\n    raise Exception(\\nException: This model isn\\'t mapped yet. model=gemini/gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\\n', 'model': 'gemini-1.5-flash-8b-latest', 'cache_hit': None, 'custom_llm_provider': 'gemini', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}\n",
      "2025-02-09 23:44:40,997 - LiteLLM - DEBUG -- l.861: response_cost_failure_debug_information: {'error_str': \"This model isn't mapped yet. model=gemini/gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\", 'traceback_str': 'Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4269, in _get_model_info_helper\\n    raise ValueError(\\nValueError: This model isn\\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 843, in _response_cost_calculator\\n    response_cost = litellm.response_cost_calculator(\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 833, in response_cost_calculator\\n    raise e\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 821, in response_cost_calculator\\n    response_cost = completion_cost(\\n                    ^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 771, in completion_cost\\n    raise e\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 750, in completion_cost\\n    ) = cost_per_token(\\n        ^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/cost_calculator.py\", line 283, in cost_per_token\\n    return gemini_cost_per_token(model=model, usage=usage_block)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/gemini/cost_calculator.py\", line 19, in cost_per_token\\n    return generic_cost_per_token(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py\", line 140, in generic_cost_per_token\\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4452, in get_model_info\\n    _model_info = _get_model_info_helper(\\n                  ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 4373, in _get_model_info_helper\\n    raise Exception(\\nException: This model isn\\'t mapped yet. model=gemini/gemini-1.5-flash-8b-latest, custom_llm_provider=gemini. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\\n', 'model': 'gemini-1.5-flash-8b-latest', 'cache_hit': None, 'custom_llm_provider': 'gemini', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}\n",
      "\u001b[92m23:44:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4179 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash-8b-latest', 'combined_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'stripped_model_name': 'gemini-1.5-flash-8b-latest', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'custom_llm_provider': 'gemini'}\n",
      "2025-02-09 23:44:41,000 - LiteLLM - DEBUG -- l.4179: checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash-8b-latest', 'combined_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'stripped_model_name': 'gemini-1.5-flash-8b-latest', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash-8b-latest', 'custom_llm_provider': 'gemini'}\n",
      "\u001b[92m23:44:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4370 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "2025-02-09 23:44:41,002 - LiteLLM - DEBUG -- l.4370: Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
      "\u001b[92m23:44:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2976 - Model=gemini-1.5-flash-8b-latest is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload\n",
      "2025-02-09 23:44:41,006 - LiteLLM - DEBUG -- l.2976: Model=gemini-1.5-flash-8b-latest is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload\n",
      "2025-02-09 23:44:41,083 - urllib3.connectionpool - DEBUG -- l.544: https://link.alphasignal.ai:443 \"HEAD /Z6dVYR HTTP/1.1\" 302 0\n",
      "2025-02-09 23:44:41,086 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): mistral.ai:443\n",
      "2025-02-09 23:44:41,423 - urllib3.connectionpool - DEBUG -- l.544: https://mistral.ai:443 \"HEAD /en/news/all-new-le-chat HTTP/1.1\" 200 0\n",
      "2025-02-09 23:44:41,430 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): link.alphasignal.ai:443\n",
      "2025-02-09 23:44:41,543 - urllib3.connectionpool - DEBUG -- l.544: https://link.alphasignal.ai:443 \"HEAD /Yrwvxf HTTP/1.1\" 302 0\n",
      "2025-02-09 23:44:41,545 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): landing.ai:443\n",
      "2025-02-09 23:44:41,729 - urllib3.connectionpool - DEBUG -- l.544: https://landing.ai:443 \"HEAD /agentic-object-detection HTTP/1.1\" 403 0\n",
      "2025-02-09 23:44:41,734 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): link.alphasignal.ai:443\n",
      "2025-02-09 23:44:41,820 - urllib3.connectionpool - DEBUG -- l.544: https://link.alphasignal.ai:443 \"HEAD /UF6bj7 HTTP/1.1\" 302 0\n",
      "2025-02-09 23:44:41,821 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): github.blog:443\n",
      "2025-02-09 23:44:41,922 - urllib3.connectionpool - DEBUG -- l.544: https://github.blog:443 \"HEAD /news-insights/product-news/github-copilot-the-agent-awakens/ HTTP/1.1\" 200 0\n",
      "2025-02-09 23:44:41,927 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): link.alphasignal.ai:443\n",
      "2025-02-09 23:44:42,023 - urllib3.connectionpool - DEBUG -- l.544: https://link.alphasignal.ai:443 \"HEAD /pE0RTm HTTP/1.1\" 302 0\n",
      "2025-02-09 23:44:42,025 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): developers.googleblog.com:443\n",
      "2025-02-09 23:44:43,576 - urllib3.connectionpool - DEBUG -- l.544: https://developers.googleblog.com:443 \"HEAD /en/imagen-3-arrives-in-the-gemini-api/ HTTP/1.1\" 200 0\n",
      "2025-02-09 23:44:43,581 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): link.alphasignal.ai:443\n",
      "2025-02-09 23:44:43,675 - urllib3.connectionpool - DEBUG -- l.544: https://link.alphasignal.ai:443 \"HEAD /stKaZF HTTP/1.1\" 302 0\n",
      "2025-02-09 23:44:43,677 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): x.com:443\n",
      "2025-02-09 23:44:43,891 - urllib3.connectionpool - DEBUG -- l.544: https://x.com:443 \"HEAD /OpenAI/status/1887616278661112259 HTTP/1.1\" 403 0\n",
      "2025-02-09 23:44:43,894 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): link.alphasignal.ai:443\n",
      "2025-02-09 23:44:44,144 - urllib3.connectionpool - DEBUG -- l.544: https://link.alphasignal.ai:443 \"HEAD /qcV4LG HTTP/1.1\" 302 0\n",
      "2025-02-09 23:44:44,146 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): pikadditions.org:443\n",
      "2025-02-09 23:44:44,280 - urllib3.connectionpool - DEBUG -- l.544: https://pikadditions.org:443 \"HEAD / HTTP/1.1\" 200 0\n",
      "2025-02-09 23:44:44,284 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): link.alphasignal.ai:443\n",
      "2025-02-09 23:44:44,377 - urllib3.connectionpool - DEBUG -- l.544: https://link.alphasignal.ai:443 \"HEAD /5hUes6 HTTP/1.1\" 302 0\n",
      "2025-02-09 23:44:44,379 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-02-09 23:44:44,464 - urllib3.connectionpool - DEBUG -- l.544: https://arxiv.org:443 \"HEAD /abs/2502.03544 HTTP/1.1\" 200 0\n",
      "2025-02-09 23:44:44,466 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): link.alphasignal.ai:443\n",
      "2025-02-09 23:44:44,556 - urllib3.connectionpool - DEBUG -- l.544: https://link.alphasignal.ai:443 \"HEAD /lKJLbL HTTP/1.1\" 302 0\n",
      "2025-02-09 23:44:44,558 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-02-09 23:44:44,621 - urllib3.connectionpool - DEBUG -- l.544: https://arxiv.org:443 \"HEAD /abs/2501.19393 HTTP/1.1\" 200 0\n",
      "2025-02-09 23:44:44,623 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): link.alphasignal.ai:443\n",
      "2025-02-09 23:44:44,711 - urllib3.connectionpool - DEBUG -- l.544: https://link.alphasignal.ai:443 \"HEAD /KDRgwL HTTP/1.1\" 302 0\n",
      "2025-02-09 23:44:44,713 - urllib3.connectionpool - DEBUG -- l.1049: Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-02-09 23:44:44,777 - urllib3.connectionpool - DEBUG -- l.544: https://arxiv.org:443 \"HEAD /html/2501.13946v1 HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "news_sources = alpha_signal_parser(emails[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906b8a68-bc87-46e8-ae62-92b7da5c7938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:44:44.784617Z",
     "iopub.status.busy": "2025-02-10T04:44:44.784328Z",
     "iopub.status.idle": "2025-02-10T04:44:44.794628Z",
     "shell.execute_reply": "2025-02-10T04:44:44.793832Z",
     "shell.execute_reply.started": "2025-02-10T04:44:44.784597Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Mistral AI brings Le Chat to iOS, Android, and enterprise',\n",
       "  'url': 'https://mistral.ai/en/news/all-new-le-chat',\n",
       "  'source_of_the_news': 'AlphaSignal <news@alphasignal.ai>',\n",
       "  'text': 'French AI lab, Mistral AI updates its chatbot\\xa0LE CHAT and makes it\\r\\navailable on iOS, Android, and enterprise infrastructure. It\\r\\nintroduces FLASH ANSWERS, A BUILT-IN CODE INTERPRETER, and REAL-TIME\\r\\nSEARCH, competing with ChatGPT, Claude, and DeepSeek.\\r\\n\\r\\nCORE FUNCTIONALITIES\\r\\n\\r\\nLe Chat provides tools for both general use and technical\\r\\napplications, offering:\\r\\n\\r\\n \\t* OCR ENGINE: Processes PDFs, spreadsheets, and complex or\\r\\nlow-quality files.\\r\\n\\r\\n \\t* CODE INTERPRETER: Runs scripts, handles data analysis, and creates\\r\\nvisualizations.\\r\\n\\r\\n \\t* IMAGE GENERATION: Uses BLACK FOREST LABS’ FLUX ULTRA to generate\\r\\nphotorealistic visuals.\\r\\n\\r\\n \\t* REAL-TIME INFORMATION RETRIEVAL: Combines static pre-trained\\r\\nknowledge with updated data from web searches and news outlets.\\r\\n\\r\\n \\t* Its MULTI-STEP AGENT FRAMEWORK lets you AUTOMATE WORKFLOWS by\\r\\nintegrating with messaging systems and databases.\\r\\n\\r\\nPERFORMANCE\\r\\n\\r\\nLe Chat runs on Mistral’s latest models and introduces FLASH\\r\\nANSWERS, a feature enabling RESPONSE SPEEDS OF UP TO 1,000 WORDS PER\\r\\nSECOND. It supports real-time web search with data from journalism\\r\\nsources and social platforms. The assistant processes text, images,\\r\\nand structured data files without external dependencies.\\r\\n\\r\\nCOMPARISON WITH OTHER CHATBOTS\\r\\n\\r\\n \\t* Faster than ChatGPT and Claude in response generation.\\r\\n\\r\\n \\t* Stronger OCR and document parsing than Claude.\\r\\n\\r\\n \\t* Code execution similar to ChatGPT Plus.\\r\\n\\r\\n \\t* Web search supports social media and journalism sources.\\r\\n\\r\\n \\t* Claude is known\\xa0\\xa0for token efficiency, but Le Chat provides\\r\\ncustom enterprise deployments and flexible infrastructure setups.\\r\\n\\r\\n \\t* Le Chat adheres to EU privacy standards (GDPR).\\r\\n\\r\\nENTERPRISE DEPLOYMENTS AND CUSTOMIZATION\\r\\nLe Chat targets enterprises with options for SAAS, ON-PREMISE, and VPC\\r\\nDEPLOYMENT. Teams can customize models and ensure on-site data\\r\\nsecurity, making it a viable alternative to U.S. and Chinese\\r\\nproviders.\\r\\n\\r\\nACCESS AND AVAILABILITY\\r\\n\\r\\nYou can find Le Chat available on web, iOS, and Android. It offers\\r\\nthree tiers:\\r\\n\\r\\n \\t* FREE PLAN: Limited usage with access to core features.\\r\\n\\r\\n \\t* PRO PLAN: Faster responses, priority access, and extended usage\\r\\nlimits.\\r\\n\\r\\n \\t* ENTERPRISE: Custom deployment on ON-PREMISE OR VPC INFRASTRUCTURE,\\r\\nmodel fine-tuning, and API rate customization.',\n",
       "  'news_summary': \"Mistral AI's Le Chat chatbot is now available on iOS, Android, and enterprise platforms.  Featuring a built-in code interpreter, real-time search, and image generation capabilities, Le Chat competes with ChatGPT, Claude, and DeepSeek.  The chatbot boasts fast response speeds, robust OCR and document parsing, and code execution comparable to ChatGPT Plus.  Le Chat supports enterprise deployments with options for SaaS, on-premise, and VPC deployments, allowing for model customization and on-site data security.  The chatbot is available in free, pro, and enterprise tiers.\",\n",
       "  'date_source': 'Fri, 7 Feb 2025 19:20:06 +0000',\n",
       "  'date_source_time_zone': 'utc',\n",
       "  'news_provider': 'mistral.ai'},\n",
       " {'title': 'Andrew Ng presents Agentic Object Detection',\n",
       "  'url': 'https://landing.ai/agentic-object-detection',\n",
       "  'source_of_the_news': 'AlphaSignal <news@alphasignal.ai>',\n",
       "  'text': '',\n",
       "  'news_summary': 'human-like object recognition via text-driven reasoning.',\n",
       "  'date_source': 'Fri, 7 Feb 2025 19:20:06 +0000',\n",
       "  'date_source_time_zone': 'utc',\n",
       "  'news_provider': 'landing.ai'},\n",
       " {'title': 'GitHub upgrades Copilot with agent mode',\n",
       "  'url': 'https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/',\n",
       "  'source_of_the_news': 'AlphaSignal <news@alphasignal.ai>',\n",
       "  'text': '',\n",
       "  'news_summary': 'GitHub upgrades Copilot with agent mode , multi-file edits, and an upcoming autonomous coding agent.',\n",
       "  'date_source': 'Fri, 7 Feb 2025 19:20:06 +0000',\n",
       "  'date_source_time_zone': 'utc',\n",
       "  'news_provider': 'github.blog'},\n",
       " {'title': 'Google releases Imagen 3 in Gemini API',\n",
       "  'url': 'https://developers.googleblog.com/en/imagen-3-arrives-in-the-gemini-api/',\n",
       "  'source_of_the_news': 'AlphaSignal <news@alphasignal.ai>',\n",
       "  'text': '',\n",
       "  'news_summary': 'high quality, watermarked image generation with improved prompt adherence.',\n",
       "  'date_source': 'Fri, 7 Feb 2025 19:20:06 +0000',\n",
       "  'date_source_time_zone': 'utc',\n",
       "  'news_provider': 'developers.googleblog.com'},\n",
       " {'title': 'OpenAI refines CoT reasoning in O3-Mini',\n",
       "  'url': 'https://x.com/OpenAI/status/1887616278661112259',\n",
       "  'source_of_the_news': 'AlphaSignal <news@alphasignal.ai>',\n",
       "  'text': '',\n",
       "  'news_summary': 'OpenAI refines CoT reasoning in O3-Mini  models, expanding visibility.',\n",
       "  'date_source': 'Fri, 7 Feb 2025 19:20:06 +0000',\n",
       "  'date_source_time_zone': 'utc',\n",
       "  'news_provider': 'x.com'},\n",
       " {'title': 'Pika Labs launches Pikadditions',\n",
       "  'url': 'https://pikadditions.org/',\n",
       "  'source_of_the_news': 'AlphaSignal <news@alphasignal.ai>',\n",
       "  'text': '',\n",
       "  'news_summary': 'add any object or person to any video with ease.',\n",
       "  'date_source': 'Fri, 7 Feb 2025 19:20:06 +0000',\n",
       "  'date_source_time_zone': 'utc',\n",
       "  'news_provider': 'pikadditions.org'},\n",
       " {'title': \"Google AI's AlphaGeometry2 surpasses gold medalists\",\n",
       "  'url': 'https://arxiv.org/abs/2502.03544',\n",
       "  'source_of_the_news': 'AlphaSignal <news@alphasignal.ai>',\n",
       "  'text': '',\n",
       "  'news_summary': \"Google AI's AlphaGeometry2 surpasses gold medalists  in Olympiad geometry, improving problem coverage and solving rates.\",\n",
       "  'date_source': 'Fri, 7 Feb 2025 19:20:06 +0000',\n",
       "  'date_source_time_zone': 'utc',\n",
       "  'news_provider': 'arxiv.org'},\n",
       " {'title': 's1-32B improves test-time scaling',\n",
       "  'url': 'https://arxiv.org/abs/2501.19393',\n",
       "  'source_of_the_news': 'AlphaSignal <news@alphasignal.ai>',\n",
       "  'text': '',\n",
       "  'news_summary': 's1-32B improves test-time scaling  with budget forcing, boosting math reasoning and competition scores.',\n",
       "  'date_source': 'Fri, 7 Feb 2025 19:20:06 +0000',\n",
       "  'date_source_time_zone': 'utc',\n",
       "  'news_provider': 'arxiv.org'},\n",
       " {'title': 'Multi-agent system reduces AI hallucinations',\n",
       "  'url': 'https://arxiv.org/html/2501.13946v1',\n",
       "  'source_of_the_news': 'AlphaSignal <news@alphasignal.ai>',\n",
       "  'text': '',\n",
       "  'news_summary': 'Multi-agent system reduces AI hallucinations using structured reviews, NLP-based coordination, and new KPIs.',\n",
       "  'date_source': 'Fri, 7 Feb 2025 19:20:06 +0000',\n",
       "  'date_source_time_zone': 'utc',\n",
       "  'news_provider': 'arxiv.org'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c073d77-4142-40f2-9957-dc305924db2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:25:45.010385Z",
     "iopub.status.busy": "2025-02-10T04:25:45.010073Z",
     "iopub.status.idle": "2025-02-10T04:25:45.018545Z",
     "shell.execute_reply": "2025-02-10T04:25:45.016868Z",
     "shell.execute_reply.started": "2025-02-10T04:25:45.010360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://mistral.ai/en/news/all-new-le-chat'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sources[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8692d99-73cf-4713-abe3-f5d2a96ad75d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:30:24.500697Z",
     "iopub.status.busy": "2025-02-10T04:30:24.500362Z",
     "iopub.status.idle": "2025-02-10T04:30:24.514072Z",
     "shell.execute_reply": "2025-02-10T04:30:24.512512Z",
     "shell.execute_reply.started": "2025-02-10T04:30:24.500671Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/home/logs/database_news_stories.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a69b588-4a99-4223-8e39-f390e8f521ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:30:30.741393Z",
     "iopub.status.busy": "2025-02-10T04:30:30.740718Z",
     "iopub.status.idle": "2025-02-10T04:30:30.756780Z",
     "shell.execute_reply": "2025-02-10T04:30:30.755370Z",
     "shell.execute_reply.started": "2025-02-10T04:30:30.741355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                                           object\n",
       "url                                             object\n",
       "news_provider                                   object\n",
       "source_of_the_news                              object\n",
       "text                                            object\n",
       "news_summary                                    object\n",
       "date_source                 datetime64[ns, US/Eastern]\n",
       "date_source_time_zone                           object\n",
       "version                                         object\n",
       "competitive_intelligence                        object\n",
       "themes                                          object\n",
       "market_intelligence                             object\n",
       "personalities                                   object\n",
       "score_category_count                             int64\n",
       "unique_id                                        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6160c-4398-428d-9f43-46a7be72f5ba",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f5c812-5117-4fc2-b493-dcaa44f68ea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:04:43.807732Z",
     "iopub.status.busy": "2025-02-10T04:04:43.807317Z",
     "iopub.status.idle": "2025-02-10T04:04:43.815582Z",
     "shell.execute_reply": "2025-02-10T04:04:43.814523Z",
     "shell.execute_reply.started": "2025-02-10T04:04:43.807703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sender', 'subject', 'date_utc', 'id', 'text'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57cac741-6738-4c4c-9040-7314a49c9d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:04:44.681899Z",
     "iopub.status.busy": "2025-02-10T04:04:44.681539Z",
     "iopub.status.idle": "2025-02-10T04:04:44.689669Z",
     "shell.execute_reply": "2025-02-10T04:04:44.688543Z",
     "shell.execute_reply.started": "2025-02-10T04:04:44.681874Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AlphaSignal <news@alphasignal.ai>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails[0]['sender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc5974f5-df32-4855-8701-ddbd2c5a71c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:04:45.832275Z",
     "iopub.status.busy": "2025-02-10T04:04:45.831911Z",
     "iopub.status.idle": "2025-02-10T04:04:45.839727Z",
     "shell.execute_reply": "2025-02-10T04:04:45.838438Z",
     "shell.execute_reply.started": "2025-02-10T04:04:45.832244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🔥Mistral AI updates Chatbot, Faster than ChatGPT & Claude'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails[0]['subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d360ca96-2773-48a9-aa82-5fd0ccfefc0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:04:49.355081Z",
     "iopub.status.busy": "2025-02-10T04:04:49.354752Z",
     "iopub.status.idle": "2025-02-10T04:04:49.362633Z",
     "shell.execute_reply": "2025-02-10T04:04:49.361411Z",
     "shell.execute_reply.started": "2025-02-10T04:04:49.355057Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Andrew Ng's agentic object detection, GitHub Copilot's agent mode,\n",
      "Google's Imagen 3 on Gemini API,... \n",
      "\n",
      "SIGNUP [1]  |  WORK WITH US [2]  |  FOLLOW ON X\n",
      "[3]  |  READ ON WEB [4]\n",
      "\n",
      " . \n",
      "\n",
      "HEY ,\n",
      "\n",
      "Welcome to AlphaSignal – the most read newsletter by AI\n",
      "developers. \n",
      "\n",
      "We bring you the top 1% of news, papers, models, and repos, all\n",
      "summarized to keep you updated on the latest in AI.\n",
      "\n",
      "IN TODAY'S SIGNAL\n",
      "\n",
      "_Read time: 5 min 41 sec_\n",
      "\n",
      "🎖️ TOP NEWS\n",
      "\n",
      " \t*\n",
      "Mistral AI brings Le Chat to iOS, Android, and enterprise [5] with web\n",
      "search and automation tools.\n",
      "\n",
      "⚡️ TRENDING SIGNALS\n",
      "\n",
      " \t*\n",
      "Andrew Ng presents Agentic Object Detection [6]: human-like object\n",
      "recognition via text-driven reasoning.\n",
      "\n",
      " \t*\n",
      "GitHub upgrades Copilot with agent mode [7], multi-file edits, and an\n",
      "upcoming autonomous coding agent.\n",
      "\n",
      " \t*\n",
      "Google releases Imagen 3 in Gemini API [8]: high quality, watermarked\n",
      "image generation with improved prompt adherence.\n",
      "\n",
      " \t*\n",
      "OpenAI refines CoT reasoning in O3-Mini [9] models, expanding\n",
      "visibility.\n",
      "\n",
      " \t*\n",
      "Pika Labs launches Pikadditions [10]: add any object or person to any\n",
      "video with ease.\n",
      "\n",
      "💻TOP PAPERS\n",
      "\n",
      " \t*\n",
      "Google AI's AlphaGeometry2 surpasses gold medalists [11] in Olympiad\n",
      "geometry, improving problem coverage and solving rates.\n",
      "\n",
      " \t*\n",
      "s1-32B improves test-time scaling [12] with budget forcing, boosting\n",
      "math reasoning and competition scores.\n",
      "\n",
      " \t*\n",
      "Multi-agent system reduces AI hallucinations [13]using structured\n",
      "reviews, NLP-based coordination, and new KPIs.\n",
      "\n",
      "🧠 PYTHON TIP\n",
      "\n",
      " \t*\n",
      "How to generate high-quality images with Imagen 3 using the Gemini\n",
      "API.\n",
      "\n",
      "IF YOU'RE ENJOYING ALPHASIGNAL PLEASE FORWARD THIS EMAIL TO A\n",
      "COLLEAGUE. \n",
      "\n",
      "It helps us keep this content free.\n",
      "\n",
      "TOP NEWS\n",
      "\n",
      "AI Assistant\n",
      "\n",
      "MISTRAL AI UPGRADES ITS CHATBOT WITH FASTER RESPONSES, CODE EXECUTION\n",
      "FOR MOBILE AND ENTERPRISE DEPLOYMENT\n",
      "\n",
      "⇧ 13,532 Likes\n",
      "\n",
      "\t\t [5] \n",
      "\n",
      "WHAT'S NEW\n",
      "\n",
      "French AI lab, Mistral AI updates its chatbot LE CHAT and makes it\n",
      "available on iOS, Android, and enterprise infrastructure. It\n",
      "introduces FLASH ANSWERS, A BUILT-IN CODE INTERPRETER, and REAL-TIME\n",
      "SEARCH, competing with ChatGPT, Claude, and DeepSeek.\n",
      "\n",
      "CORE FUNCTIONALITIES\n",
      "\n",
      "Le Chat provides tools for both general use and technical\n",
      "applications, offering:\n",
      "\n",
      " \t* OCR ENGINE: Processes PDFs, spreadsheets, and complex or\n",
      "low-quality files.\n",
      "\n",
      " \t* CODE INTERPRETER: Runs scripts, handles data analysis, and creates\n",
      "visualizations.\n",
      "\n",
      " \t* IMAGE GENERATION: Uses BLACK FOREST LABS’ FLUX ULTRA to generate\n",
      "photorealistic visuals.\n",
      "\n",
      " \t* REAL-TIME INFORMATION RETRIEVAL: Combines static pre-trained\n",
      "knowledge with updated data from web searches and news outlets.\n",
      "\n",
      " \t* Its MULTI-STEP AGENT FRAMEWORK lets you AUTOMATE WORKFLOWS by\n",
      "integrating with messaging systems and databases.\n",
      "\n",
      "PERFORMANCE\n",
      "\n",
      "Le Chat runs on Mistral’s latest models and introduces FLASH\n",
      "ANSWERS, a feature enabling RESPONSE SPEEDS OF UP TO 1,000 WORDS PER\n",
      "SECOND. It supports real-time web search with data from journalism\n",
      "sources and social platforms. The assistant processes text, images,\n",
      "and structured data files without external dependencies.\n",
      "\n",
      "COMPARISON WITH OTHER CHATBOTS\n",
      "\n",
      " \t* Faster than ChatGPT and Claude in response generation.\n",
      "\n",
      " \t* Stronger OCR and document parsing than Claude.\n",
      "\n",
      " \t* Code execution similar to ChatGPT Plus.\n",
      "\n",
      " \t* Web search supports social media and journalism sources.\n",
      "\n",
      " \t* Claude is known  for token efficiency, but Le Chat provides\n",
      "custom enterprise deployments and flexible infrastructure setups.\n",
      "\n",
      " \t* Le Chat adheres to EU privacy standards (GDPR).\n",
      "\n",
      "ENTERPRISE DEPLOYMENTS AND CUSTOMIZATION\n",
      "Le Chat targets enterprises with options for SAAS, ON-PREMISE, and VPC\n",
      "DEPLOYMENT. Teams can customize models and ensure on-site data\n",
      "security, making it a viable alternative to U.S. and Chinese\n",
      "providers.\n",
      "\n",
      "ACCESS AND AVAILABILITY\n",
      "\n",
      "You can find Le Chat available on web, iOS, and Android. It offers\n",
      "three tiers:\n",
      "\n",
      " \t* FREE PLAN: Limited usage with access to core features.\n",
      "\n",
      " \t* PRO PLAN: Faster responses, priority access, and extended usage\n",
      "limits.\n",
      "\n",
      " \t* ENTERPRISE: Custom deployment on ON-PREMISE OR VPC INFRASTRUCTURE,\n",
      "model fine-tuning, and API rate customization.\n",
      "\n",
      "\t\tTRY NOW [5] \n",
      "\n",
      "TRENDING SIGNALS\n",
      "\n",
      "Computer Vision\n",
      "\n",
      "ANDREW NG INTRODUCES A REASONING DRIVEN APPROACH TO OBJECT DETECTION,\n",
      "USES AGENTIC WORKFLOWS TO ANALYZE AND IDENTIFY OBJECTS [6]\n",
      "\n",
      "⇧ 3,285 Likes\n",
      "\n",
      "Coding Assistant\n",
      "\n",
      "GITHUB UNVEILS COPILOT AGENT MODE: SELF-ITERATING CODE, TERMINAL\n",
      "GUIDANCE, AND AUTOMATED ERROR FIXING IN VS CODE [7]\n",
      "\n",
      "⇧ 6,385 Likes\n",
      "\n",
      "Image Generation\n",
      "\n",
      "GOOGLE LAUNCHES IMAGEN 3 ON GEMINI API WITH IMPROVED PROMPT ACCURACY,\n",
      "MULTI-STYLE SUPPORT, AND AI CONTENT VERIFICATION [8]\n",
      "\n",
      "⇧ 1,024 Likes\n",
      "\n",
      "LLM\n",
      "\n",
      "OPENAI UPDATES O3-MINI WITH IMPROVED CHAIN-OF-THOUGHT REASONING FOR\n",
      "FREE AND PAID USERS [9]\n",
      "\n",
      "⇧ 4,283 Likes\n",
      "\n",
      "AI Video Editing\n",
      "\n",
      "PIKA LABS RELEASES VIDEO TO VIDEO PLATFORM THAT ALLOWS ADDITION OF ANY\n",
      "OBJECT OR PERSON INTO EXISTING VIDEOS WITH 15 FREE GENERATIONS [10]\n",
      "\n",
      "⇧ 1,384 Likes\n",
      "\n",
      "TOP PAPERS\n",
      "\n",
      "Mathematical Reasoning\n",
      "\n",
      "GOLD-MEDALIST PERFORMANCE IN SOLVING OLYMPIAD GEOMETRY WITH\n",
      "ALPHAGEOMETRY2 [11]\n",
      "\n",
      "⇧ 6,128 Likes\n",
      "\n",
      "PROBLEM\n",
      "\n",
      "Google AI's AlphaGeometry struggled with solving complex Olympiad\n",
      "geometry problems, especially those involving object movements and\n",
      "linear equations of angles, ratios, and distances. Its coverage of\n",
      "International Math Olympiad (IMO) geometry problems was limited.\n",
      "\n",
      "SOLUTION\n",
      "AlphaGeometry2 extends the problem-solving language, improves search\n",
      "with the Gemini architecture, and introduces a novel knowledge-sharing\n",
      "mechanism to combine multiple search trees. Enhancements in the\n",
      "symbolic engine and synthetic data generation further refine its\n",
      "accuracy.\n",
      "\n",
      "RESULTS\n",
      "AlphaGeometry2 boosts IMO geometry problem coverage from 66% to 88%\n",
      "and increases the solving rate from 54% to 84%, surpassing average\n",
      "gold medalists and contributing to a silver-medal performance at IMO\n",
      "2024.\n",
      "\n",
      "LLM Reasoning\n",
      "\n",
      "S1: SIMPLE TEST-TIME SCALING [12]\n",
      "\n",
      "⇧ 4,593 Likes\n",
      "\n",
      "PROBLEM\n",
      "\n",
      "Test-time scaling helps language models perform better by using extra\n",
      "compute, but OpenAI’s O1 model showed this without sharing details.\n",
      "A simple way to achieve this and improve reasoning was needed.\n",
      "\n",
      "SOLUTION\n",
      "The s1-32B model is trained on a small dataset (s1K) of 1,000\n",
      "reasoning questions. It uses budget forcing, a method that controls\n",
      "how long the model thinks by adding or stopping “Wait” signals,\n",
      "helping it fix mistakes.\n",
      "\n",
      "RESULTS\n",
      "s1-32B outperforms O1-preview, improving math competition scores by up\n",
      "to 27% (MATH, AIME24). Budget forcing boosts AIME24 accuracy from 50%\n",
      "to 57%. The model, data, and code are open-source.\n",
      "\n",
      "AI Safety and Reliability\n",
      "\n",
      "HALLUCINATION MITIGATION USING AGENTIC AI NATURAL LANGUAGE-BASED\n",
      "FRAMEWORKS [13]\n",
      "\n",
      "⇧ 2,493 Likes\n",
      "\n",
      "PROBLEM\n",
      "\n",
      "Generative AI models often produce hallucinations, making them less\n",
      "reliable and reducing trust in AI systems. A better approach was\n",
      "needed to detect and refine misleading content while maintaining clear\n",
      "and factual responses.\n",
      "\n",
      "SOLUTION\n",
      "A multi-agent system is designed using over 300 prompts to induce\n",
      "hallucinations. AI agents at different levels review and refine\n",
      "outputs using distinct language models, structured JSON communication,\n",
      "and the OVON framework for seamless interaction. New KPIs are\n",
      "introduced to measure hallucination levels.\n",
      "\n",
      "RESULTS\n",
      "The multi-agent approach lowers hallucination scores, improves\n",
      "clarity, and makes speculative content more transparent. Structured\n",
      "meta-information exchange enhances AI explainability, boosting trust\n",
      "in AI-generated responses.\n",
      "\n",
      "PYTHON TIP\n",
      "\n",
      "GENERATE AN IMAGE WITH IMAGEN 3 USING GEMINI API\n",
      "\n",
      "Google's IMAGEN 3 is now available via the GEMINI API, allowing you to\n",
      "generate high-quality images with improved prompt adherence. It\n",
      "supports various styles, from hyperrealism to anime, and includes a\n",
      "SYNTHID WATERMARK for AI attribution.\n",
      "\n",
      "APPLICATION\n",
      "\n",
      "This API is ideal for design, advertising, content creation, and\n",
      "prototyping. It offers customizable aspect ratios and prompt-based\n",
      "generation, making it a powerful tool for creative and commercial\n",
      "applications.\n",
      "\n",
      "This Python code snippet demonstrates how to generate an image with\n",
      "Imagen 3 using the Gemini API.\n",
      "\n",
      "from google import genai\n",
      "from google.genai import types\n",
      "from PIL import Image\n",
      "from io import BytesIO\n",
      "\n",
      "client = genai.Client(api_key='GEMINI_API_KEY')\n",
      "\n",
      "response = client.models.generate_images(\n",
      "    model='imagen-3.0-generate-002',\n",
      "    prompt='a portrait of a sheepadoodle wearing cape',\n",
      "    config=types.GenerateImagesConfig(\n",
      "        number_of_images=1,\n",
      "    )\n",
      ")\n",
      "for generated_image in response.generated_images:\n",
      "  image = Image.open(BytesIO(generated_image.image.image_bytes))\n",
      "  image.show()\n",
      "\n",
      "HOW WAS TODAY’S EMAIL?\n",
      "\n",
      "AWESOME    DECENT     NOT GREAT\n",
      "\n",
      "At Alpha Signal, our mission is to build a sharp, engaged community\n",
      "focused on AI, machine learning, and cutting-edge language models,\n",
      "helping over 200,000 developers stay informed and ahead. We’re\n",
      "passionate about curating the best in AI, from top research and\n",
      "trending technical blogs to expert insights and tailored job\n",
      "opportunities. We keep you connected to the breakthroughs and\n",
      "discussions that matter, so you can stay in the loop without endless\n",
      "searching. We also work closely with partners who value the future of\n",
      "AI, including employers and advertisers who want to reach an audience\n",
      "as passionate about AI as we are.\n",
      "\n",
      "Our partnerships are based on shared values of ethics, responsibility,\n",
      "and a commitment to building a better world through technology.Privacy\n",
      "is a priority at Alpha Signal. Our Privacy Policy clearly explains how\n",
      "we collect, store, and use your personal and non-personal information.\n",
      "By using our website, you accept these terms, which you can review on\n",
      "our website. This policy applies across all Alpha Signal pages,\n",
      "outlining your rights and how to contact us if you want to adjust the\n",
      "use of your information. We’re based in the United States. By using\n",
      "our site, you agree to be governed by U.S. laws.\n",
      "\n",
      "Looking to promote your company, product, service, or event to\n",
      "200,000+ AI developers? Let's work together.\n",
      "\n",
      "\t\tWORK WITH US [2] \n",
      "\n",
      "Stop receiving emails here [14]\n",
      "\n",
      "214 Barton Springs Rd, Austin, Texas, 78704, United States of America\n",
      "\n",
      " \n",
      "\n",
      "Links:\n",
      "------\n",
      "[1] https://link.alphasignal.ai/HWyWVm\n",
      "[2] https://link.alphasignal.ai/qBDJtP\n",
      "[3] https://link.alphasignal.ai/uIKFtp\n",
      "[4] https://araneoides.eomail1.com/web-version?ep=1&lc=0833873c-e381-11ef-a3f9-cbdf36f97402&p=686320f2-e586-11ef-8073-e1d644c8c090&pt=campaign&t=1738956006&s=500545ddc16f1bef398809bb750062650fa2e232d79ebc54d415c0bb5f955c45\n",
      "[5] https://link.alphasignal.ai/Z6dVYR\n",
      "[6] https://link.alphasignal.ai/Yrwvxf\n",
      "[7] https://link.alphasignal.ai/UF6bj7\n",
      "[8] https://link.alphasignal.ai/pE0RTm\n",
      "[9] https://link.alphasignal.ai/stKaZF\n",
      "[10] https://link.alphasignal.ai/qcV4LG\n",
      "[11] https://link.alphasignal.ai/5hUes6\n",
      "[12] https://link.alphasignal.ai/lKJLbL\n",
      "[13] https://link.alphasignal.ai/KDRgwL\n",
      "[14] https://araneoides.eomail1.com/unsubscribe?ep=1&l=9a4eb768-071d-11ef-b70b-19296a53e269&lc=0833873c-e381-11ef-a3f9-cbdf36f97402&p=686320f2-e586-11ef-8073-e1d644c8c090&pt=campaign&pv=4&spa=1738955841&t=1738956006&s=71a80f6481a811dc78ce58d6c2076abf8e33df42a7d6160853a52c1cf5863528\n"
     ]
    }
   ],
   "source": [
    "print(emails[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72da0416-f1d2-41a5-ac2e-2bf15d89aaf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T05:24:38.849861Z",
     "iopub.status.busy": "2025-02-08T05:24:38.849294Z",
     "iopub.status.idle": "2025-02-08T05:24:38.859555Z",
     "shell.execute_reply": "2025-02-08T05:24:38.857627Z",
     "shell.execute_reply.started": "2025-02-08T05:24:38.849828Z"
    }
   },
   "outputs": [],
   "source": [
    "SECTIONS = [\n",
    "    \"TOP NEWS\",\n",
    "    \"TRENDING SIGNALS\",\n",
    "    \"TOP PAPERS\",\n",
    "    \"PYTHON TIP\",\n",
    "]\n",
    "txt = emails[0]['text']\n",
    "sections = []\n",
    "for sec in SECTIONS:\n",
    "    _tmp = txt.split(sec)\n",
    "    sections.append(_tmp[0])\n",
    "    txt = _tmp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd8cf410-2094-4a7e-8273-1580098ce960",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T05:24:58.473927Z",
     "iopub.status.busy": "2025-02-08T05:24:58.473586Z",
     "iopub.status.idle": "2025-02-08T05:24:58.481545Z",
     "shell.execute_reply": "2025-02-08T05:24:58.480464Z",
     "shell.execute_reply.started": "2025-02-08T05:24:58.473886Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " \t*\n",
      "Mistral AI brings Le Chat to iOS, Android, and enterprise [5] with web\n",
      "search and automation tools.\n",
      "\n",
      "⚡️ \n",
      "\n",
      "\n",
      " \t*\n",
      "Andrew Ng presents Agentic Object Detection [6]: human-like object\n",
      "recognition via text-driven reasoning.\n",
      "\n",
      " \t*\n",
      "GitHub upgrades Copilot with agent mode [7], multi-file edits, and an\n",
      "upcoming autonomous coding agent.\n",
      "\n",
      " \t*\n",
      "Google releases Imagen 3 in Gemini API [8]: high quality, watermarked\n",
      "image generation with improved prompt adherence.\n",
      "\n",
      " \t*\n",
      "OpenAI refines CoT reasoning in O3-Mini [9] models, expanding\n",
      "visibility.\n",
      "\n",
      " \t*\n",
      "Pika Labs launches Pikadditions [10]: add any object or person to any\n",
      "video with ease.\n",
      "\n",
      "💻\n",
      "\n",
      "\n",
      " \t*\n",
      "Google AI's AlphaGeometry2 surpasses gold medalists [11] in Olympiad\n",
      "geometry, improving problem coverage and solving rates.\n",
      "\n",
      " \t*\n",
      "s1-32B improves test-time scaling [12] with budget forcing, boosting\n",
      "math reasoning and competition scores.\n",
      "\n",
      " \t*\n",
      "Multi-agent system reduces AI hallucinations [13]using structured\n",
      "reviews, NLP-based coordination, and new KPIs.\n",
      "\n",
      "🧠 \n"
     ]
    }
   ],
   "source": [
    "for pp in sections[1:]:\n",
    "    print(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7d5f887-6faa-4f38-bd85-6f0236e509c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T04:20:40.868554Z",
     "iopub.status.busy": "2025-02-10T04:20:40.868139Z",
     "iopub.status.idle": "2025-02-10T04:20:40.875727Z",
     "shell.execute_reply": "2025-02-10T04:20:40.874566Z",
     "shell.execute_reply.started": "2025-02-10T04:20:40.868523Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "French AI lab, Mistral AI updates its chatbot LE CHAT and makes it\n",
      "available on iOS, Android, and enterprise infrastructure. It\n",
      "introduces FLASH ANSWERS, A BUILT-IN CODE INTERPRETER, and REAL-TIME\n",
      "SEARCH, competing with ChatGPT, Claude, and DeepSeek.\n",
      "\n",
      "CORE FUNCTIONALITIES\n",
      "\n",
      "Le Chat provides tools for both general use and technical\n",
      "applications, offering:\n",
      "\n",
      " \t* OCR ENGINE: Processes PDFs, spreadsheets, and complex or\n",
      "low-quality files.\n",
      "\n",
      " \t* CODE INTERPRETER: Runs scripts, handles data analysis, and creates\n",
      "visualizations.\n",
      "\n",
      " \t* IMAGE GENERATION: Uses BLACK FOREST LABS’ FLUX ULTRA to generate\n",
      "photorealistic visuals.\n",
      "\n",
      " \t* REAL-TIME INFORMATION RETRIEVAL: Combines static pre-trained\n",
      "knowledge with updated data from web searches and news outlets.\n",
      "\n",
      " \t* Its MULTI-STEP AGENT FRAMEWORK lets you AUTOMATE WORKFLOWS by\n",
      "integrating with messaging systems and databases.\n",
      "\n",
      "PERFORMANCE\n",
      "\n",
      "Le Chat runs on Mistral’s latest models and introduces FLASH\n",
      "ANSWERS, a feature enabling RESPONSE SPEEDS OF UP TO 1,000 WORDS PER\n",
      "SECOND. It supports real-time web search with data from journalism\n",
      "sources and social platforms. The assistant processes text, images,\n",
      "and structured data files without external dependencies.\n",
      "\n",
      "COMPARISON WITH OTHER CHATBOTS\n",
      "\n",
      " \t* Faster than ChatGPT and Claude in response generation.\n",
      "\n",
      " \t* Stronger OCR and document parsing than Claude.\n",
      "\n",
      " \t* Code execution similar to ChatGPT Plus.\n",
      "\n",
      " \t* Web search supports social media and journalism sources.\n",
      "\n",
      " \t* Claude is known  for token efficiency, but Le Chat provides\n",
      "custom enterprise deployments and flexible infrastructure setups.\n",
      "\n",
      " \t* Le Chat adheres to EU privacy standards (GDPR).\n",
      "\n",
      "ENTERPRISE DEPLOYMENTS AND CUSTOMIZATION\n",
      "Le Chat targets enterprises with options for SAAS, ON-PREMISE, and VPC\n",
      "DEPLOYMENT. Teams can customize models and ensure on-site data\n",
      "security, making it a viable alternative to U.S. and Chinese\n",
      "providers.\n",
      "\n",
      "ACCESS AND AVAILABILITY\n",
      "\n",
      "You can find Le Chat available on web, iOS, and Android. It offers\n",
      "three tiers:\n",
      "\n",
      " \t* FREE PLAN: Limited usage with access to core features.\n",
      "\n",
      " \t* PRO PLAN: Faster responses, priority access, and extended usage\n",
      "limits.\n",
      "\n",
      " \t* ENTERPRISE: Custom deployment on ON-PREMISE OR VPC INFRASTRUCTURE,\n",
      "model fine-tuning, and API rate customization.\n",
      "\n",
      "\t\t\n"
     ]
    }
   ],
   "source": [
    "print(emails[0]['text'].split(\"TOP NEWS\")[2].split(\"WHAT'S NEW\")[1].split(\"TRENDING SIGNALS\")[0].split(\"TRY NOW\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b57101-0670-47c7-81d3-4821cb3fdcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f8a341-81e6-49c1-82ba-f1c99ad8626c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ea2b2-62e5-41a2-a626-66d55cb73efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b627033-16dd-4f5e-b91a-9175fe4ba95b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TLDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd4c716-ed21-4261-9701-b5149468e47b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T03:57:21.654149Z",
     "iopub.status.busy": "2024-12-18T03:57:21.653821Z",
     "iopub.status.idle": "2024-12-18T03:57:23.507015Z",
     "shell.execute_reply": "2024-12-18T03:57:23.505769Z",
     "shell.execute_reply.started": "2024-12-18T03:57:21.654111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails = gmail.fetch_emails(sender='TLDR AI <dan@tldrnewsletter.com>', after='2024-12-01', before='2024-12-07')\n",
    "len(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb76247-7448-4993-8188-ae5bf9c96c0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T03:57:23.509191Z",
     "iopub.status.busy": "2024-12-18T03:57:23.508722Z",
     "iopub.status.idle": "2024-12-18T03:57:23.516883Z",
     "shell.execute_reply": "2024-12-18T03:57:23.515611Z",
     "shell.execute_reply.started": "2024-12-18T03:57:23.509162Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sender': 'TLDR AI <dan@tldrnewsletter.com>', 'subject': \"OpenAI o1 System Card 📇, Perplexity Publisher Program 🔍, DeepMind's Genie 2 🧞\", 'date_utc': 'Fri, 6 Dec 2024 14:16:24 +0000', 'id': '1939c5434a9f7fd8', 'text': \"This report outlines the safety work carried out prior to releasing\\r\\nOpenAI o1 and o1-mini, including external red teaming and frontier\\r\\nrisk\\r\\nevaluation\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\u200c\\xa0\\r\\n\\r\\n\\r\\n Sign Up [1] |Advertise [2]|View Online [3] \\r\\n\\r\\n\\t\\tTLDR \\r\\n\\r\\n\\t\\tTOGETHER WITH [Writer] [4]\\r\\n\\r\\nTLDR AI 2024-12-06\\r\\n\\r\\n THE FASTEST WAY TO BUILD AI APPS (SPONSOR) [4] \\r\\n\\r\\n Writer is excited to introduce Writer AI Studio [4], the fastest way\\r\\nto build AI apps, products, and features. Writer's unique full-stack\\r\\ndesign makes it easy to prototype, deploy, and test AI apps [4] –\\r\\nallowing developers to build with APIs, a drag-and-drop open-source\\r\\nPython framework, or a no-code builder, so you have flexibility to\\r\\nbuild the way you want.\\r\\n\\r\\nWriter comes with a suite of top-ranking LLMs and has built-in RAG for\\r\\neasy integration with your data. Check it out if you're looking to\\r\\nstreamline how you build and integrate AI apps.\\r\\n\\r\\nStart building with Writer AI STUDIO. [4]\\r\\n\\r\\n🚀 \\r\\n\\r\\nHEADLINES & LAUNCHES\\r\\n\\r\\n OPENAI O1 SYSTEM CARD (3 MINUTE READ) [5] \\r\\n\\r\\n This report outlines the safety work carried out prior to releasing\\r\\nOpenAI o1 and o1-mini, including external red teaming and frontier\\r\\nrisk evaluations according to OpenAI's Preparedness Framework. \\r\\n\\r\\n PERPLEXITY EXPANDING IT'S PUBLISHER'S PROGRAM (4 MINUTE READ) [6] \\r\\n\\r\\n Perplexity has added over a dozen international news organizations to\\r\\nits Publishers' Program, offering tools, revenue sharing, and support\\r\\nto strengthen collaboration with global media. \\r\\n\\r\\n DEEPMIND'S GENIE 2 CAN GENERATE INTERACTIVE WORLDS THAT LOOK LIKE\\r\\nVIDEO GAMES (4 MINUTE READ) [7] \\r\\n\\r\\n DeepMind's Genie 2 is an advanced AI model that generates diverse,\\r\\ninteractive 3D worlds from images and text descriptions. The model is\\r\\nparticularly adept at simulating complex elements like physics and NPC\\r\\nbehavior, setting it apart from similar models by maintaining scene\\r\\nconsistency. Positioned as a research tool, Genie 2 is designed for\\r\\nprototyping interactive experiences and evaluating AI agents. \\r\\n\\r\\n🧠 \\r\\n\\r\\nRESEARCH & INNOVATION\\r\\n\\r\\n PALIGEMMA 2 (23 MINUTE READ) [8] \\r\\n\\r\\n Paligemma 2 is one of the best VLMs on the market today. It uses\\r\\nSigLIP and Gemma. \\r\\n\\r\\n LAND COVER MAPPING WITH ASANET (29 MINUTE READ) [9] \\r\\n\\r\\n The Asymmetric Semantic Aligning Network (ASANet) improves land cover\\r\\nclassification using both SAR and RGB images. \\r\\n\\r\\n FASTER MULTI-MODAL MODELS WITH TOKEN MERGING (14 MINUTE READ) [10] \\r\\n\\r\\n Researchers have developed a training-free method to make multi-modal\\r\\nlanguage models (LLMs) more efficient without sacrificing much\\r\\nperformance. Their approach reduces computational demands by up to 7\\r\\ntimes by smartly merging and pruning visual data tokens. \\r\\n\\r\\n🧑\\u200d💻 \\r\\n\\r\\nENGINEERING & RESOURCES\\r\\n\\r\\n RETHINKING CHANGE MANAGEMENT IN THE AGE OF AI (SPONSOR) [11] \\r\\n\\r\\n With the rise of AI and evolving customer expectations, businesses\\r\\nhave to move beyond temporary fixes and adopt a culture of continuous\\r\\ntransformation. Get the guide by Camunda [11] to find out how to align\\r\\nstakeholders, leverage technology and data, and embed a mindset of\\r\\ncontinuous change. Read the guide [11] \\r\\n\\r\\n GRAPHCAST (GITHUB REPO) [12] \\r\\n\\r\\n DeepMind has open sourced its GraphCast algorithm, which is\\r\\nsubstantially better and faster at localized weather prediction at 36\\r\\nhours. It also runs in a fraction of the time. \\r\\n\\r\\n CREATING OPTICAL ILLUSIONS (GITHUB REPO) [13] \\r\\n\\r\\n This project advances visual anagram generation - images that change\\r\\nappearance when flipped or rotated - by refining how diffusion models\\r\\nhandle multiple viewpoints. \\r\\n\\r\\n 3D SCENE COMPLETION FOR AUTONOMOUS VEHICLES (GITHUB REPO) [14] \\r\\n\\r\\n ScoreLiDAR is a new method that speeds up 3D LiDAR scene completion\\r\\nfor autonomous vehicles. \\r\\n\\r\\n🎁 \\r\\n\\r\\nMISCELLANEOUS\\r\\n\\r\\n NEW FISH AUDIO MODEL (3 MINUTE READ) [15] \\r\\n\\r\\n The newest Fish Audio 1.5 is currently #2 on Text To Speech\\r\\nLeaderboards behind ElevenLabs. It supports voice cloning and is fast\\r\\nto run, although the output quality seems hit or miss. \\r\\n\\r\\n OPENAI PARTNERS WITH ANDURIL (5 MINUTE READ) [16] \\r\\n\\r\\n OpenAI and defense company Anduril have partnered to create a\\r\\nnational security-based AI. \\r\\n\\r\\n KEY LEADERS BEHIND GOOGLE'S VIRAL NOTEBOOKLM ARE LEAVING TO CREATE\\r\\nTHEIR OWN STARTUP (2 MINUTE READ) [17] \\r\\n\\r\\n Three key Google NotebookLM members have left to start a new stealth\\r\\nAI venture. The startup aims to leverage the latest AI models to\\r\\ncreate consumer-facing, user-first AI products. It remains in its\\r\\nearly stages without a clear focus or announced funding. \\r\\n\\r\\n⚡ \\r\\n\\r\\nQUICK LINKS\\r\\n\\r\\n HOW CITI, RIPJAR, AND CLOUDFACTORY ARE USING AI IRL (SPONSOR) [18] \\r\\n\\r\\n AI in the real world isn't just ChatGPT. It's building robust systems\\r\\nwith firm guardrails and solving practical challenges around\\r\\ninference, data, and models. Watch the recorded presentation from\\r\\nAI_IRL London. [18] \\r\\n\\r\\n DEEPTHOUGHT 8B REASONING MODEL (HUGGING FACE HUB) [19] \\r\\n\\r\\n Ruliad has released a small reasoning model that takes extra time to\\r\\nthink through problems. \\r\\n\\r\\n GUI AGENTS (2 MINUTE READ) [20] \\r\\n\\r\\n A collection of research papers and projects on large language\\r\\nmodel-brained GUI agents. \\r\\n\\r\\n SAM ALTMAN SAYS ARTIFICIAL GENERAL INTELLIGENCE IS ON THE HORIZON (1\\r\\nMINUTE READ) [21] \\r\\n\\r\\n Sam Altman, CEO of OpenAI, stated at the DealBook Summit that\\r\\nArtificial General Intelligence might impact everyday life less\\r\\nsignificantly than expected. \\r\\n\\r\\nWant to advertise in TLDR? 📰\\r\\n\\r\\n If your company is interested in reaching an audience of AI\\r\\nprofessionals and decision makers, you may want to ADVERTISE WITH US\\r\\n[22]. \\r\\n\\r\\n If you have any comments or feedback, just respond to this email! \\r\\n\\r\\nThanks for reading, \\r\\nAndrew Tan & Andrew Carr \\r\\n\\r\\nIf you don't want to receive future editions of TLDR AI, please\\r\\nunsubscribe from TLDR AI [23] or manage all of your TLDR newsletter\\r\\nsubscriptions [24]. \\r\\n\\r\\n \\r\\n\\r\\nLinks:\\r\\n------\\r\\n[1] https://tldr.tech/ai?utm_source=tldrai\\r\\n[2] https://advertise.tldr.tech/?utm_source=tldrai&utm_medium=newsletter&utm_campaign=advertisetopnav\\r\\n[3] https://a.tldrnewsletter.com/web-version?ep=1&lc=57aff87c-ae02-11ef-9074-4b7e3c69f0c6&p=68d03084-b3c7-11ef-928d-09d27092d252&pt=campaign&t=1733494584&s=a94ebb372672f7844f8d11fcad3029c68791b13faf831d296ca3c8f639256320\\r\\n[4] https://writer.com/product/ai-studio/?utm_source=tldr&utm_medium=newsletter&utm_campaign=ai_studio\\r\\n[5] https://links.tldrnewsletter.com/xS4pwo\\r\\n[6] https://links.tldrnewsletter.com/UUkpwY\\r\\n[7] https://techcrunch.com/2024/12/04/deepminds-genie-2-can-generate-interactive-worlds-that-look-like-video-games/?utm_source=tldrai\\r\\n[8] https://arxiv.org/abs/2412.03555?utm_source=tldrai\\r\\n[9] https://arxiv.org/abs/2412.02044v1?utm_source=tldrai\\r\\n[10] https://arxiv.org/abs/2412.03248v1?utm_source=tldrai\\r\\n[11] https://page.camunda.com/wp-rethinking-change-management?utm_medium=paid_leadgen&utm_source=tldr&utm_campaign=Guide.RethinkingChangeManagement.24Q4.EN&utm_content=Q4program\\r\\n[12] https://github.com/google-deepmind/graphcast?utm_source=tldrai\\r\\n[13] https://github.com/pixtella/anagram-mtl?utm_source=tldrai\\r\\n[14] https://github.com/happyw1nd/scorelidar?utm_source=tldrai\\r\\n[15] https://threadreaderapp.com/thread/1864370933496205728.html?utm_source=tldrai\\r\\n[16] https://www.cnbc.com/2024/12/04/openai-partners-with-defense-company-anduril.html?utm_source=tldrai\\r\\n[17] https://techcrunch.com/2024/12/04/key-leaders-behind-googles-viral-notebooklm-are-leaving-to-create-their-own-startup/?utm_source=tldrai\\r\\n[18] https://www.cloudfactory.com/webinar/ai-irl?utm_campaign=Core_AP_EG&utm_source=tldr&utm_medium=mediapartner&utm_content=email_quicklinks\\r\\n[19] https://huggingface.co/ruliad/deepthought-8b-llama-v0.01-alpha?utm_source=tldrai\\r\\n[20] https://vyokky.github.io/LLM-Brained-GUI-Agents-Survey/?utm_source=tldrai\\r\\n[21] https://www.nytimes.com/video/business/100000009858580/sam-altman-openai-dealbook.html?utm_source=tldrai\\r\\n[22] https://advertise.tldr.tech/?utm_source=tldrai&utm_medium=newsletter&utm_campaign=advertisecta\\r\\n[23] https://a.tldrnewsletter.com/unsubscribe?ep=1&l=eedf6b14-3de3-11ed-9a32-0241b9615763&lc=57aff87c-ae02-11ef-9074-4b7e3c69f0c6&p=68d03084-b3c7-11ef-928d-09d27092d252&pt=campaign&pv=4&spa=1733493669&t=1733494584&s=ea38715a39323501a5e9b28f17e7c39f0df3e5d14a1e4155a2a433bd6fecc396\\r\\n[24] https://tldr.tech/ai/manage?email=automatic.newsletters%40gmail.com\"}\n"
     ]
    }
   ],
   "source": [
    "print(emails[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "307c5c9b-b998-49d7-9d07-e626f16174c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T03:57:23.518900Z",
     "iopub.status.busy": "2024-12-18T03:57:23.518363Z",
     "iopub.status.idle": "2024-12-18T03:57:26.538013Z",
     "shell.execute_reply": "2024-12-18T03:57:26.537032Z",
     "shell.execute_reply.started": "2024-12-18T03:57:23.518837Z"
    }
   },
   "outputs": [],
   "source": [
    "news_sources = tldr_parser(emails[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71e839c4-5710-4f8b-86f0-910533d57a28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T03:57:26.543033Z",
     "iopub.status.busy": "2024-12-18T03:57:26.542764Z",
     "iopub.status.idle": "2024-12-18T03:57:26.553985Z",
     "shell.execute_reply": "2024-12-18T03:57:26.552249Z",
     "shell.execute_reply.started": "2024-12-18T03:57:26.543011Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'OPENAI O1 SYSTEM CARD',\n",
       "  'url': 'https://openai.com/index/openai-o1-system-card/',\n",
       "  'source_of_the_news': 'TLDR AI <dan@tldrnewsletter.com>',\n",
       "  'news_summary': \"This report outlines the safety work carried out prior to releasing OpenAI o1 and o1-mini, including external red teaming and frontier risk evaluations according to OpenAI's Preparedness Framework.\",\n",
       "  'date_source': 'Fri, 6 Dec 2024 14:16:24 +0000',\n",
       "  'new_provider': 'openai.com'},\n",
       " {'title': \"PERPLEXITY EXPANDING IT'S PUBLISHER'S PROGRAM\",\n",
       "  'url': 'https://www.perplexity.ai/hub/blog/perplexity-expands-publisher-program-with-15-new-media-partners',\n",
       "  'source_of_the_news': 'TLDR AI <dan@tldrnewsletter.com>',\n",
       "  'news_summary': \"Perplexity has added over a dozen international news organizations to its Publishers' Program, offering tools, revenue sharing, and support to strengthen collaboration with global media.\",\n",
       "  'date_source': 'Fri, 6 Dec 2024 14:16:24 +0000',\n",
       "  'new_provider': 'www.perplexity.ai'},\n",
       " {'title': \"DEEPMIND'S GENIE 2 CAN GENERATE INTERACTIVE WORLDS THAT LOOK LIKE\\r\\nVIDEO GAMES\",\n",
       "  'url': 'https://techcrunch.com/2024/12/04/deepminds-genie-2-can-generate-interactive-worlds-that-look-like-video-games/',\n",
       "  'source_of_the_news': 'TLDR AI <dan@tldrnewsletter.com>',\n",
       "  'news_summary': \"DeepMind's Genie 2 is an advanced AI model that generates diverse, interactive 3D worlds from images and text descriptions. The model is particularly adept at simulating complex elements like physics and NPC behavior, setting it apart from similar models by maintaining scene consistency. Positioned as a research tool, Genie 2 is designed for prototyping interactive experiences and evaluating AI agents.\",\n",
       "  'date_source': 'Fri, 6 Dec 2024 14:16:24 +0000',\n",
       "  'new_provider': 'techcrunch.com'},\n",
       " {'title': 'PALIGEMMA 2',\n",
       "  'url': 'https://arxiv.org/abs/2412.03555',\n",
       "  'source_of_the_news': 'TLDR AI <dan@tldrnewsletter.com>',\n",
       "  'news_summary': 'Paligemma 2 is one of the best VLMs on the market today. It uses SigLIP and Gemma.',\n",
       "  'date_source': 'Fri, 6 Dec 2024 14:16:24 +0000',\n",
       "  'new_provider': 'arxiv.org'},\n",
       " {'title': 'LAND COVER MAPPING WITH ASANET',\n",
       "  'url': 'https://arxiv.org/abs/2412.02044v1',\n",
       "  'source_of_the_news': 'TLDR AI <dan@tldrnewsletter.com>',\n",
       "  'news_summary': 'The Asymmetric Semantic Aligning Network (ASANet) improves land cover classification using both SAR and RGB images.',\n",
       "  'date_source': 'Fri, 6 Dec 2024 14:16:24 +0000',\n",
       "  'new_provider': 'arxiv.org'},\n",
       " {'title': 'FASTER MULTI-MODAL MODELS WITH TOKEN MERGING',\n",
       "  'url': 'https://arxiv.org/abs/2412.03248v1',\n",
       "  'source_of_the_news': 'TLDR AI <dan@tldrnewsletter.com>',\n",
       "  'news_summary': 'Researchers have developed a training-free method to make multi-modal language models (LLMs) more efficient without sacrificing much performance. Their approach reduces computational demands by up to 7 times by smartly merging and pruning visual data tokens.',\n",
       "  'date_source': 'Fri, 6 Dec 2024 14:16:24 +0000',\n",
       "  'new_provider': 'arxiv.org'},\n",
       " {'title': 'NEW FISH AUDIO MODEL',\n",
       "  'url': 'https://threadreaderapp.com/thread/1864370933496205728.html',\n",
       "  'source_of_the_news': 'TLDR AI <dan@tldrnewsletter.com>',\n",
       "  'news_summary': 'The newest Fish Audio 1.5 is currently #2 on Text To Speech Leaderboards behind ElevenLabs. It supports voice cloning and is fast to run, although the output quality seems hit or miss.',\n",
       "  'date_source': 'Fri, 6 Dec 2024 14:16:24 +0000',\n",
       "  'new_provider': 'threadreaderapp.com'},\n",
       " {'title': 'OPENAI PARTNERS WITH ANDURIL',\n",
       "  'url': 'https://www.cnbc.com/2024/12/04/openai-partners-with-defense-company-anduril.html',\n",
       "  'source_of_the_news': 'TLDR AI <dan@tldrnewsletter.com>',\n",
       "  'news_summary': 'OpenAI and defense company Anduril have partnered to create a national security-based AI.',\n",
       "  'date_source': 'Fri, 6 Dec 2024 14:16:24 +0000',\n",
       "  'new_provider': 'www.cnbc.com'},\n",
       " {'title': \"KEY LEADERS BEHIND GOOGLE'S VIRAL NOTEBOOKLM ARE LEAVING TO CREATE\\r\\nTHEIR OWN STARTUP\",\n",
       "  'url': 'https://techcrunch.com/2024/12/04/key-leaders-behind-googles-viral-notebooklm-are-leaving-to-create-their-own-startup/',\n",
       "  'source_of_the_news': 'TLDR AI <dan@tldrnewsletter.com>',\n",
       "  'news_summary': 'Three key Google NotebookLM members have left to start a new stealth AI venture. The startup aims to leverage the latest AI models to create consumer-facing, user-first AI products. It remains in its early stages without a clear focus or announced funding.',\n",
       "  'date_source': 'Fri, 6 Dec 2024 14:16:24 +0000',\n",
       "  'new_provider': 'techcrunch.com'},\n",
       " {'title': 'GUI AGENTS',\n",
       "  'url': 'https://vyokky.github.io/LLM-Brained-GUI-Agents-Survey/',\n",
       "  'source_of_the_news': 'TLDR AI <dan@tldrnewsletter.com>',\n",
       "  'news_summary': 'A collection of research papers and projects on large language model-brained GUI agents.',\n",
       "  'date_source': 'Fri, 6 Dec 2024 14:16:24 +0000',\n",
       "  'new_provider': 'vyokky.github.io'},\n",
       " {'title': 'SAM ALTMAN SAYS ARTIFICIAL GENERAL INTELLIGENCE IS ON THE HORIZON',\n",
       "  'url': 'https://www.nytimes.com/video/business/100000009858580/sam-altman-openai-dealbook.html',\n",
       "  'source_of_the_news': 'TLDR AI <dan@tldrnewsletter.com>',\n",
       "  'news_summary': 'Sam Altman, CEO of OpenAI, stated at the DealBook Summit that Artificial General Intelligence might impact everyday life less significantly than expected.',\n",
       "  'date_source': 'Fri, 6 Dec 2024 14:16:24 +0000',\n",
       "  'new_provider': 'www.nytimes.com'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d525e49-4da2-45e1-9464-9534da083d2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T03:57:26.556760Z",
     "iopub.status.busy": "2024-12-18T03:57:26.556125Z",
     "iopub.status.idle": "2024-12-18T03:57:26.569026Z",
     "shell.execute_reply": "2024-12-18T03:57:26.567518Z",
     "shell.execute_reply.started": "2024-12-18T03:57:26.556731Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Links:',\n",
       " '------',\n",
       " '[1] https://tldr.tech/ai?utm_source=tldrai',\n",
       " '[2] https://advertise.tldr.tech/?utm_source=tldrai&utm_medium=newsletter&utm_campaign=advertisetopnav',\n",
       " '[3] https://a.tldrnewsletter.com/web-version?ep=1&lc=57aff87c-ae02-11ef-9074-4b7e3c69f0c6&p=68d03084-b3c7-11ef-928d-09d27092d252&pt=campaign&t=1733494584&s=a94ebb372672f7844f8d11fcad3029c68791b13faf831d296ca3c8f639256320',\n",
       " '[4] https://writer.com/product/ai-studio/?utm_source=tldr&utm_medium=newsletter&utm_campaign=ai_studio',\n",
       " '[5] https://links.tldrnewsletter.com/xS4pwo',\n",
       " '[6] https://links.tldrnewsletter.com/UUkpwY',\n",
       " '[7] https://techcrunch.com/2024/12/04/deepminds-genie-2-can-generate-interactive-worlds-that-look-like-video-games/?utm_source=tldrai',\n",
       " '[8] https://arxiv.org/abs/2412.03555?utm_source=tldrai',\n",
       " '[9] https://arxiv.org/abs/2412.02044v1?utm_source=tldrai',\n",
       " '[10] https://arxiv.org/abs/2412.03248v1?utm_source=tldrai',\n",
       " '[11] https://page.camunda.com/wp-rethinking-change-management?utm_medium=paid_leadgen&utm_source=tldr&utm_campaign=Guide.RethinkingChangeManagement.24Q4.EN&utm_content=Q4program',\n",
       " '[12] https://github.com/google-deepmind/graphcast?utm_source=tldrai',\n",
       " '[13] https://github.com/pixtella/anagram-mtl?utm_source=tldrai',\n",
       " '[14] https://github.com/happyw1nd/scorelidar?utm_source=tldrai',\n",
       " '[15] https://threadreaderapp.com/thread/1864370933496205728.html?utm_source=tldrai',\n",
       " '[16] https://www.cnbc.com/2024/12/04/openai-partners-with-defense-company-anduril.html?utm_source=tldrai',\n",
       " '[17] https://techcrunch.com/2024/12/04/key-leaders-behind-googles-viral-notebooklm-are-leaving-to-create-their-own-startup/?utm_source=tldrai',\n",
       " '[18] https://www.cloudfactory.com/webinar/ai-irl?utm_campaign=Core_AP_EG&utm_source=tldr&utm_medium=mediapartner&utm_content=email_quicklinks',\n",
       " '[19] https://huggingface.co/ruliad/deepthought-8b-llama-v0.01-alpha?utm_source=tldrai',\n",
       " '[20] https://vyokky.github.io/LLM-Brained-GUI-Agents-Survey/?utm_source=tldrai',\n",
       " '[21] https://www.nytimes.com/video/business/100000009858580/sam-altman-openai-dealbook.html?utm_source=tldrai',\n",
       " '[22] https://advertise.tldr.tech/?utm_source=tldrai&utm_medium=newsletter&utm_campaign=advertisecta',\n",
       " '[23] https://a.tldrnewsletter.com/unsubscribe?ep=1&l=eedf6b14-3de3-11ed-9a32-0241b9615763&lc=57aff87c-ae02-11ef-9074-4b7e3c69f0c6&p=68d03084-b3c7-11ef-928d-09d27092d252&pt=campaign&pv=4&spa=1733493669&t=1733494584&s=ea38715a39323501a5e9b28f17e7c39f0df3e5d14a1e4155a2a433bd6fecc396',\n",
       " '[24] https://tldr.tech/ai/manage?email=automatic.newsletters%40gmail.com']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = emails[0]['text'].split(\"\\r\\n\\r\\n\")[-1].split('\\r\\n')\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "080e2f7e-682d-4d52-9777-b78f90bc6dbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T04:10:17.868124Z",
     "iopub.status.busy": "2024-12-18T04:10:17.867091Z",
     "iopub.status.idle": "2024-12-18T04:10:17.883057Z",
     "shell.execute_reply": "2024-12-18T04:10:17.881271Z",
     "shell.execute_reply.started": "2024-12-18T04:10:17.868092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"tldr\" in links[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d795687-19b3-476c-bd8a-545f6079d5ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T03:57:26.571641Z",
     "iopub.status.busy": "2024-12-18T03:57:26.571060Z",
     "iopub.status.idle": "2024-12-18T03:57:26.591964Z",
     "shell.execute_reply": "2024-12-18T03:57:26.589583Z",
     "shell.execute_reply.started": "2024-12-18T03:57:26.571606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[9', 'https://arxiv.org/abs/2412.02044v1?utm_source=tldrai']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[10].split('] ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcaa5ebe-e59f-4538-b4d4-7245bc06d41c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T03:58:17.160985Z",
     "iopub.status.busy": "2024-12-18T03:58:17.159327Z",
     "iopub.status.idle": "2024-12-18T03:58:17.170409Z",
     "shell.execute_reply": "2024-12-18T03:58:17.168798Z",
     "shell.execute_reply.started": "2024-12-18T03:58:17.160949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, 'https://openai.com/index/openai-o1-system-card/')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(news_sources[0]['url']), news_sources[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb4e07-652d-45d2-978b-97d4d61a6f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
