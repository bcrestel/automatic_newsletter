{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e739d28b-747d-4409-935d-94010b1ad9cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:55:34.883382Z",
     "iopub.status.busy": "2025-06-20T02:55:34.883052Z",
     "iopub.status.idle": "2025-06-20T02:55:34.910026Z",
     "shell.execute_reply": "2025-06-20T02:55:34.902161Z",
     "shell.execute_reply.started": "2025-06-20T02:55:34.883308Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s -- l.%(lineno)d: %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d5207a-3bc5-4431-8317-fbd0d9bea875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:55:34.923885Z",
     "iopub.status.busy": "2025-06-20T02:55:34.923449Z",
     "iopub.status.idle": "2025-06-20T02:55:44.957991Z",
     "shell.execute_reply": "2025-06-20T02:55:44.957409Z",
     "shell.execute_reply.started": "2025-06-20T02:55:34.923857Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:55:39,065 - httpx - INFO -- l.1025: HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.newsletters.main import runner as runner_newsletters\n",
    "from src.scoring.main import runner as runner_scoring\n",
    "from src.reporting.report import Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b15ea69-48b9-4128-b8c6-90c3765c8cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:55:45.067032Z",
     "iopub.status.busy": "2025-06-20T02:55:44.958861Z",
     "iopub.status.idle": "2025-06-20T02:56:35.888626Z",
     "shell.execute_reply": "2025-06-20T02:56:35.887829Z",
     "shell.execute_reply.started": "2025-06-20T02:55:45.066993Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:55:45,074 - googleapiclient.discovery_cache - INFO -- l.49: file_cache is only supported with oauth2client<4.0.0\n",
      "2025-06-19 22:55:45,079 - src.newsletters.main - INFO -- l.32: Query emails\n",
      "2025-06-19 22:55:47,124 - src.newsletters.main - INFO -- l.38: Using 6 sender(s): dict_keys(['TLDR AI <dan@tldrnewsletter.com>', 'AlphaSignal <news@alphasignal.ai>', 'TLDR <dan@tldrnewsletter.com>', 'TLDR Product <dan@tldrnewsletter.com>', 'Bloomberg Technology <noreply@news.bloomberg.com>', 'TechCrunch <newsletters@techcrunch.com>']).\n",
      "2025-06-19 22:55:47,125 - src.newsletters.main - INFO -- l.40: Found 2 emails from sender TLDR AI <dan@tldrnewsletter.com>\n",
      "2025-06-19 22:55:47,128 - src.newsletters.main - INFO -- l.40: Found 0 emails from sender AlphaSignal <news@alphasignal.ai>\n",
      "2025-06-19 22:55:47,130 - src.newsletters.main - INFO -- l.40: Found 2 emails from sender TLDR <dan@tldrnewsletter.com>\n",
      "2025-06-19 22:55:47,131 - src.newsletters.main - INFO -- l.40: Found 1 emails from sender TLDR Product <dan@tldrnewsletter.com>\n",
      "2025-06-19 22:55:47,133 - src.newsletters.main - INFO -- l.40: Found 0 emails from sender Bloomberg Technology <noreply@news.bloomberg.com>\n",
      "2025-06-19 22:55:47,133 - src.newsletters.main - INFO -- l.40: Found 0 emails from sender TechCrunch <newsletters@techcrunch.com>\n",
      "2025-06-19 22:55:47,134 - src.newsletters.main - INFO -- l.43: Parsing emails to extract news stories\n",
      "2025-06-19 22:55:47,135 - src.newsletters.parser.tldr - INFO -- l.26: Parsing email 'ChatGPT ads üì∞, online shopping AI agents üõçÔ∏è, Marc Benioff on AI ü§ñ'\n",
      "2025-06-19 22:55:49,486 - src.newsletters.parser.tldr - INFO -- l.26: Parsing email 'Elon Musk vs. OpenAI ‚öîÔ∏è, Perplexity hardware ü§ñ, INTELLECT-1 Release 1Ô∏è‚É£'\n",
      "2025-06-19 22:55:51,771 - src.newsletters.parser.tldr - INFO -- l.26: Parsing email 'SpaceX eyes $350B valuation üí∞, Intel CEO resigns üíº, Browser Company's AI demo üåé'\n",
      "2025-06-19 22:55:55,242 - src.newsletters.parser.tldr - INFO -- l.26: Parsing email 'Tesla's next-gen self driving üöó, US killed Meta crypto ü™ô, the rise of PIPs üíº'\n",
      "2025-06-19 22:55:58,271 - src.newsletters.parser.tldr - INFO -- l.26: Parsing email 'Levels of data proficiency ü™ú, product manager vs owner üíª, skeptoptimism üí°'\n",
      "2025-06-19 22:56:00,054 - src.newsletters.main - INFO -- l.57: Newsletters block complete. Found 60 news stories in total.\n",
      "2025-06-19 22:56:00,055 - src.scoring.main - INFO -- l.24: Converting all news stories to a structured format.\n",
      "2025-06-19 22:56:00,769 - src.scoring.main - INFO -- l.28: Querying the metadata tags from Google Sheets.\n",
      "2025-06-19 22:56:00,772 - googleapiclient.discovery_cache - INFO -- l.49: file_cache is only supported with oauth2client<4.0.0\n"
     ]
    }
   ],
   "source": [
    "report_date_range = ['2024-12-01', '2024-12-03']\n",
    "news_stories, parser_error = runner_newsletters(after=report_date_range[0], before=report_date_range[1])\n",
    "df_news_stories, target_fields = runner_scoring(news_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aade827-6665-4117-a5cb-9cae8271b823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:56:35.890342Z",
     "iopub.status.busy": "2025-06-20T02:56:35.889383Z",
     "iopub.status.idle": "2025-06-20T02:56:35.901831Z",
     "shell.execute_reply": "2025-06-20T02:56:35.900392Z",
     "shell.execute_reply.started": "2025-06-20T02:56:35.890301Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:35,892 - src.reporting.report - INFO -- l.66: Will generate report for range 2024-12-01 to 2024-12-03\n",
      "2025-06-19 22:56:35,893 - src.reporting.report - INFO -- l.294: Target fields were successfully verified.\n",
      "2025-06-19 22:56:35,894 - src.reporting.report - INFO -- l.254: We'll use the news stories provided by the dataframe.\n",
      "2025-06-19 22:56:35,895 - src.reporting.report - INFO -- l.80: Found 60 candidate news stories for this report.\n"
     ]
    }
   ],
   "source": [
    "report = Report(\n",
    "    df_scored_news_stories=df_news_stories, \n",
    "    target_fields=target_fields, \n",
    "    report_date_range=report_date_range,\n",
    "    score_col=\"score_category_count\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d53fd88e-5f31-4ce7-a208-c96139a0fcbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T04:49:45.104935Z",
     "iopub.status.busy": "2025-02-05T04:49:45.103642Z",
     "iopub.status.idle": "2025-02-05T04:49:45.121674Z",
     "shell.execute_reply": "2025-02-05T04:49:45.119054Z",
     "shell.execute_reply.started": "2025-02-05T04:49:45.104905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nreport.df_ns.loc[len(report.df_ns)] = {\"title\": \"Fake News!!!\", \"news_provider\": \"mainstream media Inc.\", \"source_of_the_news\": \"Orange Clown\",\\n                                       \"news_summary\": \"This is a summary\",\\n                                       \"competitive_intelligence\": [\"Big Bank\"], \"themes\": [], \"market_intelligence\": []}\\nreport.df_ns.loc[len(report.df_ns)] = {\"title\": \"Swasticar 101\", \"news_provider\": \"X\", \"source_of_the_news\": \"Musketeer\", \\n                                       \"news_summary\": \"This is another summary, much longer. Woh!\",\\n                                       \"competitive_intelligence\": [\"Big Competitor\"], \"themes\": [], \"market_intelligence\": []}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "report.df_ns.loc[len(report.df_ns)] = {\"title\": \"Fake News!!!\", \"news_provider\": \"mainstream media Inc.\", \"source_of_the_news\": \"Orange Clown\",\n",
    "                                       \"news_summary\": \"This is a summary\",\n",
    "                                       \"competitive_intelligence\": [\"Big Bank\"], \"themes\": [], \"market_intelligence\": []}\n",
    "report.df_ns.loc[len(report.df_ns)] = {\"title\": \"Swasticar 101\", \"news_provider\": \"X\", \"source_of_the_news\": \"Musketeer\", \n",
    "                                       \"news_summary\": \"This is another summary, much longer. Woh!\",\n",
    "                                       \"competitive_intelligence\": [\"Big Competitor\"], \"themes\": [], \"market_intelligence\": []}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c988ab-1c4b-44e8-87c6-f9bb65e29fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:08:01.924881Z",
     "iopub.status.busy": "2025-06-19T19:08:01.924520Z",
     "iopub.status.idle": "2025-06-19T19:08:02.109279Z",
     "shell.execute_reply": "2025-06-19T19:08:02.108364Z",
     "shell.execute_reply.started": "2025-06-19T19:08:01.924857Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>news_provider</th>\n",
       "      <th>source_of_the_news</th>\n",
       "      <th>text</th>\n",
       "      <th>news_summary</th>\n",
       "      <th>date_source</th>\n",
       "      <th>date_source_time_zone</th>\n",
       "      <th>version</th>\n",
       "      <th>competitive_intelligence</th>\n",
       "      <th>themes</th>\n",
       "      <th>market_intelligence</th>\n",
       "      <th>personalities</th>\n",
       "      <th>score_category_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADS MIGHT BE COMING TO CHATGPT</td>\n",
       "      <td>https://techcrunch.com/2024/12/02/ads-might-be...</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>OpenAI is considering an advertising business ...</td>\n",
       "      <td>2024-12-03 09:13:10-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI]</td>\n",
       "      <td>[OpenAI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE RACE IS ON TO MAKE AI AGENTS DO YOUR ONLIN...</td>\n",
       "      <td>https://techcrunch.com/2024/12/02/the-race-is-...</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Tech companies are developing AI shopping agen...</td>\n",
       "      <td>2024-12-03 09:13:10-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Agents]</td>\n",
       "      <td>[OpenAI, Google, Amazon, Perplexity]</td>\n",
       "      <td>[]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SALESFORCE CEO MARC BENIOFF HAS THOUGHTS ON AI...</td>\n",
       "      <td>https://www.bigtechnology.com/p/salesforce-ceo...</td>\n",
       "      <td>www.bigtechnology.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Salesforce CEO Marc Benioff envisions companie...</td>\n",
       "      <td>2024-12-03 09:13:10-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Agents]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEMO: DECOUPLED MOMENTUM OPTIMIZATION</td>\n",
       "      <td>https://arxiv.org/abs/2411.19870</td>\n",
       "      <td>arxiv.org</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>2.5x faster, 100x less communication volume, a...</td>\n",
       "      <td>2024-12-03 09:13:10-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Model, Model Training]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REWARD HACKING</td>\n",
       "      <td>https://lilianweng.github.io/posts/2024-11-28-...</td>\n",
       "      <td>lilianweng.github.io</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Lilian Weng has another great blog post discus...</td>\n",
       "      <td>2024-12-03 09:13:10-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Model, Model Training]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OPENAI TARGETS 1BN USERS IN NEXT PHASE OF GROWTH</td>\n",
       "      <td>https://www.ft.com/content/e91cb018-873c-4388-...</td>\n",
       "      <td>www.ft.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>OpenAI aims to reach 1 billion users by launch...</td>\n",
       "      <td>2024-12-03 09:13:10-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Agents]</td>\n",
       "      <td>[OpenAI, Google, Microsoft]</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GOOGLE'S PLAN TO KEEP AI OUT OF SEARCH TRIAL R...</td>\n",
       "      <td>https://arstechnica.com/tech-policy/2024/11/go...</td>\n",
       "      <td>arstechnica.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>US District Judge Amit Mehta suggests AI could...</td>\n",
       "      <td>2024-12-03 09:13:10-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Funding]</td>\n",
       "      <td>[OpenAI, Google, Microsoft]</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UV GUIDE TO PYTORCH</td>\n",
       "      <td>https://docs.astral.sh/uv/guides/integration/p...</td>\n",
       "      <td>docs.astral.sh</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Documentation on how to use the new package ma...</td>\n",
       "      <td>2024-12-03 09:13:10-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AI COMPANY MISTRAL IS LATEST EUROPEAN STARTUP ...</td>\n",
       "      <td>https://www.semafor.com/article/11/27/2024/ai-...</td>\n",
       "      <td>www.semafor.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>European AI startup Mistral, known for open-we...</td>\n",
       "      <td>2024-12-03 09:13:10-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI]</td>\n",
       "      <td>[Mistral AI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OPENAI GETS NEW $1.5 BILLION INVESTMENT FROM S...</td>\n",
       "      <td>https://www.cnbc.com/2024/11/26/openai-gets-1p...</td>\n",
       "      <td>www.cnbc.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>OpenAI is allowing employees to sell $1.5 bill...</td>\n",
       "      <td>2024-12-03 09:13:10-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Funding]</td>\n",
       "      <td>[OpenAI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ELON MUSK SEEKS TO BLOCK OPENAI'S FOR-PROFIT C...</td>\n",
       "      <td>https://www.theverge.com/2024/11/30/24309697/e...</td>\n",
       "      <td>www.theverge.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Elon Musk's legal team has filed a motion to p...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Legal]</td>\n",
       "      <td>[OpenAI, Twitter]</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PERPLEXITY MULLS GETTING INTO HARDWARE</td>\n",
       "      <td>https://techcrunch.com/2024/11/26/perplexity-m...</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Perplexity's CEO plans to develop a \"simple, u...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Multimodal, Funding]</td>\n",
       "      <td>[Perplexity]</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>INFLECTION AI CEO SAYS IT'S DONE TRYING TO MAK...</td>\n",
       "      <td>https://techcrunch.com/2024/11/26/inflection-c...</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Inflection AI shifted its focus from developin...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Model, Funding]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>INTELLECT-1 RELEASE: THE FIRST GLOBALLY TRAINE...</td>\n",
       "      <td>https://www.primeintellect.ai/blog/intellect-1...</td>\n",
       "      <td>www.primeintellect.ai</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>INTELLECT-1 is a 10B parameter model trained o...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Model Training, Evaluation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DETECT AND LEARN UNSEEN OBJECTS</td>\n",
       "      <td>https://arxiv.org/abs/2411.18207v1</td>\n",
       "      <td>arxiv.org</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>This new framework pushes object detection int...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Multimodal]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MAKING UNDERWATER IMAGES CLEAR</td>\n",
       "      <td>https://arxiv.org/abs/2411.18296v1</td>\n",
       "      <td>arxiv.org</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>HUPE is an AI-powered method that improves und...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Multimodal]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MAPPING THE IONOSPHERE WITH THE POWER OF ANDROID</td>\n",
       "      <td>https://research.google/blog/mapping-the-ionos...</td>\n",
       "      <td>research.google</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Google researchers were able to accurately map...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Google]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>INTRODUCING LTNTORCH</td>\n",
       "      <td>https://arxiv.org/abs/2409.16045v1</td>\n",
       "      <td>arxiv.org</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Logic Tensor Networks (LTN) merge deep learnin...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ML&amp;DL, Reasoning models]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>REFINING PRETRAINING DATA PROGRAMMATICALLY</td>\n",
       "      <td>https://gair-nlp.github.io/ProX/homepage.html</td>\n",
       "      <td>gair-nlp.github.io</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>ProX is a framework that treats data refinemen...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Model, Model Training]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CHATS WITH VIDEOS IN REAL TIME</td>\n",
       "      <td>https://huggingface.co/wangyueqian/MMDuet</td>\n",
       "      <td>huggingface.co</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>MMDuet is a novel \"video-text duet\" interactio...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BUILDING LLMS IS PROBABLY NOT GOING BE A BRILL...</td>\n",
       "      <td>https://calpaterson.com/porter.html</td>\n",
       "      <td>calpaterson.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>LLM makers like OpenAI face significant challe...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Model]</td>\n",
       "      <td>[OpenAI, NVIDIA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ROX</td>\n",
       "      <td>https://www.notboring.co/p/rox</td>\n",
       "      <td>www.notboring.co</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>A new AI-native CRM, Rox, aims to disrupt Sale...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Funding]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PLAYAI RAISES $21M FUNDING AND RELEASES A NEW ...</td>\n",
       "      <td>https://blog.play.ai/blog/21m-funding</td>\n",
       "      <td>blog.play.ai</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>PlayAI raised $21 Million to advance voice-fir...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Multimodal, Funding]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ANTHROPIC SAYS CLAUDE AI CAN MATCH YOUR UNIQUE...</td>\n",
       "      <td>https://www.theverge.com/2024/11/26/24306575/a...</td>\n",
       "      <td>www.theverge.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Anthropic's Claude AI now offers customizable ...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI]</td>\n",
       "      <td>[Anthropic]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>CONVERTING GPT TO LLAMA</td>\n",
       "      <td>https://github.com/rasbt/LLMs-from-scratch/tre...</td>\n",
       "      <td>github.com</td>\n",
       "      <td>TLDR AI &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>This repository contains code for converting a...</td>\n",
       "      <td>2024-12-02 09:29:42-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI]</td>\n",
       "      <td>[Meta]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SPACEX WEIGHS TENDER OFFER AT ROUGHLY $350 BIL...</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2024-1...</td>\n",
       "      <td>www.bloomberg.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>SpaceX is in discussions to sell insider share...</td>\n",
       "      <td>2024-12-03 06:27:29-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Funding]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>INTEL CEO PAT GELSINGER RESIGNS AFTER STRUGGLI...</td>\n",
       "      <td>https://www.wsj.com/tech/intel-ceo-gelsinger-r...</td>\n",
       "      <td>www.wsj.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Intel's Chief Executive Pat Gelsinger has resi...</td>\n",
       "      <td>2024-12-03 06:27:29-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>WORLD LABS' AI CAN GENERATE INTERACTIVE 3D SCE...</td>\n",
       "      <td>https://techcrunch.com/2024/12/02/world-labs-a...</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>World Labs has unveiled an AI system that can ...</td>\n",
       "      <td>2024-12-03 06:27:29-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>12-POUND US ROBOT BALLOONS CAPTURE NYC-SIZED A...</td>\n",
       "      <td>https://interestingengineering.com/innovation/...</td>\n",
       "      <td>interestingengineering.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Near Space Labs has launched a fleet of balloo...</td>\n",
       "      <td>2024-12-03 06:27:29-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Evaluation, Robotics]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>THE BROWSER COMPANY TEASES DIA, ITS NEW AI BRO...</td>\n",
       "      <td>https://techcrunch.com/2024/12/02/the-browser-...</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Dia is a new browser by The Browser Company th...</td>\n",
       "      <td>2024-12-03 06:27:29-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI]</td>\n",
       "      <td>[Amazon]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>TESLA CEO ELON MUSK LOSES BID TO GET $56 BILLI...</td>\n",
       "      <td>https://www.cnbc.com/2024/12/02/tesla-ceo-elon...</td>\n",
       "      <td>www.cnbc.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>A Delaware judge has upheld the ruling that El...</td>\n",
       "      <td>2024-12-03 06:27:29-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Legal]</td>\n",
       "      <td>[Twitter]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ASK HN: HOW CAN I GROW AS AN ENGINEER WITHOUT ...</td>\n",
       "      <td>https://news.ycombinator.com/item?id=42289955</td>\n",
       "      <td>news.ycombinator.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Engineers without seniors to learn from can gr...</td>\n",
       "      <td>2024-12-03 06:27:29-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CHATGPT WITH ADS? OPENAI EXPLORES NEW BUSINESS...</td>\n",
       "      <td>https://www.pcmag.com/news/chatgpt-with-ads-op...</td>\n",
       "      <td>www.pcmag.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>OpenAI is considering an ad-based business model.</td>\n",
       "      <td>2024-12-03 06:27:29-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[OpenAI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>OPEN SOURCE ROUTER FIRMWARE PROJECT OPENWRT SH...</td>\n",
       "      <td>https://www.theregister.com/2024/12/02/openwrt...</td>\n",
       "      <td>www.theregister.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>The OpenWrt One features a dual-core MediaTek ...</td>\n",
       "      <td>2024-12-03 06:27:29-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ORIOLEDB BETA7: BENCHMARKS</td>\n",
       "      <td>https://www.orioledb.com/blog/orioledb-beta7-b...</td>\n",
       "      <td>www.orioledb.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>OrioleDB is a storage extension for PostgreSQL...</td>\n",
       "      <td>2024-12-03 06:27:29-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Evaluation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>EXCLUSIVE: HERE'S WHAT THE CANCELED PIXEL TABL...</td>\n",
       "      <td>https://www.androidauthority.com/exclusive-can...</td>\n",
       "      <td>www.androidauthority.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Google's canceled Pixel Tablet 2 included plan...</td>\n",
       "      <td>2024-12-03 06:27:29-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Google]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>TESLA FULL SELF-DRIVING (FSD) V13 STARTS INITI...</td>\n",
       "      <td>https://www.teslarati.com/tesla-full-self-driv...</td>\n",
       "      <td>www.teslarati.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Tesla has started rolling out Tesla FSD V13 to...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Robotics]</td>\n",
       "      <td>[Twitter]</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>META PLANS TO BUILD A $10B SUBSEA CABLE SPANNI...</td>\n",
       "      <td>https://techcrunch.com/2024/11/29/meta-plans-t...</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Meta is the second-biggest driver of internet ...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Meta]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>TESLA OPTIMUS SHOWS OFF NEW UPGRADE</td>\n",
       "      <td>https://www.teslarati.com/tesla-optimus-new-ha...</td>\n",
       "      <td>www.teslarati.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Tesla's Optimus has received new hands. They c...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Robotics]</td>\n",
       "      <td>[Twitter]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>SOMEONE JUST WON $50,000 BY CONVINCING AN AI A...</td>\n",
       "      <td>https://threadreaderapp.com/thread/18622998457...</td>\n",
       "      <td>threadreaderapp.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Freysa is an AI agent that was released with t...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Agents]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>IF NOT REACT, THEN WHAT?</td>\n",
       "      <td>https://infrequently.org/2024/11/if-not-react-...</td>\n",
       "      <td>infrequently.org</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>React is a legacy technology, but it continues...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>BUILDING A DISTRIBUTED LOG USING S3 (UNDER 150...</td>\n",
       "      <td>https://avi.im/blag/2024/s3-log/</td>\n",
       "      <td>avi.im</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>This tutorial shows readers how to implement a...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Funding]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>THE MOST HATED WAY OF FIRING SOMEONE IS MORE P...</td>\n",
       "      <td>https://www.wsj.com/business/firing-someone-pe...</td>\n",
       "      <td>www.wsj.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>The use of performance improvement plans (PIPs...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[RAI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>HOW LIBRA WAS KILLED</td>\n",
       "      <td>https://threadreaderapp.com/thread/18626545067...</td>\n",
       "      <td>threadreaderapp.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Libra was an advanced, high-performance, payme...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Meta]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>OPENAI IS AT WAR WITH ITS OWN SORA VIDEO TESTE...</td>\n",
       "      <td>https://arstechnica.com/ai/2024/11/openai-is-a...</td>\n",
       "      <td>arstechnica.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>OpenAI cut off testing access to its Sora vide...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Multimodal]</td>\n",
       "      <td>[OpenAI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>THE FASCINATING SECURITY MODEL OF DARK WEB MAR...</td>\n",
       "      <td>https://boehs.org/node/dark-web-security</td>\n",
       "      <td>boehs.org</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>The success of the current most trusted dark w...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>TESLA IS LOOKING TO HIRE A TEAM TO REMOTELY CO...</td>\n",
       "      <td>https://gizmodo.com/tesla-is-looking-to-hire-a...</td>\n",
       "      <td>gizmodo.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>While Tesla will use AI to power its autonomou...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI]</td>\n",
       "      <td>[Twitter]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>BREAKING DOWN THE DOJ'S PLAN TO END GOOGLE'S S...</td>\n",
       "      <td>https://www.theverge.com/2024/11/27/24302415/d...</td>\n",
       "      <td>www.theverge.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Experts watching the case say that a forced Ch...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Google]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>PRETTY &amp; DENSE</td>\n",
       "      <td>https://github.com/daveshap/Claude_Sentience/b...</td>\n",
       "      <td>github.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>This prompt for Claude makes it produce highly...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>WHAT'S GOING ON WITH CHATGPT AND THE NAME 'DAV...</td>\n",
       "      <td>https://mashable.com/article/chatgpt-name-davi...</td>\n",
       "      <td>mashable.com</td>\n",
       "      <td>TLDR &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>ChatGPT seems to crash whenever it tries to me...</td>\n",
       "      <td>2024-12-02 06:26:44-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4 LEVELS OF DATA PROFICIENCY</td>\n",
       "      <td>https://itamargilad.com/data-driven/</td>\n",
       "      <td>itamargilad.com</td>\n",
       "      <td>TLDR Product &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Data is vital for product companies but prone ...</td>\n",
       "      <td>2024-12-03 06:06:24-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, RAI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>HOW TO EXPERIMENT IN THE WORLD OF AI</td>\n",
       "      <td>https://productify.substack.com/p/how-to-exper...</td>\n",
       "      <td>productify.substack.com</td>\n",
       "      <td>TLDR Product &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Three key shifts have redefined the AI product...</td>\n",
       "      <td>2024-12-03 06:06:24-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Model, ML&amp;DL, Evaluation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>PRODUCT MANAGER VS. PRODUCT OWNER: KEY DIFFERE...</td>\n",
       "      <td>https://www.justanotherpm.com/blog/product-man...</td>\n",
       "      <td>www.justanotherpm.com</td>\n",
       "      <td>TLDR Product &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Product managers (PMs) and product owners (POs...</td>\n",
       "      <td>2024-12-03 06:06:24-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>NAVIGATE THE AI WORKFORCE REVOLUTION: LESSONS ...</td>\n",
       "      <td>https://lewislin.substack.com/p/part-12-naviga...</td>\n",
       "      <td>lewislin.substack.com</td>\n",
       "      <td>TLDR Product &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>A century of technological shifts, from mass p...</td>\n",
       "      <td>2024-12-03 06:06:24-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>PRODUCT STRATEGY STACK VS. DECISION STACK: WHA...</td>\n",
       "      <td>https://newsletter.herbig.co/posts/344</td>\n",
       "      <td>newsletter.herbig.co</td>\n",
       "      <td>TLDR Product &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>The decision stack and product strategy stack ...</td>\n",
       "      <td>2024-12-03 06:06:24-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Model Training]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>BEING IN THE DETAILS</td>\n",
       "      <td>https://theengineeringmanager.substack.com/p/b...</td>\n",
       "      <td>theengineeringmanager.substack.com</td>\n",
       "      <td>TLDR Product &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>In a shift toward efficiency, tech companies a...</td>\n",
       "      <td>2024-12-03 06:06:24-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>SALESFORCE CEO MARC BENIOFF HAS THOUGHTS ON AI...</td>\n",
       "      <td>https://www.bigtechnology.com/p/salesforce-ceo...</td>\n",
       "      <td>www.bigtechnology.com</td>\n",
       "      <td>TLDR Product &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Rather than personal AI assistants, Salesforce...</td>\n",
       "      <td>2024-12-03 06:06:24-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AI&amp;GenAI, Agents]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>THE FTC IS INVESTIGATING UBER OVER ITS SUBSCRI...</td>\n",
       "      <td>https://sherwood.news/business/ftc-investigati...</td>\n",
       "      <td>sherwood.news</td>\n",
       "      <td>TLDR Product &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>The FTC is investigating Uber One's subscripti...</td>\n",
       "      <td>2024-12-03 06:06:24-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Amazon]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>SKEPTOPTIMISM ‚Äì THINKING SLOW, ACTING FAST</td>\n",
       "      <td>https://cutlefish.substack.com/p/tbm-325-skept...</td>\n",
       "      <td>cutlefish.substack.com</td>\n",
       "      <td>TLDR Product &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>Skeptical optimists balance deep, deliberate p...</td>\n",
       "      <td>2024-12-03 06:06:24-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>DRIVER AND NAVIGATOR</td>\n",
       "      <td>https://threadreaderapp.com/thread/18626328344...</td>\n",
       "      <td>threadreaderapp.com</td>\n",
       "      <td>TLDR Product &lt;dan@tldrnewsletter.com&gt;</td>\n",
       "      <td></td>\n",
       "      <td>The driver + navigator model fosters collabora...</td>\n",
       "      <td>2024-12-03 06:06:24-05:00</td>\n",
       "      <td>US/Eastern</td>\n",
       "      <td>1.3.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                      ADS MIGHT BE COMING TO CHATGPT   \n",
       "1   THE RACE IS ON TO MAKE AI AGENTS DO YOUR ONLIN...   \n",
       "2   SALESFORCE CEO MARC BENIOFF HAS THOUGHTS ON AI...   \n",
       "3               DEMO: DECOUPLED MOMENTUM OPTIMIZATION   \n",
       "4                                      REWARD HACKING   \n",
       "5    OPENAI TARGETS 1BN USERS IN NEXT PHASE OF GROWTH   \n",
       "6   GOOGLE'S PLAN TO KEEP AI OUT OF SEARCH TRIAL R...   \n",
       "7                                 UV GUIDE TO PYTORCH   \n",
       "8   AI COMPANY MISTRAL IS LATEST EUROPEAN STARTUP ...   \n",
       "9   OPENAI GETS NEW $1.5 BILLION INVESTMENT FROM S...   \n",
       "10  ELON MUSK SEEKS TO BLOCK OPENAI'S FOR-PROFIT C...   \n",
       "11             PERPLEXITY MULLS GETTING INTO HARDWARE   \n",
       "12  INFLECTION AI CEO SAYS IT'S DONE TRYING TO MAK...   \n",
       "13  INTELLECT-1 RELEASE: THE FIRST GLOBALLY TRAINE...   \n",
       "14                    DETECT AND LEARN UNSEEN OBJECTS   \n",
       "15                     MAKING UNDERWATER IMAGES CLEAR   \n",
       "16   MAPPING THE IONOSPHERE WITH THE POWER OF ANDROID   \n",
       "17                               INTRODUCING LTNTORCH   \n",
       "18         REFINING PRETRAINING DATA PROGRAMMATICALLY   \n",
       "19                     CHATS WITH VIDEOS IN REAL TIME   \n",
       "20  BUILDING LLMS IS PROBABLY NOT GOING BE A BRILL...   \n",
       "21                                                ROX   \n",
       "22  PLAYAI RAISES $21M FUNDING AND RELEASES A NEW ...   \n",
       "23  ANTHROPIC SAYS CLAUDE AI CAN MATCH YOUR UNIQUE...   \n",
       "24                            CONVERTING GPT TO LLAMA   \n",
       "25  SPACEX WEIGHS TENDER OFFER AT ROUGHLY $350 BIL...   \n",
       "26  INTEL CEO PAT GELSINGER RESIGNS AFTER STRUGGLI...   \n",
       "27  WORLD LABS' AI CAN GENERATE INTERACTIVE 3D SCE...   \n",
       "28  12-POUND US ROBOT BALLOONS CAPTURE NYC-SIZED A...   \n",
       "29  THE BROWSER COMPANY TEASES DIA, ITS NEW AI BRO...   \n",
       "30  TESLA CEO ELON MUSK LOSES BID TO GET $56 BILLI...   \n",
       "31  ASK HN: HOW CAN I GROW AS AN ENGINEER WITHOUT ...   \n",
       "32  CHATGPT WITH ADS? OPENAI EXPLORES NEW BUSINESS...   \n",
       "33  OPEN SOURCE ROUTER FIRMWARE PROJECT OPENWRT SH...   \n",
       "34                         ORIOLEDB BETA7: BENCHMARKS   \n",
       "35  EXCLUSIVE: HERE'S WHAT THE CANCELED PIXEL TABL...   \n",
       "36  TESLA FULL SELF-DRIVING (FSD) V13 STARTS INITI...   \n",
       "37  META PLANS TO BUILD A $10B SUBSEA CABLE SPANNI...   \n",
       "38                TESLA OPTIMUS SHOWS OFF NEW UPGRADE   \n",
       "39  SOMEONE JUST WON $50,000 BY CONVINCING AN AI A...   \n",
       "40                           IF NOT REACT, THEN WHAT?   \n",
       "41  BUILDING A DISTRIBUTED LOG USING S3 (UNDER 150...   \n",
       "42  THE MOST HATED WAY OF FIRING SOMEONE IS MORE P...   \n",
       "43                               HOW LIBRA WAS KILLED   \n",
       "44  OPENAI IS AT WAR WITH ITS OWN SORA VIDEO TESTE...   \n",
       "45  THE FASCINATING SECURITY MODEL OF DARK WEB MAR...   \n",
       "46  TESLA IS LOOKING TO HIRE A TEAM TO REMOTELY CO...   \n",
       "47  BREAKING DOWN THE DOJ'S PLAN TO END GOOGLE'S S...   \n",
       "48                                     PRETTY & DENSE   \n",
       "49  WHAT'S GOING ON WITH CHATGPT AND THE NAME 'DAV...   \n",
       "50                       4 LEVELS OF DATA PROFICIENCY   \n",
       "51               HOW TO EXPERIMENT IN THE WORLD OF AI   \n",
       "52  PRODUCT MANAGER VS. PRODUCT OWNER: KEY DIFFERE...   \n",
       "53  NAVIGATE THE AI WORKFORCE REVOLUTION: LESSONS ...   \n",
       "54  PRODUCT STRATEGY STACK VS. DECISION STACK: WHA...   \n",
       "55                               BEING IN THE DETAILS   \n",
       "56  SALESFORCE CEO MARC BENIOFF HAS THOUGHTS ON AI...   \n",
       "57  THE FTC IS INVESTIGATING UBER OVER ITS SUBSCRI...   \n",
       "58         SKEPTOPTIMISM ‚Äì THINKING SLOW, ACTING FAST   \n",
       "59                               DRIVER AND NAVIGATOR   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://techcrunch.com/2024/12/02/ads-might-be...   \n",
       "1   https://techcrunch.com/2024/12/02/the-race-is-...   \n",
       "2   https://www.bigtechnology.com/p/salesforce-ceo...   \n",
       "3                    https://arxiv.org/abs/2411.19870   \n",
       "4   https://lilianweng.github.io/posts/2024-11-28-...   \n",
       "5   https://www.ft.com/content/e91cb018-873c-4388-...   \n",
       "6   https://arstechnica.com/tech-policy/2024/11/go...   \n",
       "7   https://docs.astral.sh/uv/guides/integration/p...   \n",
       "8   https://www.semafor.com/article/11/27/2024/ai-...   \n",
       "9   https://www.cnbc.com/2024/11/26/openai-gets-1p...   \n",
       "10  https://www.theverge.com/2024/11/30/24309697/e...   \n",
       "11  https://techcrunch.com/2024/11/26/perplexity-m...   \n",
       "12  https://techcrunch.com/2024/11/26/inflection-c...   \n",
       "13  https://www.primeintellect.ai/blog/intellect-1...   \n",
       "14                 https://arxiv.org/abs/2411.18207v1   \n",
       "15                 https://arxiv.org/abs/2411.18296v1   \n",
       "16  https://research.google/blog/mapping-the-ionos...   \n",
       "17                 https://arxiv.org/abs/2409.16045v1   \n",
       "18      https://gair-nlp.github.io/ProX/homepage.html   \n",
       "19          https://huggingface.co/wangyueqian/MMDuet   \n",
       "20                https://calpaterson.com/porter.html   \n",
       "21                     https://www.notboring.co/p/rox   \n",
       "22              https://blog.play.ai/blog/21m-funding   \n",
       "23  https://www.theverge.com/2024/11/26/24306575/a...   \n",
       "24  https://github.com/rasbt/LLMs-from-scratch/tre...   \n",
       "25  https://www.bloomberg.com/news/articles/2024-1...   \n",
       "26  https://www.wsj.com/tech/intel-ceo-gelsinger-r...   \n",
       "27  https://techcrunch.com/2024/12/02/world-labs-a...   \n",
       "28  https://interestingengineering.com/innovation/...   \n",
       "29  https://techcrunch.com/2024/12/02/the-browser-...   \n",
       "30  https://www.cnbc.com/2024/12/02/tesla-ceo-elon...   \n",
       "31      https://news.ycombinator.com/item?id=42289955   \n",
       "32  https://www.pcmag.com/news/chatgpt-with-ads-op...   \n",
       "33  https://www.theregister.com/2024/12/02/openwrt...   \n",
       "34  https://www.orioledb.com/blog/orioledb-beta7-b...   \n",
       "35  https://www.androidauthority.com/exclusive-can...   \n",
       "36  https://www.teslarati.com/tesla-full-self-driv...   \n",
       "37  https://techcrunch.com/2024/11/29/meta-plans-t...   \n",
       "38  https://www.teslarati.com/tesla-optimus-new-ha...   \n",
       "39  https://threadreaderapp.com/thread/18622998457...   \n",
       "40  https://infrequently.org/2024/11/if-not-react-...   \n",
       "41                   https://avi.im/blag/2024/s3-log/   \n",
       "42  https://www.wsj.com/business/firing-someone-pe...   \n",
       "43  https://threadreaderapp.com/thread/18626545067...   \n",
       "44  https://arstechnica.com/ai/2024/11/openai-is-a...   \n",
       "45           https://boehs.org/node/dark-web-security   \n",
       "46  https://gizmodo.com/tesla-is-looking-to-hire-a...   \n",
       "47  https://www.theverge.com/2024/11/27/24302415/d...   \n",
       "48  https://github.com/daveshap/Claude_Sentience/b...   \n",
       "49  https://mashable.com/article/chatgpt-name-davi...   \n",
       "50               https://itamargilad.com/data-driven/   \n",
       "51  https://productify.substack.com/p/how-to-exper...   \n",
       "52  https://www.justanotherpm.com/blog/product-man...   \n",
       "53  https://lewislin.substack.com/p/part-12-naviga...   \n",
       "54             https://newsletter.herbig.co/posts/344   \n",
       "55  https://theengineeringmanager.substack.com/p/b...   \n",
       "56  https://www.bigtechnology.com/p/salesforce-ceo...   \n",
       "57  https://sherwood.news/business/ftc-investigati...   \n",
       "58  https://cutlefish.substack.com/p/tbm-325-skept...   \n",
       "59  https://threadreaderapp.com/thread/18626328344...   \n",
       "\n",
       "                         news_provider                     source_of_the_news  \\\n",
       "0                       techcrunch.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "1                       techcrunch.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "2                www.bigtechnology.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "3                            arxiv.org       TLDR AI <dan@tldrnewsletter.com>   \n",
       "4                 lilianweng.github.io       TLDR AI <dan@tldrnewsletter.com>   \n",
       "5                           www.ft.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "6                      arstechnica.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "7                       docs.astral.sh       TLDR AI <dan@tldrnewsletter.com>   \n",
       "8                      www.semafor.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "9                         www.cnbc.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "10                    www.theverge.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "11                      techcrunch.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "12                      techcrunch.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "13               www.primeintellect.ai       TLDR AI <dan@tldrnewsletter.com>   \n",
       "14                           arxiv.org       TLDR AI <dan@tldrnewsletter.com>   \n",
       "15                           arxiv.org       TLDR AI <dan@tldrnewsletter.com>   \n",
       "16                     research.google       TLDR AI <dan@tldrnewsletter.com>   \n",
       "17                           arxiv.org       TLDR AI <dan@tldrnewsletter.com>   \n",
       "18                  gair-nlp.github.io       TLDR AI <dan@tldrnewsletter.com>   \n",
       "19                      huggingface.co       TLDR AI <dan@tldrnewsletter.com>   \n",
       "20                     calpaterson.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "21                    www.notboring.co       TLDR AI <dan@tldrnewsletter.com>   \n",
       "22                        blog.play.ai       TLDR AI <dan@tldrnewsletter.com>   \n",
       "23                    www.theverge.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "24                          github.com       TLDR AI <dan@tldrnewsletter.com>   \n",
       "25                   www.bloomberg.com          TLDR <dan@tldrnewsletter.com>   \n",
       "26                         www.wsj.com          TLDR <dan@tldrnewsletter.com>   \n",
       "27                      techcrunch.com          TLDR <dan@tldrnewsletter.com>   \n",
       "28          interestingengineering.com          TLDR <dan@tldrnewsletter.com>   \n",
       "29                      techcrunch.com          TLDR <dan@tldrnewsletter.com>   \n",
       "30                        www.cnbc.com          TLDR <dan@tldrnewsletter.com>   \n",
       "31                news.ycombinator.com          TLDR <dan@tldrnewsletter.com>   \n",
       "32                       www.pcmag.com          TLDR <dan@tldrnewsletter.com>   \n",
       "33                 www.theregister.com          TLDR <dan@tldrnewsletter.com>   \n",
       "34                    www.orioledb.com          TLDR <dan@tldrnewsletter.com>   \n",
       "35            www.androidauthority.com          TLDR <dan@tldrnewsletter.com>   \n",
       "36                   www.teslarati.com          TLDR <dan@tldrnewsletter.com>   \n",
       "37                      techcrunch.com          TLDR <dan@tldrnewsletter.com>   \n",
       "38                   www.teslarati.com          TLDR <dan@tldrnewsletter.com>   \n",
       "39                 threadreaderapp.com          TLDR <dan@tldrnewsletter.com>   \n",
       "40                    infrequently.org          TLDR <dan@tldrnewsletter.com>   \n",
       "41                              avi.im          TLDR <dan@tldrnewsletter.com>   \n",
       "42                         www.wsj.com          TLDR <dan@tldrnewsletter.com>   \n",
       "43                 threadreaderapp.com          TLDR <dan@tldrnewsletter.com>   \n",
       "44                     arstechnica.com          TLDR <dan@tldrnewsletter.com>   \n",
       "45                           boehs.org          TLDR <dan@tldrnewsletter.com>   \n",
       "46                         gizmodo.com          TLDR <dan@tldrnewsletter.com>   \n",
       "47                    www.theverge.com          TLDR <dan@tldrnewsletter.com>   \n",
       "48                          github.com          TLDR <dan@tldrnewsletter.com>   \n",
       "49                        mashable.com          TLDR <dan@tldrnewsletter.com>   \n",
       "50                     itamargilad.com  TLDR Product <dan@tldrnewsletter.com>   \n",
       "51             productify.substack.com  TLDR Product <dan@tldrnewsletter.com>   \n",
       "52               www.justanotherpm.com  TLDR Product <dan@tldrnewsletter.com>   \n",
       "53               lewislin.substack.com  TLDR Product <dan@tldrnewsletter.com>   \n",
       "54                newsletter.herbig.co  TLDR Product <dan@tldrnewsletter.com>   \n",
       "55  theengineeringmanager.substack.com  TLDR Product <dan@tldrnewsletter.com>   \n",
       "56               www.bigtechnology.com  TLDR Product <dan@tldrnewsletter.com>   \n",
       "57                       sherwood.news  TLDR Product <dan@tldrnewsletter.com>   \n",
       "58              cutlefish.substack.com  TLDR Product <dan@tldrnewsletter.com>   \n",
       "59                 threadreaderapp.com  TLDR Product <dan@tldrnewsletter.com>   \n",
       "\n",
       "   text                                       news_summary  \\\n",
       "0        OpenAI is considering an advertising business ...   \n",
       "1        Tech companies are developing AI shopping agen...   \n",
       "2        Salesforce CEO Marc Benioff envisions companie...   \n",
       "3        2.5x faster, 100x less communication volume, a...   \n",
       "4        Lilian Weng has another great blog post discus...   \n",
       "5        OpenAI aims to reach 1 billion users by launch...   \n",
       "6        US District Judge Amit Mehta suggests AI could...   \n",
       "7        Documentation on how to use the new package ma...   \n",
       "8        European AI startup Mistral, known for open-we...   \n",
       "9        OpenAI is allowing employees to sell $1.5 bill...   \n",
       "10       Elon Musk's legal team has filed a motion to p...   \n",
       "11       Perplexity's CEO plans to develop a \"simple, u...   \n",
       "12       Inflection AI shifted its focus from developin...   \n",
       "13       INTELLECT-1 is a 10B parameter model trained o...   \n",
       "14       This new framework pushes object detection int...   \n",
       "15       HUPE is an AI-powered method that improves und...   \n",
       "16       Google researchers were able to accurately map...   \n",
       "17       Logic Tensor Networks (LTN) merge deep learnin...   \n",
       "18       ProX is a framework that treats data refinemen...   \n",
       "19       MMDuet is a novel \"video-text duet\" interactio...   \n",
       "20       LLM makers like OpenAI face significant challe...   \n",
       "21       A new AI-native CRM, Rox, aims to disrupt Sale...   \n",
       "22       PlayAI raised $21 Million to advance voice-fir...   \n",
       "23       Anthropic's Claude AI now offers customizable ...   \n",
       "24       This repository contains code for converting a...   \n",
       "25       SpaceX is in discussions to sell insider share...   \n",
       "26       Intel's Chief Executive Pat Gelsinger has resi...   \n",
       "27       World Labs has unveiled an AI system that can ...   \n",
       "28       Near Space Labs has launched a fleet of balloo...   \n",
       "29       Dia is a new browser by The Browser Company th...   \n",
       "30       A Delaware judge has upheld the ruling that El...   \n",
       "31       Engineers without seniors to learn from can gr...   \n",
       "32       OpenAI is considering an ad-based business model.   \n",
       "33       The OpenWrt One features a dual-core MediaTek ...   \n",
       "34       OrioleDB is a storage extension for PostgreSQL...   \n",
       "35       Google's canceled Pixel Tablet 2 included plan...   \n",
       "36       Tesla has started rolling out Tesla FSD V13 to...   \n",
       "37       Meta is the second-biggest driver of internet ...   \n",
       "38       Tesla's Optimus has received new hands. They c...   \n",
       "39       Freysa is an AI agent that was released with t...   \n",
       "40       React is a legacy technology, but it continues...   \n",
       "41       This tutorial shows readers how to implement a...   \n",
       "42       The use of performance improvement plans (PIPs...   \n",
       "43       Libra was an advanced, high-performance, payme...   \n",
       "44       OpenAI cut off testing access to its Sora vide...   \n",
       "45       The success of the current most trusted dark w...   \n",
       "46       While Tesla will use AI to power its autonomou...   \n",
       "47       Experts watching the case say that a forced Ch...   \n",
       "48       This prompt for Claude makes it produce highly...   \n",
       "49       ChatGPT seems to crash whenever it tries to me...   \n",
       "50       Data is vital for product companies but prone ...   \n",
       "51       Three key shifts have redefined the AI product...   \n",
       "52       Product managers (PMs) and product owners (POs...   \n",
       "53       A century of technological shifts, from mass p...   \n",
       "54       The decision stack and product strategy stack ...   \n",
       "55       In a shift toward efficiency, tech companies a...   \n",
       "56       Rather than personal AI assistants, Salesforce...   \n",
       "57       The FTC is investigating Uber One's subscripti...   \n",
       "58       Skeptical optimists balance deep, deliberate p...   \n",
       "59       The driver + navigator model fosters collabora...   \n",
       "\n",
       "                 date_source date_source_time_zone version  \\\n",
       "0  2024-12-03 09:13:10-05:00            US/Eastern   1.3.3   \n",
       "1  2024-12-03 09:13:10-05:00            US/Eastern   1.3.3   \n",
       "2  2024-12-03 09:13:10-05:00            US/Eastern   1.3.3   \n",
       "3  2024-12-03 09:13:10-05:00            US/Eastern   1.3.3   \n",
       "4  2024-12-03 09:13:10-05:00            US/Eastern   1.3.3   \n",
       "5  2024-12-03 09:13:10-05:00            US/Eastern   1.3.3   \n",
       "6  2024-12-03 09:13:10-05:00            US/Eastern   1.3.3   \n",
       "7  2024-12-03 09:13:10-05:00            US/Eastern   1.3.3   \n",
       "8  2024-12-03 09:13:10-05:00            US/Eastern   1.3.3   \n",
       "9  2024-12-03 09:13:10-05:00            US/Eastern   1.3.3   \n",
       "10 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "11 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "12 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "13 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "14 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "15 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "16 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "17 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "18 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "19 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "20 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "21 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "22 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "23 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "24 2024-12-02 09:29:42-05:00            US/Eastern   1.3.3   \n",
       "25 2024-12-03 06:27:29-05:00            US/Eastern   1.3.3   \n",
       "26 2024-12-03 06:27:29-05:00            US/Eastern   1.3.3   \n",
       "27 2024-12-03 06:27:29-05:00            US/Eastern   1.3.3   \n",
       "28 2024-12-03 06:27:29-05:00            US/Eastern   1.3.3   \n",
       "29 2024-12-03 06:27:29-05:00            US/Eastern   1.3.3   \n",
       "30 2024-12-03 06:27:29-05:00            US/Eastern   1.3.3   \n",
       "31 2024-12-03 06:27:29-05:00            US/Eastern   1.3.3   \n",
       "32 2024-12-03 06:27:29-05:00            US/Eastern   1.3.3   \n",
       "33 2024-12-03 06:27:29-05:00            US/Eastern   1.3.3   \n",
       "34 2024-12-03 06:27:29-05:00            US/Eastern   1.3.3   \n",
       "35 2024-12-03 06:27:29-05:00            US/Eastern   1.3.3   \n",
       "36 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "37 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "38 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "39 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "40 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "41 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "42 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "43 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "44 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "45 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "46 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "47 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "48 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "49 2024-12-02 06:26:44-05:00            US/Eastern   1.3.3   \n",
       "50 2024-12-03 06:06:24-05:00            US/Eastern   1.3.3   \n",
       "51 2024-12-03 06:06:24-05:00            US/Eastern   1.3.3   \n",
       "52 2024-12-03 06:06:24-05:00            US/Eastern   1.3.3   \n",
       "53 2024-12-03 06:06:24-05:00            US/Eastern   1.3.3   \n",
       "54 2024-12-03 06:06:24-05:00            US/Eastern   1.3.3   \n",
       "55 2024-12-03 06:06:24-05:00            US/Eastern   1.3.3   \n",
       "56 2024-12-03 06:06:24-05:00            US/Eastern   1.3.3   \n",
       "57 2024-12-03 06:06:24-05:00            US/Eastern   1.3.3   \n",
       "58 2024-12-03 06:06:24-05:00            US/Eastern   1.3.3   \n",
       "59 2024-12-03 06:06:24-05:00            US/Eastern   1.3.3   \n",
       "\n",
       "   competitive_intelligence                                themes  \\\n",
       "0                        []                            [AI&GenAI]   \n",
       "1                        []                    [AI&GenAI, Agents]   \n",
       "2                        []                    [AI&GenAI, Agents]   \n",
       "3                        []               [Model, Model Training]   \n",
       "4                        []               [Model, Model Training]   \n",
       "5                        []                    [AI&GenAI, Agents]   \n",
       "6                        []                   [AI&GenAI, Funding]   \n",
       "7                        []                                    []   \n",
       "8                        []                            [AI&GenAI]   \n",
       "9                        []                   [AI&GenAI, Funding]   \n",
       "10                       []                               [Legal]   \n",
       "11                       []       [AI&GenAI, Multimodal, Funding]   \n",
       "12                       []            [AI&GenAI, Model, Funding]   \n",
       "13                       []          [Model Training, Evaluation]   \n",
       "14                       []                [AI&GenAI, Multimodal]   \n",
       "15                       []                [AI&GenAI, Multimodal]   \n",
       "16                       []                                    []   \n",
       "17                       []             [ML&DL, Reasoning models]   \n",
       "18                       []               [Model, Model Training]   \n",
       "19                       []                            [AI&GenAI]   \n",
       "20                       []                     [AI&GenAI, Model]   \n",
       "21                       []                   [AI&GenAI, Funding]   \n",
       "22                       []       [AI&GenAI, Multimodal, Funding]   \n",
       "23                       []                            [AI&GenAI]   \n",
       "24                       []                            [AI&GenAI]   \n",
       "25                       []                             [Funding]   \n",
       "26                       []                            [AI&GenAI]   \n",
       "27                       []                            [AI&GenAI]   \n",
       "28                       []      [AI&GenAI, Evaluation, Robotics]   \n",
       "29                       []                            [AI&GenAI]   \n",
       "30                       []                               [Legal]   \n",
       "31                       []                                    []   \n",
       "32                       []                                    []   \n",
       "33                       []                                    []   \n",
       "34                       []                          [Evaluation]   \n",
       "35                       []                                    []   \n",
       "36                       []                  [AI&GenAI, Robotics]   \n",
       "37                       []                                    []   \n",
       "38                       []                            [Robotics]   \n",
       "39                       []                    [AI&GenAI, Agents]   \n",
       "40                       []                                    []   \n",
       "41                       []                             [Funding]   \n",
       "42                       []                                 [RAI]   \n",
       "43                       []                                    []   \n",
       "44                       []                          [Multimodal]   \n",
       "45                       []                                    []   \n",
       "46                       []                            [AI&GenAI]   \n",
       "47                       []                                    []   \n",
       "48                       []                                    []   \n",
       "49                       []                                    []   \n",
       "50                       []                       [AI&GenAI, RAI]   \n",
       "51                       []  [AI&GenAI, Model, ML&DL, Evaluation]   \n",
       "52                       []                                    []   \n",
       "53                       []                            [AI&GenAI]   \n",
       "54                       []                      [Model Training]   \n",
       "55                       []                                    []   \n",
       "56                       []                    [AI&GenAI, Agents]   \n",
       "57                       []                                    []   \n",
       "58                       []                                    []   \n",
       "59                       []                                    []   \n",
       "\n",
       "                     market_intelligence personalities  score_category_count  \n",
       "0                               [OpenAI]            []                     2  \n",
       "1   [OpenAI, Google, Amazon, Perplexity]            []                     6  \n",
       "2                                     []            []                     2  \n",
       "3                                     []            []                     2  \n",
       "4                                     []            []                     2  \n",
       "5            [OpenAI, Google, Microsoft]            []                     5  \n",
       "6            [OpenAI, Google, Microsoft]            []                     5  \n",
       "7                                     []            []                     0  \n",
       "8                           [Mistral AI]            []                     2  \n",
       "9                               [OpenAI]            []                     3  \n",
       "10                     [OpenAI, Twitter]            []                     3  \n",
       "11                          [Perplexity]            []                     4  \n",
       "12                                    []            []                     3  \n",
       "13                                    []            []                     2  \n",
       "14                                    []            []                     2  \n",
       "15                                    []            []                     2  \n",
       "16                              [Google]            []                     0  \n",
       "17                                    []            []                     2  \n",
       "18                                    []            []                     2  \n",
       "19                                    []            []                     1  \n",
       "20                      [OpenAI, NVIDIA]            []                     4  \n",
       "21                                    []            []                     2  \n",
       "22                                    []            []                     3  \n",
       "23                           [Anthropic]            []                     2  \n",
       "24                                [Meta]            []                     2  \n",
       "25                                    []            []                     1  \n",
       "26                                    []            []                     1  \n",
       "27                                    []            []                     1  \n",
       "28                                    []            []                     3  \n",
       "29                              [Amazon]            []                     2  \n",
       "30                             [Twitter]            []                     2  \n",
       "31                                    []            []                     0  \n",
       "32                              [OpenAI]            []                     0  \n",
       "33                                    []            []                     0  \n",
       "34                                    []            []                     1  \n",
       "35                              [Google]            []                     0  \n",
       "36                             [Twitter]            []                     3  \n",
       "37                                [Meta]            []                     0  \n",
       "38                             [Twitter]            []                     2  \n",
       "39                                    []            []                     2  \n",
       "40                                    []            []                     0  \n",
       "41                                    []            []                     1  \n",
       "42                                    []            []                     1  \n",
       "43                                [Meta]            []                     0  \n",
       "44                              [OpenAI]            []                     2  \n",
       "45                                    []            []                     0  \n",
       "46                             [Twitter]            []                     2  \n",
       "47                              [Google]            []                     0  \n",
       "48                                    []            []                     0  \n",
       "49                                    []            []                     0  \n",
       "50                                    []            []                     2  \n",
       "51                                    []            []                     4  \n",
       "52                                    []            []                     0  \n",
       "53                                    []            []                     1  \n",
       "54                                    []            []                     1  \n",
       "55                                    []            []                     0  \n",
       "56                                    []            []                     2  \n",
       "57                              [Amazon]            []                     0  \n",
       "58                                    []            []                     0  \n",
       "59                                    []            []                     0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.df_ns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52771dab-4f5d-4039-8f8b-15a4485f4cff",
   "metadata": {},
   "source": [
    "# Report.create_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc1af151-fc11-4852-b8a3-229cfb8d068d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:56:35.904561Z",
     "iopub.status.busy": "2025-06-20T02:56:35.904241Z",
     "iopub.status.idle": "2025-06-20T03:12:45.538786Z",
     "shell.execute_reply": "2025-06-20T03:12:45.537062Z",
     "shell.execute_reply.started": "2025-06-20T02:56:35.904538Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:35,909 - src.genai_model.genai_model - INFO -- l.106: Found 13 models for model_type large\n",
      "2025-06-19 22:56:35,911 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-exp-1206', 'openrouter/google/gemini-exp-1206:free', 'gemini/gemini-exp-1121', 'openrouter/google/gemini-exp-1121:free', 'gemini/gemini-exp-1114', 'openrouter/google/gemini-exp-1114:free', 'gemini/gemini-1.5-pro-002', 'gemini/gemini-1.5-pro-exp-0827', 'openrouter/meta-llama/llama-3.1-405b-instruct:free', 'gemini/gemini-1.5-pro-001', 'mistral/mistral-large-2411', 'mistral/mistral-large-2407', 'mistral/mistral-large-2402']\n",
      "2025-06-19 22:56:35,915 - src.genai_model.genai_model - INFO -- l.106: Found 13 models for model_type large\n",
      "2025-06-19 22:56:35,916 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-exp-1206', 'openrouter/google/gemini-exp-1206:free', 'gemini/gemini-exp-1121', 'openrouter/google/gemini-exp-1121:free', 'gemini/gemini-exp-1114', 'openrouter/google/gemini-exp-1114:free', 'gemini/gemini-1.5-pro-002', 'gemini/gemini-1.5-pro-exp-0827', 'openrouter/meta-llama/llama-3.1-405b-instruct:free', 'gemini/gemini-1.5-pro-001', 'mistral/mistral-large-2411', 'mistral/mistral-large-2407', 'mistral/mistral-large-2402']\n",
      "\u001b[92m22:56:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 22:56:35,946 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 22:56:36,199 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:36,222 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m22:56:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 22:56:36,224 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:36,734 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 22:56:36,749 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m22:56:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 22:56:36,751 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 22:56:36,928 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:56:36,944 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m22:56:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 22:56:36,946 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 22:56:37,289 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:56:37,309 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m22:56:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 22:56:37,310 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 22:56:37,495 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:37,510 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m22:56:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 22:56:37,512 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 22:56:37,831 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:56:37,855 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m22:56:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 22:56:37,857 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 22:56:38,016 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:38,032 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m22:56:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 22:56:38,033 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:38,205 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:56:38,222 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m22:56:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 22:56:38,224 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:38,677 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:56:38,694 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m22:56:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 22:56:38,695 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 22:56:38,864 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:56:38,881 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m22:56:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 22:56:38,882 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:41,474 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:41 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:41,494 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:41,498 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 22:56:41,499 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m22:56:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:42,175 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:42,610 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:42 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:42,613 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:42,617 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:43,068 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:43 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:43,072 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:43,076 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:43,572 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:43 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:43,576 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:43,580 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:44,046 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:44 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:44,049 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:44,054 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:44,543 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:44 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:44,546 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:44,549 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:45,115 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:45 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:45,118 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:45,121 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:45,650 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:45 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:45,653 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:45,656 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:46,244 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:46 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:46,248 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:46,251 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:46,755 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:46 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:46,759 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:46,763 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:47,268 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:47 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:47,272 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:47,275 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:47,450 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:47,466 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:56:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:56:47,467 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:48,087 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:48,105 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:56:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:56:48,106 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:48,547 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:48 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:48,552 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:48,555 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:48,721 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:48,733 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:56:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:56:48,735 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:49,316 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:49,342 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:56:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:56:49,344 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:49,725 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:49 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:49,728 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:49,732 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:49,896 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:49,913 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:56:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:56:49,915 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:50,457 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:50,472 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:56:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:56:50,474 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:50,876 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:50 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:50,878 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:50,882 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:51,056 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:51,071 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:56:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:56:51,072 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:51,591 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:51,606 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:56:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:56:51,607 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:51,982 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:51 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:51,985 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:51,989 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:52,152 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:52,168 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:56:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:56:52,169 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:52,638 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:52,653 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:56:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:56:52,654 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:53,014 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:53 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:53,016 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:53,020 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:53,184 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:53,199 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:56:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:56:53,200 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:53,698 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:53,713 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:56:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:56:53,715 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:54,161 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:54 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:54,167 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:54,170 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:54,354 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:54,382 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:56:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:56:54,383 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:55,160 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:55,176 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:56:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:56:55,177 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:55,547 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:55 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:55,550 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:55,555 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:55,777 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:55,797 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:56:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:56:55,799 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:56,387 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:56,406 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:56:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:56:56,408 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:56,797 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:56 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:56,801 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:56,806 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:56,969 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:56,986 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:56:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:56:56,987 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:57,513 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:57,529 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:56:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:56:57,530 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:57,925 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:57 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:57,927 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:57,933 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:58,104 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:58,120 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:56:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:56:58,123 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:58,528 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:56:58,543 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:56:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:56:58,544 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:56:58,912 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:56:58 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:56:58,917 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:56:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:58,921 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:56:59,618 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:01,592 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m22:57:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:01,745 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:02,670 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:02,706 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:02,711 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:03,150 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:03 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:03,157 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:03,183 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:03,405 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:03,426 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:03,431 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:04,060 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:04,108 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:04,119 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:04,581 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:04 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:04,591 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:04,600 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:04,827 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:04,853 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:04,855 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:05,466 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:05,505 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:05,507 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:06,025 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:06 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:06,032 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:06,040 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:06,269 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:06,298 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:06,300 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:06,999 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:07,016 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:07,018 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:07,459 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:07 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:07,465 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:07,471 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:07,939 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:07 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:07,944 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:07,949 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:08,384 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:08 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:08,387 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:08,391 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:08,608 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:08,630 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:08,635 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:09,443 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:09,461 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:09,467 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:09,851 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:09 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:09,854 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:09,859 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:10,320 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:10 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:10,331 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:10,335 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:10,775 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:10 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:10,778 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:10,781 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:11,250 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:11 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:11,253 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:11,257 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:11,680 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:11 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:11,685 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:11,691 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:12,128 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:12,133 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:12,140 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:12,573 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:12,577 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:12,580 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:13,008 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:13 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:13,010 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:13,013 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:13,569 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:13 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:13,572 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:13,576 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:14,087 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:14,093 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:14,101 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:14,579 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:14,581 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:14,586 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:15,042 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:15,046 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:15,049 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:15,224 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:15,289 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:15,291 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:15,860 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:15,883 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:15,886 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:16,354 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:16 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:16,360 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:16,364 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:16,532 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:16,546 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:16,548 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:16,994 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:17,014 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:17,016 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:17,399 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:17 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:17,403 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:17,409 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:17,632 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:17,648 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:17,650 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:18,106 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:18,123 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:18,125 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:18,546 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:18 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:18,551 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:18,557 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:18,723 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:18,740 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:18,742 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:19,445 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:19,462 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:19,464 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:19,846 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:19 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:19,851 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:19,855 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:20,042 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:20,058 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:20,059 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:20,543 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:20,560 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:20,562 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:20,994 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:20 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:20,998 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:21,001 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:21,213 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:21,324 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:21,331 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:21,797 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:21,812 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:21,814 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:22,275 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:22 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:22,284 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:22,288 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:22,475 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:22,490 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:22,493 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:23,064 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:23,079 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:23,080 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:23,455 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:23 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:23,460 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:23,467 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:23,638 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:23,663 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:23,666 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:24,206 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:24,221 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:24,222 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:24,592 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:24 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:24,595 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:24,601 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:24,760 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:24,776 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:24,780 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:25,256 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:25,272 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:25,279 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:25,667 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:25 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:25,671 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:25,675 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:25,844 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:25,860 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:25,861 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:26,381 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:26,396 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:26,398 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:26,791 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:26 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:26,795 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:26,801 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:26,985 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:27,000 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:27,001 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:27,543 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:27,560 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:27,561 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:27,951 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:27 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:27,954 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:27,963 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:28,164 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:28,178 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:28,180 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:28,652 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:28,674 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:28,676 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:29,095 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:29 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:29,099 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:29,103 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:29,269 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:29,283 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:29,286 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:29,901 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:29,918 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:29,919 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:30,339 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:30 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:30,343 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:30,351 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:30,517 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:30,530 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:30,532 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:31,303 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:31,320 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:31,322 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:31,847 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:31 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:31,850 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:31,854 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:32,013 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:32,028 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:32,030 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:32,423 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:32,440 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:32,441 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:32,854 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:32 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:32,857 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:32,861 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:33,043 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:33,057 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:33,059 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:33,527 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:33,543 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:33,544 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:33,937 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:33 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:33,949 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 22:57:34,058 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 22:57:34,214 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:34,229 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m22:57:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 22:57:34,230 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:34,581 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 22:57:34,596 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m22:57:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 22:57:34,598 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 22:57:34,759 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:34,776 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m22:57:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 22:57:34,795 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:35,137 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:35,153 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m22:57:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 22:57:35,155 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 22:57:35,317 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:35,334 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m22:57:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 22:57:35,336 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:35,829 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:35,845 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m22:57:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 22:57:35,847 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 22:57:36,014 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:36,029 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m22:57:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:36,035 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:36,223 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:36,240 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m22:57:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 22:57:36,242 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:36,562 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:36,578 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m22:57:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 22:57:36,579 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:36,872 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:36,888 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m22:57:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 22:57:36,891 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:39,823 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:39 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:39,831 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:39,835 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 22:57:39,837 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m22:57:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:40,655 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:40,834 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:40,850 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:40,852 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:41,293 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:41,310 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:41,312 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:41,484 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:41,502 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:57:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:41,503 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:57:41,675 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:41,688 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:57:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:41,689 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:41,843 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:41,857 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:57:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:57:41,859 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:42,128 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:42,144 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:57:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:57:42,147 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:42,605 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:42 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:42,608 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:42,611 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:42,787 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:42,808 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:42,811 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:43,235 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:43,251 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:43,253 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:43,405 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:43,420 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:57:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:57:43,422 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:43,592 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:43,607 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:57:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:43,608 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:43,774 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:43,790 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:57:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:57:43,791 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:44,166 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:44,183 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:57:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:57:44,185 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:44,561 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:44 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:44,564 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:44,569 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:44,729 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:44,744 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:44,746 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:45,224 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:45,239 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:45,244 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:45,409 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:45,427 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m22:57:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:57:45,429 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:57:45,586 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:45,603 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:57:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:45,604 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:45,763 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:45,781 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:57:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:57:45,783 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:46,186 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:46,202 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:57:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:57:46,206 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:46,585 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:46 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:46,587 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:46,591 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:46,755 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:46,770 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:46,772 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:47,275 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:47,291 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:47,293 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:47,465 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:47,482 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m22:57:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:57:47,483 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:57:47,697 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:47,713 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:57:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:47,715 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:47,877 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:47,892 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:57:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:57:47,894 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:48,300 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:48,316 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:57:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:57:48,318 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:48,708 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:48 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:48,711 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:48,716 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:48,890 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:48,910 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:48,912 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:49,427 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:49,443 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:49,445 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:49,610 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:49,625 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:57:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:57:49,626 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:49,798 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:49,814 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:57:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:49,817 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:50,013 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:50,029 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:57:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:57:50,030 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:50,451 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:50,466 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:57:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:57:50,468 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:50,844 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:50 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:50,847 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:50,851 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:51,013 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:51,028 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:51,030 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:51,524 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:51,540 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:51,546 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:51,722 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:51,737 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:57:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:57:51,738 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:57:51,891 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:51,906 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:57:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:51,908 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:52,060 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:52,076 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:57:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:57:52,078 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:52,406 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:52,424 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:57:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:57:52,426 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:52,854 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:52 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:52,856 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:52,859 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:53,010 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:53,024 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:53,025 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:53,637 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:53,653 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:53,654 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:53,829 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:53,845 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:57:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:57:53,846 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:54,019 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:54,036 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:57:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:54,038 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:54,195 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:54,211 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:57:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:57:54,212 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:54,818 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:54,832 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:57:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:57:54,833 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:55,231 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:55 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:55,234 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:55,239 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:55,396 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:55,416 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:55,419 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:56,088 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:56,104 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:56,106 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:56,274 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:56,290 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:57:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:57:56,291 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:56,438 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:56,455 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:57:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:56,457 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:56,623 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:56,638 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:57:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:57:56,640 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:56,972 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:56,989 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:57:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:57:56,990 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:57,422 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:57 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:57,425 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:57,429 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:57,593 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:57,609 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:57,611 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:58,179 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:58,195 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:57:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:58,200 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:57:58,366 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:58,387 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:57:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:58,389 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:57:58,573 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:58,588 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:57:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:58,589 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:57:58,756 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:58,771 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:57:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:57:58,773 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:59,146 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:57:59,162 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:57:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:57:59,164 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:57:59,546 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:59 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:57:59,549 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:57:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:59,552 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:57:59,721 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:57:59,737 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:57:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:57:59,739 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:00,904 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:00,920 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:58:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:58:00,921 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:01,130 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:01,185 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:58:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:58:01,187 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:01,425 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:01,442 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:58:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:58:01,444 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:58:01,619 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:58:01,636 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m22:58:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:58:01,639 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:58:02,089 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:58:02,107 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:58:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:58:02,110 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:02,545 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:02 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:58:02,548 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:58:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:02,551 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:02,727 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:02,749 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:58:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:58:02,751 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:03,236 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:03,251 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:58:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:58:03,252 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:58:03,408 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:03,423 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:58:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:58:03,424 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:03,580 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:03,597 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:58:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:58:03,598 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:58:03,780 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:03,794 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:58:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:58:03,796 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:58:04,128 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:58:04,146 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:58:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:58:04,148 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:04,562 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:04 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:58:04,565 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:58:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:04,569 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:04,735 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:04,751 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:58:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:58:04,752 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:05,196 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:05,222 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:58:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:58:05,224 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:58:05,394 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:05,410 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:58:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:58:05,412 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:05,571 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:05,587 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:58:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:58:05,588 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:58:05,748 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:58:05,762 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:58:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:05,764 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:58:06,139 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:58:06,156 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:58:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:58:06,158 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:06,526 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:06 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:58:06,528 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:58:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:06,531 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:06,683 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:06,697 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:58:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:58:06,699 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:07,134 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:07,151 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:58:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:58:07,153 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:58:07,319 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:07,339 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:58:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:58:07,340 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:07,503 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:07,517 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:58:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:58:07,519 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:58:07,677 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:58:07,691 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:58:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:58:07,693 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:08,012 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:58:08,028 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:58:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:58:08,029 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:08,418 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:08 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:58:08,433 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:58:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:08,476 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:08,743 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:08,758 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:58:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:58:08,761 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:09,320 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:09,332 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:58:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:58:09,334 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:58:09,486 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:09,499 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:58:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:58:09,500 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:09,658 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:09,680 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:58:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:58:09,682 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:58:09,837 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:58:09,852 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:58:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:58:09,853 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:10,224 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:58:10,240 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:58:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:58:10,241 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:10,614 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:10 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:58:10,616 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:58:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:10,619 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:11,094 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:11 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:58:11,098 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:58:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:11,101 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:11,574 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:11 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:58:11,576 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:58:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:11,579 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:12,037 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:58:12,040 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:58:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:12,043 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:12,213 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:12,225 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m22:58:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 22:58:12,226 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:12,725 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:12,737 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m22:58:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 22:58:12,738 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:12,965 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:12,986 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m22:58:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 22:58:12,987 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:13,209 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 22:58:13,224 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m22:58:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:58:13,226 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 22:58:13,391 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:58:13,405 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m22:58:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 22:58:13,407 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:13,806 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 22:58:13,822 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m22:58:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 22:58:13,825 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:58:14,211 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:58:14,215 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:58:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:14,218 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:14,649 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:58:14,653 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:58:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:14,656 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:15,167 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 22:58:15,170 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:58:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 22:58:15,174 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:26,416 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:26 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:26,433 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:26,440 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:26,944 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:26 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:26,949 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:26,953 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:27,645 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:27 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:27,648 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:27,651 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:28,155 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:28 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:28,158 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:28,161 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:28,670 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:28 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:28,673 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:28,676 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:29,146 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:29 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:29,149 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:29,153 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:29,600 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:29 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:29,605 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:29,609 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:30,052 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:30 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:30,063 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:30,067 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:30,555 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:30 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:30,566 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:30,570 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:31,089 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:31 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:31,093 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:00:31,118 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:00:31,284 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:31,303 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:00:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:00:31,306 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:31,734 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:00:31,750 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:00:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:00:31,752 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:00:31,928 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:31,943 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:00:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:00:31,945 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:32,357 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:32,373 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:00:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:00:32,375 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:00:32,530 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:32,545 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:00:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:00:32,546 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:32,869 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:32,885 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:00:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:00:32,886 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:00:33,061 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:33,090 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:00:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:00:33,092 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:00:33,256 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:33,271 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:00:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:00:33,273 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:33,709 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:33,725 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:00:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:00:33,726 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:34,079 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:34,097 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:00:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:00:34,098 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:36,427 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:36 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:36,430 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:36,433 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:00:36,434 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:00:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:38,137 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:38,589 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:38 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:38,593 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:38,597 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:38,765 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:38,780 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:38,782 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:39,305 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:39,321 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:00:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:00:39,323 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:39,717 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:39 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:39,725 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:39,729 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:39,909 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:39,926 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:39,928 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:40,534 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:40,550 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:00:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:00:40,551 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:41,283 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:41 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:41,287 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:41,290 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:41,651 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:41,671 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:41,674 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:42,251 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:42,263 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:00:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:00:42,265 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:42,778 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:42 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:42,782 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:42,785 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:42,961 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:42,977 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:42,978 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:43,450 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:43,465 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:00:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:00:43,467 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:43,868 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:43 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:43,875 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:43,879 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:44,049 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:44,065 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:44,066 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:44,574 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:44,590 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:00:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:00:44,591 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:45,144 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:45 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:45,147 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:45,150 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:45,311 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:45,333 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:45,335 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:45,851 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:45,866 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:00:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:00:45,867 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:46,242 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:46 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:46,245 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:46,249 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:46,422 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:46,442 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:46,443 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:46,992 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:47,014 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:00:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:00:47,017 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:47,421 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:47 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:47,425 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:47,428 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:47,593 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:47,608 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:47,609 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:48,220 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:48,235 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:00:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:00:48,237 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:48,631 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:48 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:48,635 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:48,638 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:48,803 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:48,817 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:48,819 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:49,451 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:49,467 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:00:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:00:49,469 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:49,964 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:49 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:49,967 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:49,971 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:50,140 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:50,155 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:50,157 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:50,763 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:50,775 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:00:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:00:50,777 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:51,194 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:51 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:51,197 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:51,200 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:51,369 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:51,384 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:51,385 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:51,909 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:51,924 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:00:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:00:51,926 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:52,422 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:52 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:52,425 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:00:52,431 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:00:52,600 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:52,618 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:00:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:00:52,620 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:52,955 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:00:52,971 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:00:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:00:52,972 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:00:53,135 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:53,152 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:00:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:00:53,154 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:53,551 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:53,566 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:00:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:00:53,568 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:00:53,736 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:53,750 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:00:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:00:53,752 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:54,114 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:54,130 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:00:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:00:54,131 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:00:54,292 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:54,329 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:00:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:00:54,331 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:00:54,499 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:54,516 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:00:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:00:54,517 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:54,871 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:54,886 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:00:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:00:54,888 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:00:55,040 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:00:55,055 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:00:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:00:55,057 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:57,722 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:57 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:57,726 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:57,728 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:00:57,730 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:00:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:58,375 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:58,538 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:58,552 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:58,553 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:59,084 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:59,101 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:00:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:00:59,102 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:00:59,509 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:59 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:00:59,511 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:00:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:59,514 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:00:59,676 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:00:59,694 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:00:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:00:59,697 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:00,187 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:00,206 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:00,208 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:00,736 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:00 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:00,740 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:00,743 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:00,908 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:00,923 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:00,925 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:01,442 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:01,464 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:01,466 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:01,906 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:01 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:01,911 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:01,914 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:02,091 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:02,105 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:02,107 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:02,607 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:02,625 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:02,627 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:03,080 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:03 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:03,083 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:03,086 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:03,263 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:03,278 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:03,280 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:03,747 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:03,762 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:03,764 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:04,188 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:04 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:04,193 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:04,196 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:04,365 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:04,380 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:04,381 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:04,925 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:04,941 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:04,943 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:05,125 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:05,140 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:01:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:01:05,142 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:01:05,312 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:05,326 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:01:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:05,328 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:05,633 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:05,649 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:01:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:01:05,651 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:06,046 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:06,060 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:01:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:01:06,061 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:06,540 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:06 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:06,543 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:06,548 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:06,713 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:06,729 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:06,731 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:07,177 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:07,189 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:07,191 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:07,341 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:07,357 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:01:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:01:07,358 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:07,521 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:07,538 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:01:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:07,539 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:07,689 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:07,704 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:01:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:01:07,706 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:08,182 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:08,199 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:01:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:01:08,201 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:08,568 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:08 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:08,599 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:08,605 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:08,807 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:08,859 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:08,862 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:09,333 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:09,359 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:09,361 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:09,537 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:09,560 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:01:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:01:09,562 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:01:09,881 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:09,897 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:01:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:09,898 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:10,061 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:10,075 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:01:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:01:10,077 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:10,421 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:10,442 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:01:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:01:10,444 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:10,853 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:10 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:10,855 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:01:10,865 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:01:11,042 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:11,057 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:01:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:01:11,059 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:11,458 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:01:11,473 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:01:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:01:11,476 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:01:11,642 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:11,655 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:01:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:01:11,657 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:11,943 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:11,957 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:01:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:01:11,958 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:12,265 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:12,283 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:01:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:01:12,285 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:12,618 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:12,637 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:01:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:01:12,638 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:01:12,812 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:12,837 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:01:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:12,839 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:13,001 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:13,018 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:01:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:01:13,021 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:13,405 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:13,421 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:01:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:01:13,424 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:01:13,583 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:13,594 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:01:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:01:13,595 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:15,175 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:15,179 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:15,181 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:01:15,183 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:01:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:15,784 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:16,280 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:16 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:16,285 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:16,288 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:16,891 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:16 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:16,894 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:16,896 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:17,405 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:17 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:17,409 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:17,413 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:17,918 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:17 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:17,922 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:17,927 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:18,431 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:18 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:18,440 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:18,443 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:18,917 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:18 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:18,921 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:18,924 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:19,455 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:19 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:19,460 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:19,462 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:19,886 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:19 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:19,888 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:19,894 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:01:19,895 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:01:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:20,513 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:21,096 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:21 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:21,101 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:01:21,109 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:01:21,272 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:21,293 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:01:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:01:21,295 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:21,708 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:01:21,724 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:01:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:01:21,725 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:01:21,883 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:21,901 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:01:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:01:21,902 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:22,322 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:22,338 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:01:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:01:22,340 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:01:22,505 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:22,520 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:01:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:01:22,522 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:23,019 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:23,037 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:01:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:01:23,039 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:01:23,196 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:23,217 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:01:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:23,219 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:23,383 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:23,399 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:01:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:01:23,401 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:23,695 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:23,713 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:01:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:01:23,715 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:01:23,876 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:23,890 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:01:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:01:23,891 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:25,744 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:25 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:25,747 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:25,748 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:01:25,750 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:01:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:26,362 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:26,779 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:26 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:26,781 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:26,784 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:27,220 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:27 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:27,223 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:27,227 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:27,396 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:27,409 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:27,410 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:27,846 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:27,863 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:27,864 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:28,370 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:28 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:28,373 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:28,376 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:28,544 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:28,561 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:28,562 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:28,939 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:28,957 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:28,958 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:29,383 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:29 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:29,386 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:29,390 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:29,555 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:29,578 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:29,580 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:30,085 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:30,102 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:30,103 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:30,525 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:30 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:30,529 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:30,535 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:30,754 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:30,783 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:30,784 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:31,442 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:31,468 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:31,469 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:31,914 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:31 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:31,917 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:31,921 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:32,093 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:32,112 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:32,114 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:32,673 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:32,690 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:32,692 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:33,189 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:33 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:33,194 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:33,197 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:33,381 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:33,398 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:33,400 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:33,860 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:33,877 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:33,878 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:34,255 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:34 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:34,259 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:34,262 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:34,525 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:34,543 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:34,546 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:35,112 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:35,132 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:35,134 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:35,646 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:35 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:35,651 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:35,655 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:35,805 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:35,819 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:35,820 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:36,341 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:36,357 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:36,358 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:36,727 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:36 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:36,730 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:36,738 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:36,903 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:36,917 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:36,918 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:37,298 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:37,314 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:37,315 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:37,778 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:37 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:37,782 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:37,785 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:38,096 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:38,110 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:38,112 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:38,561 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:38,576 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:38,578 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:39,007 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:39 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:39,009 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:39,013 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:39,182 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:39,197 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:39,199 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:39,722 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:39,736 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:39,738 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:40,131 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:40 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:40,152 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:40,157 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:40,329 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:40,344 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:40,346 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:40,947 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:40,962 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:40,964 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:41,363 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:41 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:41,367 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:41,371 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:41,539 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:41,560 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:41,562 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:42,078 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:42,093 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:42,094 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:42,545 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:42 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:42,548 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:42,552 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:42,710 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:42,727 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:42,729 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:43,514 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:43,531 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:43,532 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:43,924 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:43 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:43,928 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:43,932 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:44,098 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:44,114 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:44,115 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:44,553 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:44,569 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:44,570 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:44,986 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:44 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:44,989 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:44,991 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:45,151 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:45,173 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:45,175 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:45,666 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:45,686 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:45,687 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:46,076 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:46 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:46,080 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:46,083 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:46,252 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:46,269 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:46,271 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:46,793 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:46,815 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:46,816 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:46,977 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:46,990 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:01:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:01:46,992 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:47,154 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:47,170 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:01:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:47,171 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:47,327 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:47,343 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:01:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:01:47,344 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:47,756 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:47,773 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:01:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:01:47,774 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:48,231 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:48 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:48,235 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:48,238 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:48,400 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:48,415 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:48,416 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:48,843 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:48,860 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:48,861 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:49,024 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:49,046 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:01:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:01:49,047 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:49,276 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:49,292 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:01:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:49,293 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:49,454 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:49,468 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:01:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:01:49,469 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:49,855 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:49,870 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:01:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:01:49,872 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:50,280 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:50 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:50,283 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:50,297 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:01:50,298 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:01:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:50,964 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:51,138 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:51,152 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:51,153 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:51,611 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:51,627 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:51,629 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:51,796 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:51,813 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:01:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:01:51,816 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:51,971 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:51,994 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:01:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:51,995 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:52,147 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:52,162 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:01:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:01:52,163 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:52,448 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:52,465 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:01:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:01:52,467 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:52,854 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:52 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:52,856 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:52,860 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:53,015 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:53,037 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:53,039 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:53,416 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:53,431 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:53,433 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:53,594 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:53,610 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:01:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:01:53,611 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:53,774 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:53,789 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:01:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:53,791 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:53,951 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:53,972 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:01:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:01:53,973 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:54,496 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:54,512 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:01:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:01:54,514 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:54,993 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:54 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:54,995 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:55,000 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:55,173 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:55,189 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:55,190 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:55,710 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:55,726 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:55,731 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:55,897 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:55,913 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:01:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:01:55,914 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:56,072 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:56,093 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:01:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:56,094 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:56,262 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:56,280 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:01:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:01:56,282 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:56,655 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:56,668 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:01:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:01:56,672 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:57,063 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:57 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:57,066 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:01:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:57,068 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:57,235 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:57,250 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:57,251 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:57,760 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:57,777 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:01:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:57,779 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:01:57,942 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:57,958 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:01:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:01:57,959 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:58,123 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:58,141 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:01:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:58,144 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:01:58,298 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:58,314 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:01:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:01:58,315 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:58,674 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:01:58,690 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:01:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:01:58,692 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:01:59,110 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:59 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:59,113 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:01:59,119 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:01:59,121 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:01:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:59,797 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:01:59,965 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:01:59,983 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:01:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:01:59,985 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:00,433 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:00,449 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:00,451 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:00,671 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:00,688 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:02:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:00,689 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:00,915 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:00,930 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:02:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:00,931 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:01,346 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:01,362 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:02:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:02:01,363 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:01,756 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:01,773 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:02:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:02:01,776 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:02,163 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:02 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:02,170 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:02:02,202 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:02:02,463 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:02,487 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:02:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:02:02,490 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:02,883 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:02:02,898 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:02:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:02:02,900 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:02:03,059 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:03,072 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:02:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:02:03,074 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:03,350 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:03,369 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:02:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:02:03,370 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:02:03,549 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:03,565 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:02:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:02:03,567 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:02:03,889 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:03,904 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:02:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:02:03,906 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:02:04,066 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:04,080 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:02:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:04,082 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:04,252 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:04,269 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:02:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:02:04,270 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:04,591 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:04,606 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:02:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:02:04,609 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:02:04,769 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:04,783 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:02:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:02:04,785 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:06,452 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:06 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:06,455 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:06,458 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:02:06,460 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:02:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:07,417 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:07,591 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:07,608 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:07,609 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:08,187 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:08,202 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:08,204 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:08,372 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:08,384 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:02:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:08,385 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:08,701 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:08,716 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:02:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:08,717 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:08,864 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:08,879 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:02:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:02:08,880 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:09,147 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:09,159 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:02:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:02:09,160 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:09,561 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:09 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:09,564 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:09,566 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:09,726 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:09,747 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:09,748 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:10,234 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:10,251 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:10,252 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:10,407 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:10,422 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:02:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:10,423 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:10,589 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:10,606 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:02:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:10,609 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:10,787 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:10,802 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:02:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:02:10,804 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:02:11,160 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:11,176 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:02:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:02:11,178 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:11,564 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:11 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:11,568 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:11,577 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:12,083 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:12,085 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:12,088 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:12,596 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:12,601 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:12,604 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:13,109 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:13 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:13,114 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:13,117 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:13,619 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:13 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:13,623 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:13,627 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:14,028 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:14,030 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:14,032 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:14,540 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:14,544 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:14,548 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:15,051 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:15,053 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:15,057 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:15,469 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:15,473 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:15,477 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:15,974 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:15,978 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:15,981 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:16,487 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:16 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:16,491 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:16,494 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:17,000 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:17 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:17,002 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:17,006 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:17,178 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:17,193 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:17,194 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:17,551 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:17,571 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:17,574 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:18,025 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:18 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:18,029 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:18,033 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:18,196 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:18,210 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:18,212 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:18,741 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:18,757 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:18,758 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:19,177 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:19 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:19,182 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:19,187 - src.genai_model.genai_model - INFO -- l.106: Found 13 models for model_type large\n",
      "2025-06-19 23:02:19,189 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-exp-1206', 'openrouter/google/gemini-exp-1206:free', 'gemini/gemini-exp-1121', 'openrouter/google/gemini-exp-1121:free', 'gemini/gemini-exp-1114', 'openrouter/google/gemini-exp-1114:free', 'gemini/gemini-1.5-pro-002', 'gemini/gemini-1.5-pro-exp-0827', 'openrouter/meta-llama/llama-3.1-405b-instruct:free', 'gemini/gemini-1.5-pro-001', 'mistral/mistral-large-2411', 'mistral/mistral-large-2407', 'mistral/mistral-large-2402']\n",
      "\u001b[92m23:02:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:02:19,216 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:02:19,383 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:19,399 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:02:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:02:19,400 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:19,745 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:02:19,761 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:02:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:02:19,763 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:02:19,935 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:19,950 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:02:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:02:19,952 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:20,385 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:20,406 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:02:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:02:20,409 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:02:20,558 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:20,575 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:02:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:02:20,577 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:20,997 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:21,013 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:02:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:02:21,014 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:02:21,204 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:21,220 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:02:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:21,223 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:21,372 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:21,394 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:02:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:02:21,398 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:21,815 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:21,833 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:02:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:02:21,835 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:02:22,001 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:22,016 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:02:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:02:22,018 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:24,463 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:24 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:24,466 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:24,468 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:02:24,470 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:02:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:25,240 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:25,402 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:25,417 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:25,419 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:25,915 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:25,932 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:25,933 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:26,531 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:26 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:26,535 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:26,541 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:26,715 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:26,738 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:26,739 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:27,224 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:27,239 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:27,240 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:27,766 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:27 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:27,770 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:27,775 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:27,949 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:27,965 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:27,966 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:28,411 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:28,433 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:28,436 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:28,813 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:28 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:28,817 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:28,821 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:28,978 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:28,995 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:28,998 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:29,459 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:29,475 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:29,477 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:29,885 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:29 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:29,887 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:29,889 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:30,049 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:30,066 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:30,067 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:30,597 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:30,612 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:30,614 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:30,959 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:30 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:30,963 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:30,966 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:31,130 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:31,156 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:31,159 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:31,755 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:31,769 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:31,772 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:32,173 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:32 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:32,178 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:32,182 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:32,354 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:32,378 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:32,380 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:32,777 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:32,793 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:32,795 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:33,160 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:33 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:33,163 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:33,166 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:33,340 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:33,357 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:33,360 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:33,907 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:33,925 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:33,927 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:34,357 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:34 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:34,361 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:34,365 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:34,539 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:34,567 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:34,569 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:34,993 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:35,008 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:35,009 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:35,628 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:35 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:35,633 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:35,639 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:35,800 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:35,815 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:35,817 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:36,294 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:36,312 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:36,316 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:36,716 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:36 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:36,719 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:36,722 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:36,893 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:36,908 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:36,910 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:37,475 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:37,493 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:37,494 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:37,989 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:37 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:37,992 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:37,996 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:38,151 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:38,172 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:38,174 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:38,656 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:38,672 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:38,674 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:39,092 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:39 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:39,096 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:39,100 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:39,262 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:39,277 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:39,279 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:39,636 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:39,651 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:39,657 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:40,047 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:40 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:40,051 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:40,056 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:40,232 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:40,249 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:40,250 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:40,671 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:40,686 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:40,687 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:41,093 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:41 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:41,096 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:41,099 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:41,273 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:41,291 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:41,296 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:41,762 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:41,776 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:41,778 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:42,082 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:42,099 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:02:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:42,102 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:42,267 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:42,282 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:02:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:42,285 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:42,456 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:42,498 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:02:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:02:42,502 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:42,978 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:42,994 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:02:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:02:42,996 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:43,519 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:43 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:43,521 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:43,526 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:43,698 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:43,714 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:43,716 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:44,186 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:44,203 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:44,209 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:44,377 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:44,392 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:02:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:44,394 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:44,560 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:44,576 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:02:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:44,578 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:44,782 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:44,812 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:02:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:02:44,814 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:45,263 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:45,278 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:02:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:02:45,280 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:45,777 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:45 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:45,782 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:45,786 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:45,950 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:45,966 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:45,968 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:46,494 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:46,511 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:46,514 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:46,678 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:46,693 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:02:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:46,694 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:46,856 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:46,872 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:02:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:46,873 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:47,034 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:47,047 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:02:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:02:47,049 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:47,518 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:47,537 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:02:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:02:47,539 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:48,031 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:48 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:48,033 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:48,038 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:48,200 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:48,216 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:48,218 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:48,850 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:48,866 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:48,868 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:49,022 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:49,038 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:02:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:49,040 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:49,198 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:49,213 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:02:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:49,215 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:49,384 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:49,399 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:02:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:02:49,401 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:49,743 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:49,760 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:02:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:02:49,764 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:50,179 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:50 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:50,181 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:50,184 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:50,345 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:50,360 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:50,361 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:50,900 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:50,921 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:50,923 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:51,092 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:51,109 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:02:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:51,110 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:51,409 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:51,421 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:02:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:51,423 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:51,580 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:51,595 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:02:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:02:51,597 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:52,028 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:52,044 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:02:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:02:52,047 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:52,540 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:52 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:52,544 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:52,549 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:52,715 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:52,731 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:52,733 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:53,202 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:53,220 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:53,222 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:53,408 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:53,425 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:02:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:53,426 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:53,592 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:53,607 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:02:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:53,609 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:53,768 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:53,783 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:02:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:02:53,785 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:54,153 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:54,169 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:02:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:02:54,170 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:54,565 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:54 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:54,573 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:54,576 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:54,752 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:54,766 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:54,767 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:55,306 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:55,322 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:55,323 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:55,494 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:55,560 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:02:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:55,562 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:55,755 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:55,780 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:02:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:55,782 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:55,944 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:55,957 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:02:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:02:55,958 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:56,331 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:56,347 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:02:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:02:56,348 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:56,743 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:56 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:56,746 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:56,749 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:56,905 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:56,925 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:56,928 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:57,434 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:57,450 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:57,451 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:57,614 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:57,630 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:02:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:57,632 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:57,936 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:57,952 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:02:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:57,953 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:02:58,106 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:58,128 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:02:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:02:58,130 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:58,483 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:02:58,496 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:02:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:02:58,498 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:58,889 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:58 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:02:58,891 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:02:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:58,895 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:02:59,059 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:59,074 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:02:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:02:59,075 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:59,588 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:02:59,604 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:02:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:59,606 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:02:59,791 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:02:59,812 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:02:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:59,814 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:02:59,994 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:03:00,010 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:03:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:03:00,012 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:03:00,165 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:03:00,180 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:03:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:03:00,182 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:03:00,532 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:03:00,547 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:03:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:03:00,548 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:03:00,935 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:03:00 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:03:00,939 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:03:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:03:00,944 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:03:01,110 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:03:01,124 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:03:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:03:01,126 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:03:01,663 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:03:01,679 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:03:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:03:01,681 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:03:01,841 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:03:01,858 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:03:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:03:01,860 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:03:02,164 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:03:02,180 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:03:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:03:02,182 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:03:02,491 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:03:02,506 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:03:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:03:02,507 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:03:02,888 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:03:02,903 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:03:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:03:02,905 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:03:03,404 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:03:03 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:03:03,406 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:03:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:03:03,412 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:03:03,574 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:03:03,596 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:03:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:03:03,598 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:03:04,119 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:03:04,134 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:03:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:03:04,136 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:03:04,295 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:03:04,311 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:03:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:03:04,312 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:03:04,483 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:03:04,498 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:03:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:03:04,500 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:16,705 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:16,721 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:05:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:05:16,723 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:17,231 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:17,248 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:05:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:05:17,250 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:17,743 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:17 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:17,745 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:05:17,754 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:05:17,934 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:17,951 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:05:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:05:17,953 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:18,327 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:05:18,343 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:05:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:05:18,344 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:05:18,510 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:18,522 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:05:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:05:18,523 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:18,869 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:18,885 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:05:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:05:18,887 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:05:19,051 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:19,069 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:05:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:05:19,071 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:19,518 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:19,533 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:05:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:05:19,534 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:05:19,707 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:19,722 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:05:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:05:19,724 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:19,878 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:19,893 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:05:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:05:19,895 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:20,235 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:20,282 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:05:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:05:20,283 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:20,486 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:20,500 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:05:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:05:20,502 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:23,195 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:23 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:23,198 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:23,201 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:05:23,203 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:05:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:23,800 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:24,302 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:24 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:24,306 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:24,310 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:24,897 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:24 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:24,901 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:24,907 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:25,526 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:25 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:25,531 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:25,534 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:25,987 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:25 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:25,990 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:25,992 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:26,453 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:26 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:26,457 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:26,463 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:26,986 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:26 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:26,988 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:26,990 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:27,477 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:27 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:27,481 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:27,485 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:28,094 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:28 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:28,097 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:28,101 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:28,605 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:28 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:28,610 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:28,613 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:29,116 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:29 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:29,119 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:29,122 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:29,631 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:29 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:29,640 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:29,644 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:29,821 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:29,837 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:29,838 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:30,446 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:30,463 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:30,467 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:30,964 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:30 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:30,967 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:30,971 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:31,148 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:31,168 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:31,170 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:31,649 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:31,664 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:31,666 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:32,088 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:32 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:32,090 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:32,093 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:32,253 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:32,264 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:32,265 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:32,743 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:32,759 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:32,760 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:33,216 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:33 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:33,219 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:33,223 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:33,535 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:33,550 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:33,552 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:34,139 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:34,160 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:34,161 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:34,585 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:34 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:34,587 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:34,592 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:34,769 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:34,806 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:34,808 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:35,296 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:35,311 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:35,313 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:35,779 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:35 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:35,781 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:35,784 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:35,959 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:35,976 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:35,977 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:36,455 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:36,475 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:36,477 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:36,986 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:36 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:36,989 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:36,994 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:37,164 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:37,180 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:37,181 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:37,756 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:37,773 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:37,775 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:38,348 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:38 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:38,352 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:38,356 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:38,552 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:38,568 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:38,570 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:39,037 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:39,049 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:39,051 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:39,550 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:39 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:39,554 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:39,558 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:39,721 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:39,736 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:39,738 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:40,366 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:40,382 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:40,383 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:40,882 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:40 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:40,886 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:40,889 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:41,058 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:41,075 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:41,076 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:41,547 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:41,563 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:41,564 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:41,948 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:41 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:41,973 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:41,979 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:42,146 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:42,157 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:42,158 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:42,588 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:42,605 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:42,607 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:43,099 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:43 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:43,102 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:43,105 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:43,291 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:43,360 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:43,362 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:43,877 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:43,895 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:43,897 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:44,367 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:44 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:44,371 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:44,375 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:44,538 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:44,554 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:44,555 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:44,985 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:45,009 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:45,010 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:45,493 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:45 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:45,497 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:45,500 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:45,665 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:45,678 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:45,680 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:46,210 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:46,224 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:46,226 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:46,723 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:46 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:46,726 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:46,729 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:46,904 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:46,919 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:46,921 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:47,470 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:47,486 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:47,488 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:47,951 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:47 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:47,955 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:47,959 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:48,137 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:48,150 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:48,151 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:48,775 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:48,794 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:48,795 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:49,183 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:49 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:49,186 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:49,189 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:49,363 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:49,379 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:49,380 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:49,789 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:49,804 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:49,806 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:49,981 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:50,000 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:05:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:05:50,002 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:50,174 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:50,189 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:05:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:05:50,191 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:05:50,344 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:50,359 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:05:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:05:50,361 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:50,658 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:50,671 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:05:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:05:50,673 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:51,227 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:51 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:51,236 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:51,239 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:51,641 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:51,656 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:51,657 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:52,128 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:52,141 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:52,142 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:52,297 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:52,312 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:05:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:05:52,314 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:52,477 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:52,501 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:05:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:05:52,503 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:05:52,658 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:52,672 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:05:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:05:52,673 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:53,063 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:53,079 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:05:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:05:53,080 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:53,455 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:53 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:53,460 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:53,464 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:53,645 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:53,666 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:53,668 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:54,186 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:54,199 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:54,201 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:54,366 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:54,383 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:05:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:05:54,384 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:54,539 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:54,555 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:05:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:05:54,556 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:05:54,717 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:54,732 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:05:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:05:54,733 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:55,181 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:55,199 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:05:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:05:55,200 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:55,577 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:55 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:55,579 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:55,582 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:55,757 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:55,772 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:55,774 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:56,355 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:56,374 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:56,378 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:56,535 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:56,552 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:05:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:05:56,554 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:56,723 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:56,738 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:05:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:05:56,740 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:05:56,897 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:56,913 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:05:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:05:56,914 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:57,260 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:57,273 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:05:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:05:57,275 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:57,643 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:57 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:57,646 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:57,650 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:57,816 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:57,831 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:57,833 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:58,301 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:58,317 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:05:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:58,319 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:05:58,484 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:58,528 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:05:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:05:58,530 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:05:58,699 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:58,715 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:05:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:05:58,717 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:05:58,868 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:58,884 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:05:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:05:58,885 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:59,213 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:05:59,232 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:05:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:05:59,234 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:05:59,633 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:59 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:05:59,636 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:05:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:59,639 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:05:59,808 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:05:59,823 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:05:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:05:59,825 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:00,349 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:00,370 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:00,371 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:00,547 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:00,561 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:06:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:00,562 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:00,752 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:00,770 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:06:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:00,771 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:00,955 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:00,971 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:06:00,972 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:01,377 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:01,394 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:06:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:06:01,396 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:01,890 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:01 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:01,893 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:01,896 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:02,083 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:02,099 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:02,104 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:02,605 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:02,625 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:02,627 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:02,821 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:02,837 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:06:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:02,839 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:03,013 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:03,035 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:06:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:03,037 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:03,200 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:03,215 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:06:03,216 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:03,543 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:03,566 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:06:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:06:03,568 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:03,951 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:03 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:03,953 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:03,958 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:04,123 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:04,143 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:04,144 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:04,553 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:04,569 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:04,570 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:04,733 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:04,752 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:06:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:04,754 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:04,918 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:04,934 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:06:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:04,936 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:05,094 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:05,108 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:06:05,109 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:05,517 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:05,530 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:06:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:06:05,533 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:05,972 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:05 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:05,991 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:06,021 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:06,202 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:06,214 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:06,216 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:06,613 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:06,627 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:06,628 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:06,784 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:06,797 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:06:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:06,798 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:06,949 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:06,975 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:06:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:06,977 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:07,136 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:07,154 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:06:07,155 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:07,548 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:07,566 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:06:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:06:07,569 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:07,975 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:07 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:07,978 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:06:07,990 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:06:08,171 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:08,189 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:06:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:06:08,191 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:08,519 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:06:08,541 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:06:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:06:08,545 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:06:08,733 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:08,784 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:06:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:06:08,788 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:09,146 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:09,162 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:06:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:06:09,164 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:06:09,348 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:09,384 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:06:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:06:09,388 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:06:09,861 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:09,885 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:06:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:06:09,894 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:10,238 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:10,276 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:06:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:10,279 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:10,513 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:10,547 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:06:10,551 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:10,910 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:10,926 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:06:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:06:10,927 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:06:11,088 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:11,104 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:06:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:06:11,106 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:13,356 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:13 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:13,360 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:13,362 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:06:13,365 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:06:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:14,090 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:14,577 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:14,581 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:14,586 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:15,054 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:15,058 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:15,061 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:15,804 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:15,808 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:15,814 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:16,316 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:16 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:16,319 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:16,322 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:16,829 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:16 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:16,834 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:16,837 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:17,342 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:17 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:17,346 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:17,349 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:17,856 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:17 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:17,859 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:17,864 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:18,368 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:18 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:18,371 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:18,375 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:18,982 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:18 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:18,985 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:18,988 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:19,373 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:19 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:19,376 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:19,381 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:19,829 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:19 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:19,840 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:19,844 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:20,011 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:20,026 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:20,027 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:20,478 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:20,493 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:20,495 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:20,934 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:20 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:20,939 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:20,942 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:21,159 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:21,181 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:21,183 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:21,635 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:21,651 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:21,652 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:22,044 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:22 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:22,048 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:22,052 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:22,217 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:22,232 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:22,233 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:22,707 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:22,724 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:22,730 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:23,138 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:23 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:23,145 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:23,162 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:23,357 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:23,373 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:23,374 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:23,838 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:23,855 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:23,859 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:24,310 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:24 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:24,315 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:24,320 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:24,490 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:24,513 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:24,515 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:24,954 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:24,971 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:24,972 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:25,435 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:25 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:25,438 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:25,442 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:25,656 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:25,673 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:25,675 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:26,153 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:26,186 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:26,188 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:26,699 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:26 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:26,703 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:26,707 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:26,878 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:26,893 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:26,895 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:27,383 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:27,414 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:27,419 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:27,894 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:27 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:27,896 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:27,901 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:28,145 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:28,161 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:28,162 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:28,563 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:28,580 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:28,581 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:29,024 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:29 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:29,031 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:29,038 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:29,217 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:29,234 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:29,235 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:29,740 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:29,759 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:29,760 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:30,149 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:30 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:30,156 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:30,163 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:30,344 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:30,360 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:30,361 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:30,803 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:30,820 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:30,822 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:31,278 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:31 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:31,280 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:31,289 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:31,465 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:31,480 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:31,482 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:32,099 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:32,121 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:32,123 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:32,712 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:32 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:32,715 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:32,719 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:32,879 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:32,895 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:32,897 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:33,422 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:33,440 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:33,441 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:33,801 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:33 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:33,804 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:33,808 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:33,976 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:33,992 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:33,994 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:34,455 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:34,472 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:34,473 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:34,848 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:34 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:34,859 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:34,863 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:35,034 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:35,048 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:35,050 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:35,471 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:35,488 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:35,491 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:35,923 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:35 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:35,928 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:35,931 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:36,089 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:36,109 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:36,110 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:36,584 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:36,600 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:36,602 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:37,200 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:37 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:37,203 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:37,207 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:37,368 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:37,384 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:37,386 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:37,789 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:37,814 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:37,816 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:38,324 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:38 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:38,326 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:38,335 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:38,689 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:38,722 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:38,728 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:39,233 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:39,250 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:39,252 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:39,414 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:39,431 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:06:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:39,433 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:39,597 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:39,613 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:06:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:39,615 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:39,770 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:39,785 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:06:39,786 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:40,145 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:40,162 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:06:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:06:40,163 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:40,684 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:40 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:40,686 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:40,689 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:40,850 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:40,870 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:40,873 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:41,403 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:41,425 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:41,427 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:41,614 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:41,631 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:06:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:41,633 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:41,809 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:41,825 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:06:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:41,826 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:41,990 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:42,001 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:06:42,002 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:42,427 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:42,442 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:06:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:06:42,443 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:42,864 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:42 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:42,868 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:42,870 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:43,032 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:43,047 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:43,049 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:43,465 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:43,498 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:43,519 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:43,700 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:43,717 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:43,720 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:43,896 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:43,911 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:43,912 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:44,076 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:44,093 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:06:44,095 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:44,458 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:44,484 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:06:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:06:44,486 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:44,989 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:44 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:44,992 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:06:45,001 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:06:45,164 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:45,179 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:06:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:06:45,181 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:45,462 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:06:45,481 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:06:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:06:45,483 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:06:45,656 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:45,676 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:06:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:06:45,679 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:06:46,117 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:46,131 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:06:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:06:46,133 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:06:46,279 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:46,294 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:06:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:06:46,296 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:46,628 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:46,643 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:06:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:06:46,644 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:06:46,805 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:46,822 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:06:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:46,826 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:46,983 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:47,000 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:06:47,002 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:47,278 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:47,294 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:06:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:06:47,296 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:47,594 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:47,610 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:06:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:06:47,611 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:51,134 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:51 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:51,138 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:51,143 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:06:51,150 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:06:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:52,171 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:52,420 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:52,434 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:52,437 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:52,876 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:52,892 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:52,894 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:53,055 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:53,071 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:06:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:53,072 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:53,242 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:53,259 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:06:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:53,260 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:53,415 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:53,430 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:06:53,432 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:53,777 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:53,793 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:06:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:06:53,795 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:54,172 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:54 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:54,175 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:54,180 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:54,345 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:54,367 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:54,369 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:54,701 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:54,715 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:54,716 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:54,879 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:54,894 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:06:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:54,896 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:55,080 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:55,104 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:06:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:55,106 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:55,260 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:55,280 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:06:55,281 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:55,750 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:55,765 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:06:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:06:55,767 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:56,262 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:56 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:56,265 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:56,269 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:56,440 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:56,454 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:56,456 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:56,898 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:56,914 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:56,915 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:57,090 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:57,102 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:06:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:57,103 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:57,260 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:57,276 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:06:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:57,278 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:57,437 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:57,453 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:06:57,454 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:57,798 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:57,812 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:06:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:06:57,819 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:58,310 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:06:58 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:06:58,314 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:06:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:58,318 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:06:58,477 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:58,493 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:06:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:06:58,495 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:59,028 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:59,044 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:06:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:59,045 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:06:59,205 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:59,221 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:06:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:06:59,223 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:59,382 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:06:59,414 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:06:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:06:59,417 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:06:59,615 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:06:59,629 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:06:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:06:59,631 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:07:00,004 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:07:00,022 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:07:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:07:00,027 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:07:00,427 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:07:00 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:07:00,431 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:07:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:07:00,435 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:07:00,600 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:07:00,620 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:07:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:07:00,621 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:07:01,101 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:07:01,116 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:07:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:07:01,118 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:07:01,280 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:07:01,294 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:07:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:07:01,295 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:07:01,469 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:07:01,490 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:07:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:07:01,493 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:07:01,657 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:07:01,679 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:07:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:07:01,682 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:07:01,985 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:07:02,000 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:07:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:07:02,001 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:07:02,463 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:07:02 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:07:02,467 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:07:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:07:02,471 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:07:02,645 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:07:02,658 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:07:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:07:02,660 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:07:03,181 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:07:03,198 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:07:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:07:03,204 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:07:03,370 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:07:03,386 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:07:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:07:03,387 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:07:03,548 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:07:03,563 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:07:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:07:03,565 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:07:03,729 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:07:03,744 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:07:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:07:03,745 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:07:04,089 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:07:04,107 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:07:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:07:04,109 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:07:04,667 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:07:04 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:07:04,670 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:07:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:07:04,673 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:07:04,841 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:07:04,856 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:07:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:07:04,858 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:07:05,331 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:07:05,347 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:07:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:07:05,348 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:07:05,515 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:07:05,530 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:07:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:07:05,531 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:07:05,701 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:07:05,720 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:07:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:07:05,722 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:07:05,896 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:07:05,912 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:07:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:07:05,914 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:18,468 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:18,488 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:09:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:09:18,490 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:19,006 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:19 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:19,010 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:19,013 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:19,507 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:19 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:19,512 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:19,518 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:19,961 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:19 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:19,965 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:19,968 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:20,504 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:20 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:20,506 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:20,510 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:21,157 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:21 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:21,160 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:21,163 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:21,607 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:21 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:21,611 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:21,613 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:22,080 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:22 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:22,084 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:22,087 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:22,549 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:22 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:22,559 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:22,563 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:22,998 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:23 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:23,001 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:23,004 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:23,453 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:23 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:23,456 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:23,459 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:23,871 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:23 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:23,873 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:23,876 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:24,288 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:24 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:24,291 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:24,296 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:24,479 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:24,497 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:24,499 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:24,997 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:25,012 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:25,014 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:25,555 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:25 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:25,559 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:25,564 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:25,723 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:25,739 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:25,741 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:26,282 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:26,299 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:26,301 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:26,685 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:26 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:26,687 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:26,692 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:26,847 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:26,861 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:26,863 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:27,510 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:27,528 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:27,530 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:28,023 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:28 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:28,026 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:28,028 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:28,188 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:28,210 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:28,212 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:28,718 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:28,734 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:28,735 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:29,163 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:29 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:29,167 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:29,170 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:29,329 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:29,345 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:29,347 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:29,784 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:29,808 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:29,811 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:30,380 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:30 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:30,383 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:30,387 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:30,563 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:30,578 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:30,580 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:31,096 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:31,111 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:31,113 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:31,494 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:31 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:31,498 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:31,502 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:31,677 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:31,694 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:31,696 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:32,326 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:32,342 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:32,344 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:32,940 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:32 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:32,943 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:32,948 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:33,109 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:33,123 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:33,125 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:33,617 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:33,635 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:33,636 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:34,167 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:34 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:34,172 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:34,175 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:34,339 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:34,353 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:34,355 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:34,913 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:34,931 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:34,932 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:35,300 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:35 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:35,309 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:35,313 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:35,483 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:35,498 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:35,499 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:36,026 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:36,041 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:36,043 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:36,509 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:36 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:36,513 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:36,518 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:36,688 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:36,709 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:36,711 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:37,224 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:37,241 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:37,243 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:37,737 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:37 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:37,739 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:37,742 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:37,900 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:37,915 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:37,917 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:38,658 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:38,700 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:38,701 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:39,105 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:39 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:39,110 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:39,114 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:39,279 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:39,296 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:39,298 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:39,782 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:39,797 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:39,799 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:40,190 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:40 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:40,193 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:40,197 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:40,378 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:40,390 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:40,392 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:41,063 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:41,080 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:41,081 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:41,521 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:41 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:41,524 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:41,527 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:41,710 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:41,724 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:41,725 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:42,199 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:42,214 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:42,216 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:42,679 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:42 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:42,683 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:42,686 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:42,847 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:42,863 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:42,865 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:43,474 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:43,491 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:43,493 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:43,877 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:43 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:43,881 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:43,890 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:44,070 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:44,083 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:44,085 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:44,539 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:44,555 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:44,556 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:44,716 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:44,731 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:09:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:09:44,733 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:44,893 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:44,914 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:09:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:44,918 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:45,078 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:45,094 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:09:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:09:45,095 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:45,368 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:45,383 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:09:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:09:45,385 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:45,831 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:45 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:45,834 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:45,837 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:46,147 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:46,167 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:46,170 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:46,668 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:46,685 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:46,686 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:46,853 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:46,869 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:09:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:09:46,871 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:47,030 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:47,046 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:09:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:47,047 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:47,341 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:47,361 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:09:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:09:47,363 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:47,778 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:47,791 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:09:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:09:47,793 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:48,187 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:48 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:48,190 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:48,194 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:48,371 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:48,385 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:48,387 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:48,801 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:48,818 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:48,820 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:48,983 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:49,001 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:09:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:09:49,003 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:49,173 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:49,188 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:09:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:49,190 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:49,347 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:49,363 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:09:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:09:49,365 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:49,830 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:49,846 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:09:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:09:49,848 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:50,327 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:50 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:50,330 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:50,332 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:50,494 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:50,509 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:50,511 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:50,981 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:50,999 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:51,001 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:51,179 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:51,192 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:09:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:09:51,193 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:09:51,363 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:51,379 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:09:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:51,382 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:51,542 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:51,556 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:09:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:09:51,558 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:51,979 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:51,995 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:09:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:09:51,997 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:52,393 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:52 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:52,396 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:52,399 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:52,563 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:52,582 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:52,585 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:53,039 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:53,052 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:53,054 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:53,215 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:53,230 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:09:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:09:53,232 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:53,388 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:53,403 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:09:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:53,405 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:53,568 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:53,582 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:09:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:09:53,584 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:53,929 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:53,944 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:09:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:09:53,946 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:54,367 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:54 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:54,370 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:54,374 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:54,533 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:54,548 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:54,549 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:55,157 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:55,194 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:55,195 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:55,479 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:55,494 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:09:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:09:55,496 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:09:55,671 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:55,685 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:09:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:55,686 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:55,837 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:55,855 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:09:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:09:55,857 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:56,285 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:56,301 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:09:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:09:56,303 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:56,692 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:56 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:56,694 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:56,697 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:56,852 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:56,867 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:56,868 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:57,307 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:57,321 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:57,322 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:57,484 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:57,506 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:09:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:09:57,508 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:57,664 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:57,679 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:09:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:57,680 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:57,829 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:57,843 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:09:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:09:57,845 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:58,230 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:58,243 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:09:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:09:58,245 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:58,643 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:09:58 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:09:58,653 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:09:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:58,656 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:09:58,819 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:58,834 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:09:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:09:58,835 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:59,358 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:59,373 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:09:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:59,375 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:09:59,533 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:59,548 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:09:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:09:59,550 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:09:59,714 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:09:59,730 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:09:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:59,733 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:09:59,889 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:09:59,903 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:09:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:09:59,905 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:00,220 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:00,235 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:10:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:10:00,237 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:00,637 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:00 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:00,641 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:00,645 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:00,997 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:01,010 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:01,017 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:01,612 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:01,630 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:01,632 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:01,799 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:01,816 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:10:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:01,817 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:02,020 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:02,036 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:10:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:02,038 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:02,220 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:02,240 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:10:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:10:02,242 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:10:02,639 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:02,656 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:10:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:10:02,658 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:03,056 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:03 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:03,059 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:03,062 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:03,247 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:03,259 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:03,260 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:03,969 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:03,985 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:03,986 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:04,163 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:04,177 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:10:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:04,178 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:04,358 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:04,373 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:10:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:04,375 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:04,538 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:04,560 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:10:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:10:04,563 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:04,939 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:04,955 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:10:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:10:04,956 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:05,380 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:05 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:05,384 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:05,389 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:05,556 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:05,574 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:05,576 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:06,073 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:06,087 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:06,089 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:06,258 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:06,274 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:10:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:06,276 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:06,465 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:06,480 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:10:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:06,481 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:06,647 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:06,663 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:10:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:10:06,665 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:06,994 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:07,010 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:10:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:10:07,011 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:07,432 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:07 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:07,435 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:07,438 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:07,605 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:07,623 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:07,626 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:08,252 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:08,269 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:08,271 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:08,443 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:08,511 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:10:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:08,516 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:08,716 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:08,734 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:10:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:08,737 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:08,905 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:08,915 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:10:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:10:08,916 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:09,241 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:09,260 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:10:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:10:09,261 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:09,647 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:09 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:09,652 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:09,655 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:09,810 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:09,827 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:09,828 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:10,319 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:10,362 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:10,363 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:10,533 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:10,553 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:10:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:10,555 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:10,725 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:10,738 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:10:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:10,740 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:10,895 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:10,910 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:10:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:10:10,912 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:11,255 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:11,279 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:10:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:10:11,282 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:11,672 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:11 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:11,676 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:11,681 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:12,122 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:12,235 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:12,239 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:12,731 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:12,735 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:12,738 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:13,130 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:13 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:13,133 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:13,136 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:13,621 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:13 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:13,630 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:13,633 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:14,070 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:14,077 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:14,081 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:14,557 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:14,560 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:14,563 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:14,966 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:14,970 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:14,974 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:15,466 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:15,470 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:15,473 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:15,937 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:15,939 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:15,942 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:16,451 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:16 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:16,454 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:16,458 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:16,961 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:16 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:16,964 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:16,969 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:17,150 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:17,166 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:17,167 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:17,679 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:17,695 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:17,697 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:18,078 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:18 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:18,082 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:18,086 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:18,342 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:18,358 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:18,360 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:18,907 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:18,924 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:18,926 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:19,422 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:19 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:19,427 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:19,431 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:19,756 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:19,771 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:19,773 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:20,185 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:20,201 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:20,203 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:20,762 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:20 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:20,767 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:20,772 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:20,930 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:20,945 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:20,946 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:21,468 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:21,483 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:21,485 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:21,839 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:21 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:21,843 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:21,847 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:22,029 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:22,043 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:22,045 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:22,470 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:22,486 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:22,488 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:22,860 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:22 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:22,863 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:22,867 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:23,029 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:23,043 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:23,045 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:23,475 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:23,490 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:23,492 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:23,872 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:23 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:23,876 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:23,879 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:24,039 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:24,055 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:24,057 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:24,469 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:24,483 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:24,485 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:25,055 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:25 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:25,062 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:25,064 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:25,232 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:25,247 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:25,249 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:25,684 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:25,706 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:25,708 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:26,185 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:26 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:26,190 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:26,194 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:26,354 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:26,378 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:26,380 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:26,836 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:26,851 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:26,853 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:27,312 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:27 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:27,315 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:27,320 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:27,480 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:27,497 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:27,498 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:28,029 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:28,044 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:28,046 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:28,424 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:28 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:28,427 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:28,431 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:28,598 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:28,614 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:28,617 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:29,023 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:29,042 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:29,044 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:29,428 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:29 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:29,432 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:29,436 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:29,606 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:29,629 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:29,631 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:30,078 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:30,095 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:30,096 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:30,537 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:30 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:30,543 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:30,546 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:30,857 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:30,874 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:30,875 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:31,410 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:31,455 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:31,457 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:31,920 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:31 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:31,923 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:31,926 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:32,094 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:32,110 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:32,112 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:32,844 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:32,860 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:32,862 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:33,410 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:33 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:33,414 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:10:33,431 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:10:33,596 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:33,616 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:10:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:10:33,618 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:34,000 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:10:34,018 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:10:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:10:34,020 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:10:34,174 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:34,188 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:10:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:10:34,190 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:34,499 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:34,514 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:10:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:10:34,515 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:10:34,671 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:34,685 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:10:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:10:34,686 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:35,017 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:35,043 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:10:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:10:35,045 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:10:35,212 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:35,231 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:10:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:35,233 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:35,418 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:35,437 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:10:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:10:35,440 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:35,783 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:35,798 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:10:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:10:35,799 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:10:35,956 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:35,978 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:10:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:10:35,980 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:38,529 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:38 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:38,534 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:38,537 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:10:38,539 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:10:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:39,484 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:39,664 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:39,678 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:39,680 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:40,166 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:40,181 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:40,182 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:40,635 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:40 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:40,638 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:40,642 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:40,824 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:40,847 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:40,848 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:41,369 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:41,390 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:41,392 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:42,162 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:42 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:42,164 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:42,166 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:42,336 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:42,351 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:42,353 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:42,736 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:42,757 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:42,759 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:42,920 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:42,935 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:10:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:42,937 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:43,100 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:43,115 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:10:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:43,116 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:43,271 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:43,286 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:10:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:10:43,288 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:43,637 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:43,653 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:10:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:10:43,655 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:44,097 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:44 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:44,099 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:44,102 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:44,270 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:44,287 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:44,289 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:44,822 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:44,838 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:44,839 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:45,009 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:45,036 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:10:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:45,039 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:45,207 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:45,218 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:10:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:45,220 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:45,388 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:45,404 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:10:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:10:45,405 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:45,807 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:45,824 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:10:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:10:45,826 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:46,251 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:46 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:46,255 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:46,258 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:46,428 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:46,444 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:46,445 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:47,070 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:47,087 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:47,089 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:47,254 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:47,269 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:10:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:47,270 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:47,437 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:47,457 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:10:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:47,463 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:47,632 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:47,648 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:10:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:10:47,649 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:10:47,991 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:48,006 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:10:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:10:48,008 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:48,476 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:48 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:48,479 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:48,484 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:48,658 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:48,670 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:48,675 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:49,097 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:49,110 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:49,111 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:49,277 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:49,291 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:10:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:49,293 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:49,468 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:49,483 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:10:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:49,485 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:49,644 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:49,658 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:10:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:10:49,662 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:50,040 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:50,055 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:10:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:10:50,058 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:50,480 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:50 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:50,484 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:10:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:10:50,494 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:10:50,678 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:50,695 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:10:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:10:50,697 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:51,043 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:10:51,061 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:10:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:10:51,064 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:10:51,219 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:51,237 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:10:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:10:51,238 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:51,547 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:51,563 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:10:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:10:51,564 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:10:51,711 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:51,726 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:10:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:10:51,728 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:52,091 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:52,109 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:10:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:10:52,110 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:52,331 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:52,346 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:10:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:52,347 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:52,507 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:52,520 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:10:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:10:52,522 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:52,880 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:52,896 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:10:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:10:52,897 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:10:53,058 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:53,071 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:10:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:10:53,073 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:57,830 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:10:57 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:57,834 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:10:57,836 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:10:57,837 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:10:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:58,484 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:10:58,645 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:58,660 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:10:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:10:58,661 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:59,160 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:59,174 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:10:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:10:59,176 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:59,478 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:59,492 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:10:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:59,494 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:10:59,663 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:10:59,677 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:10:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:10:59,679 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:10:59,833 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:10:59,848 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:10:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:10:59,850 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:00,232 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:00,247 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:00,249 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:00,695 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:00 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:00,699 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:00,702 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:00,869 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:00,895 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:00,896 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:01,415 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:01,430 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:01,432 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:01,589 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:01,604 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:11:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:01,606 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:01,775 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:01,793 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:11:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:01,795 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:01,968 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:01,984 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:11:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:01,989 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:02,441 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:02,458 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:02,460 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:02,954 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:02 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:02,956 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:02,960 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:03,125 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:03,141 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:03,142 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:03,671 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:03,686 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:03,688 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:03,867 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:03,883 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:11:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:03,886 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:04,045 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:04,061 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:11:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:04,062 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:04,217 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:04,231 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:11:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:04,233 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:04,599 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:04,614 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:04,616 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:05,108 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:05 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:05,111 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:05,114 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:05,280 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:05,295 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:05,297 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:05,707 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:05,725 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:05,728 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:05,900 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:05,915 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:11:05 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:05,916 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:06,061 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:06,083 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:11:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:06,085 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:06,241 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:06,254 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:11:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:06,256 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:06,621 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:06,638 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:06 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:06,640 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:07,136 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:07 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:07,138 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:07,142 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:07,351 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:07,372 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:07,373 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:07,840 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:07,855 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:07,856 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:08,014 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:08,025 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:11:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:08,026 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:08,191 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:08,206 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:11:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:08,208 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:08,371 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:08,396 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:11:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:08,398 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:08,760 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:08,788 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:08,791 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:09,215 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:09 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:09,221 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:09,230 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:09,441 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:09,460 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:09,463 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:09,916 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:09,941 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:09,943 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:10,120 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:10,136 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:11:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:10,138 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:10,310 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:10,328 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:11:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:10,329 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:11,511 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:11,527 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:11:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:11,529 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:11,880 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:11,895 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:11,896 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:12,358 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:12,360 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:12,363 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:12,938 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:12,942 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:12,946 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:13,385 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:13 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:13,391 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:13,395 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:13,899 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:13 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:13,903 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:13,907 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:14,353 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:14,357 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:14,359 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:14,922 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:14,925 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:14,928 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:15,334 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:15,338 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:15,341 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:15,752 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:15,758 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:15,764 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:16,334 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:16 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:16,336 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:16,339 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:16,870 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:16 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:16,874 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:16,877 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:17,323 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:17 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:17,327 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:17,331 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:17,895 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:17 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:17,898 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:17,901 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:18,061 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:18,075 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:18,077 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:18,611 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:18,628 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:18 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:18,630 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:19,025 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:19 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:19,027 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:19,031 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:19,205 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:19,220 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:19,222 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:19,647 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:19,666 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:19 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:19,668 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:20,064 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:20 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:20,067 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:20,070 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:20,286 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:20,303 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:20,304 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:20,899 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:20,914 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:20,915 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:21,377 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:21 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:21,380 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:21,382 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:21,555 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:21,571 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:21,573 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:22,096 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:22,114 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:22,115 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:22,607 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:22 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:22,616 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:22,619 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:22,794 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:22,811 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:22,812 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:23,331 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:23,346 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:23,349 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:23,742 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:23 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:23,745 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:23,748 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:23,908 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:23,931 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:23,933 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:24,450 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:24,466 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:24,468 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:24,862 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:24 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:24,865 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:24,868 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:25,026 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:25,040 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:25,042 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:25,578 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:25,593 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:25,595 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:26,091 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:26 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:26,095 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:26,098 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:26,262 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:26,277 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:26,279 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:26,712 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:26,725 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:26,726 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:27,094 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:27 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:27,097 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:11:27,103 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:11:27,278 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:27,290 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:11:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:11:27,291 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:27,627 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:11:27,644 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:11:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:11:27,646 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:27,855 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:27,872 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:11:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:11:27,873 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:28,151 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:28,169 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:11:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:11:28,170 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:11:28,333 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:28,348 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:11:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:11:28,349 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:28,687 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:28,704 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:11:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:11:28,705 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:11:28,876 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:28,891 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:11:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:28,892 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:29,041 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:29,060 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:11:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:11:29,062 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:29,473 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:29,490 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:11:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:11:29,491 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:29,810 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:29,825 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:11:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:11:29,827 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:33,231 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:33 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:33,234 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:33,236 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:11:33,238 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:11:33 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:33,866 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:34,038 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:34,054 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:34,056 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:34,597 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:34,613 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:34 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:34,615 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:35,110 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:35 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:35,113 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:35,116 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:35,286 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:35,307 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:35,309 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:35,832 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:35,848 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:35,849 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:36,316 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:36 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:36,319 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:36,323 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:36,481 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:36,497 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:36,499 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:37,136 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:37,159 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:37,161 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:37,649 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:37 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:37,653 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:37,656 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:37,816 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:37,830 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:37,832 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:38,368 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:38,385 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:38,386 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:38,756 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:38 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:38,761 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:38,764 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:38,944 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:38,961 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:38,962 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:39,358 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:39,375 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:39,376 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:39,922 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:39 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:39,925 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:39,928 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:40,130 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:40,147 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:40,148 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:40,759 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:40,779 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:40,781 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:41,237 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:41 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:41,244 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:41,247 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:41,420 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:41,439 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:41,441 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:41,899 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:41,914 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:41,916 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:42,474 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:42 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:42,488 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:42,493 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:42,672 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:42,685 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:42,687 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:43,183 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:43,200 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:43,253 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:43,697 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:43 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:43,702 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:43,704 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:43,856 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:43,876 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:43,878 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:44,514 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:44,530 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:44,532 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:44,692 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:44,706 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:11:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:44,708 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:44,867 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:44,881 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:11:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:44,884 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:45,050 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:45,065 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:11:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:45,066 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:45,540 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:45,556 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:45,558 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:45,953 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:45 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:45,957 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:45 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:45,960 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:46,133 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:46,148 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:46,150 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:46,668 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:46,683 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:46,690 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:46,859 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:46,877 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:11:46 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:46,879 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:47,045 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:47,061 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:11:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:47,062 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:47,214 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:47,228 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:11:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:47,229 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:47,693 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:47,710 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:47 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:47,711 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:48,207 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:48 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:48,210 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:48,214 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:48,375 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:48,394 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:48 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:48,395 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:49,019 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:49,035 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:49,036 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:49,220 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:49,236 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:11:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:49,238 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:49,396 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:49,419 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:11:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:49,421 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:49,574 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:49,589 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:11:49 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:49,590 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:49,983 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:50,000 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:50,001 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:50,376 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:50 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:50,378 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:50,382 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:50,546 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:50,564 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:50 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:50,565 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:51,039 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:51,055 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:51,056 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:51,206 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:51,222 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:11:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:51,223 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:51,371 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:51,387 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:11:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:51,389 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:51,692 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:51,716 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:11:51 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:51,719 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:52,178 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:52,195 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:52,197 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:52,610 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:52 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:52,613 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:52,616 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:52,780 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:52,795 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:52 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:52,796 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:53,259 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:53,279 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:53,281 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:53,437 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:53,453 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:11:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:53,455 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:53,770 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:53,787 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:11:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:53,789 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:53,940 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:53,954 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:11:53 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:53,955 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:54,241 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:54,257 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:54,259 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:54,663 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:54 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:54,666 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:54,669 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:54,846 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:54,863 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:54 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:54,865 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:55,377 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:55,393 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:55,394 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:55,560 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:55,601 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:11:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:55,603 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:55,768 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:55,783 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:11:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:55,784 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:55,947 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:55,962 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:11:55 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:55,964 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:56,299 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:56,315 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:56,317 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:56,747 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:56 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:56,753 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:56 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:56,774 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:56,988 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:57,012 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:57,015 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:57,487 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:57,506 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:11:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:57,508 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:11:57,689 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:57,719 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:11:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:57,722 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:11:57,913 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:57,959 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:11:57 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:11:57,963 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:58,148 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:58,185 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:11:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:11:58,189 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:58,539 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:11:58,564 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:11:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:11:58,566 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:11:58,959 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:11:58 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:11:58,967 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:11:58 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:58,980 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:11:59,341 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:11:59,431 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:11:59 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:11:59,437 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:00,083 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:00,153 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:00,156 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:00,551 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:00,581 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:12:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:12:00,584 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:00,837 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:00,868 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:12:00 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:12:00,870 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:01,074 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:01,119 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:12:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:12:01,121 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:01,514 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:01,532 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:12:01 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:12:01,535 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:01,984 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:01 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:01,988 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:12:02,015 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:12:02,249 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:02,273 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:12:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:12:02,277 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:02,641 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:12:02,670 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:12:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:12:02,673 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:02,853 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:02,869 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:12:02 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:12:02,872 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:03,207 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:03,221 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:12:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:12:03,222 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:12:03,379 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:03,396 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:12:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:12:03,398 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:03,808 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:03,826 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:12:03 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:12:03,829 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:12:04,014 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:04,032 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:12:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:12:04,036 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:12:04,225 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:04,240 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:12:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:12:04,241 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:04,505 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:04,520 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:12:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:12:04,521 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:12:04,679 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:04,690 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:12:04 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:12:04,691 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:06,395 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:06 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:06,399 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:06,401 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:12:06,404 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:12:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:07,409 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:07,599 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:07,616 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:07 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:07,618 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:08,035 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:08,049 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:08,051 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:08,212 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:08,226 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:12:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:12:08,227 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:08,398 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:08,413 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:12:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:12:08,415 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:08,660 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:08,674 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:12:08 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:12:08,676 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:09,013 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:09,031 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:12:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:12:09,034 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:09,436 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:09 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:09,440 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:09,448 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:09,639 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:09,677 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:09 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:09,679 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:10,148 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:10,160 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:10,161 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:10,328 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:10,353 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-latest raised an Exception.\n",
      "\u001b[92m23:12:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:10,355 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini\n",
      "2025-06-19 23:12:10,534 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:10,549 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-002 raised an Exception.\n",
      "\u001b[92m23:12:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:12:10,551 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-exp-0827; provider = gemini\n",
      "2025-06-19 23:12:10,743 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:10,757 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-flash-exp-0827 raised an Exception.\n",
      "\u001b[92m23:12:10 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n",
      "2025-06-19 23:12:10,759 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-flash-1.5-exp; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:11,175 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:11,187 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-flash-1.5-exp raised an Exception.\n",
      "\u001b[92m23:12:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-06-19 23:12:11,189 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:11,569 - httpx - INFO -- l.1025: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:11 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:11,571 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:11 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:11,575 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:12,202 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:12,205 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:12,208 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:12,782 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:12 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:12,786 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:12 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:12,790 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:13,427 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:13 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:13,432 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:13 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:13,437 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:14,007 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:14 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:14,010 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:14,015 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:12:14,016 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:12:14 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:14,596 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:15,031 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:15 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:15,034 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:12:15,041 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:12:15,204 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:15,216 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:12:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:12:15,217 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:15,645 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:12:15,657 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:12:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:12:15,658 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:12:15,808 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:15,825 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:12:15 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:12:15,828 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:16,166 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:16,178 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:12:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:12:16,179 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:16,480 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:16,492 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:12:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:12:16,493 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:16,762 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:16,774 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:12:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:12:16,775 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:12:16,951 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:16,966 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:12:16 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:12:16,967 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:17,123 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:17,139 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:12:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:12:17,142 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:17,521 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:17,532 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:12:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:12:17,533 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:12:17,688 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:17,700 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:12:17 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:12:17,702 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:19,435 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:19 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:19,441 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:19,443 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:12:19,446 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:12:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:20,232 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:20,627 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:20 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:20,630 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:20 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:20,634 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:21,075 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:21 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:21,078 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:21,081 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:21,514 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:21 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:21,517 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:21 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:21,519 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:21,999 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:22 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:22,002 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:22,004 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:22,515 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:22 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:22,518 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:22 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:22,522 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:23,025 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:23 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:23,030 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:23,033 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:23,201 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:23,220 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:23,222 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:23,677 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:23,691 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:23 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:23,692 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:24,165 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:24 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:24,168 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:24,171 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:24,340 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:24,367 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:24,368 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:24,809 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:24,825 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:24 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:24,827 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:25,281 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:25 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:25,284 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:25,287 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:25,453 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:25,470 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:25,472 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:25,921 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:25,938 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:25 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:25,939 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:26,739 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:26 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:26,743 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:26,746 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:26,915 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:26,931 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:26 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:26,932 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:27,325 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:27,341 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:27,343 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:27,743 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:27 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:27,746 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:27,750 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:27,928 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:27,942 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:27 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:27,944 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:28,470 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:28,486 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:28 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:28,487 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:29,073 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:29 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:29,079 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:29,082 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:29,250 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:29,267 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:29,269 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:29,807 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:29,834 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:29 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:29,836 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:30,344 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:30 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:30,347 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:12:30,355 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1206; provider = gemini\n",
      "2025-06-19 23:12:30,516 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1206:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:30,532 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1206 raised an Exception.\n",
      "\u001b[92m23:12:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n",
      "2025-06-19 23:12:30,533 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1206:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:30,873 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-06-19 23:12:30,888 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1206:free raised an Exception.\n",
      "\u001b[92m23:12:30 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:12:30,889 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1121; provider = gemini\n",
      "2025-06-19 23:12:31,054 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1121:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:31,072 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1121 raised an Exception.\n",
      "\u001b[92m23:12:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n",
      "2025-06-19 23:12:31,074 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1121:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:31,398 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:31,414 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1121:free raised an Exception.\n",
      "\u001b[92m23:12:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:12:31,416 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-exp-1114; provider = gemini\n",
      "2025-06-19 23:12:31,575 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:31,589 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-exp-1114 raised an Exception.\n",
      "\u001b[92m23:12:31 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n",
      "2025-06-19 23:12:31,591 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-exp-1114:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:32,041 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:32,058 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-exp-1114:free raised an Exception.\n",
      "\u001b[92m23:12:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:12:32,059 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-002; provider = gemini\n",
      "2025-06-19 23:12:32,214 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-002:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:32,234 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-002 raised an Exception.\n",
      "\u001b[92m23:12:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n",
      "2025-06-19 23:12:32,237 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-exp-0827; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:32,395 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-exp-0827:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:32,412 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-exp-0827 raised an Exception.\n",
      "\u001b[92m23:12:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n",
      "2025-06-19 23:12:32,413 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= meta-llama/llama-3.1-405b-instruct:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:32,758 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:32,778 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/meta-llama/llama-3.1-405b-instruct:free raised an Exception.\n",
      "\u001b[92m23:12:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:12:32,780 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-pro-001; provider = gemini\n",
      "2025-06-19 23:12:32,937 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-001:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-19 23:12:32,952 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-1.5-pro-001 raised an Exception.\n",
      "\u001b[92m23:12:32 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n",
      "2025-06-19 23:12:32,954 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= mistral-large-2411; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:34,920 - httpx - INFO -- l.1025: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:34 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:34,923 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:34,924 - src.genai_model.genai_model - INFO -- l.106: Found 10 models for model_type medium\n",
      "2025-06-19 23:12:34,927 - src.genai_model.genai_model - INFO -- l.109: List of models included: ['gemini/gemini-2.0-flash-exp', 'openrouter/google/gemini-2.0-flash-exp:free', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-exp-0827', 'openrouter/google/gemini-flash-1.5-exp', 'groq/llama-3.3-70b-versatile', 'groq/llama-3.3-70b-specdec', 'gemini/gemini-1.5-flash-001', 'mistral/mistral-medium']\n",
      "\u001b[92m23:12:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:35,813 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:35,999 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:35,996 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:35 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:35,997 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:36,497 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:36,513 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:36,515 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:36,909 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:36 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:36,912 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:36 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:36,916 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:37,070 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:37,092 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:37,094 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:37,545 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:37,561 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:37 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:37,563 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:37,996 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:38 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:38,008 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:38,024 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:38,202 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:38,216 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:38,218 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:38,605 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:38,634 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:38 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:38,639 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:39,193 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:39 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:39,196 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:39,200 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:39,373 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:39,389 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:39,391 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:39,793 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:39,815 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:39 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:39,818 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:40,322 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:40 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:40,325 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:40,329 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:40,606 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:40,621 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:40 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:40,622 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:41,141 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:41,158 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:41,160 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:41,558 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:41 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:41,561 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:41,564 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:41,775 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:41,788 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:41 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:41,790 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:42,441 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:42,466 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:42,468 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:42,853 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:42 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:42,860 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:42 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:42,867 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:43,063 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:43,080 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:43,082 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:43,600 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:43,618 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:43 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:43,620 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:44,216 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:44 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:44,220 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:12:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:44,228 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini\n",
      "2025-06-19 23:12:44,423 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:44,450 - src.genai_model.genai_model - WARNING -- l.43: Model gemini/gemini-2.0-flash-exp raised an Exception.\n",
      "\u001b[92m23:12:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n",
      "2025-06-19 23:12:44,454 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= google/gemini-2.0-flash-exp:free; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:44,954 - httpx - INFO -- l.1025: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-06-19 23:12:44,975 - src.genai_model.genai_model - WARNING -- l.43: Model openrouter/google/gemini-2.0-flash-exp:free raised an Exception.\n",
      "\u001b[92m23:12:44 - LiteLLM:INFO\u001b[0m: utils.py:2909 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n",
      "2025-06-19 23:12:44,978 - LiteLLM - INFO -- l.2909: \n",
      "LiteLLM completion() model= gemini-1.5-flash-latest; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:12:45,356 - httpx - INFO -- l.1025: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyCh6EtCIrY1eRgbcKTd33vr8Ph36_hp9zs \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:12:45 - LiteLLM:INFO\u001b[0m: utils.py:1085 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-19 23:12:45,361 - LiteLLM - INFO -- l.1085: Wrapper: Completed Call, calling success_handler\n"
     ]
    }
   ],
   "source": [
    "report_str = report.create_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a891dcc-a64b-4985-9446-430611d4fb75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T03:14:18.994330Z",
     "iopub.status.busy": "2025-06-20T03:14:18.991295Z",
     "iopub.status.idle": "2025-06-20T03:14:19.017476Z",
     "shell.execute_reply": "2025-06-20T03:14:19.015021Z",
     "shell.execute_reply.started": "2025-06-20T03:14:18.994283Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"toc\">\n",
       "<ul>\n",
       "<li><a href=\"#competitive-intelligence\">COMPETITIVE INTELLIGENCE</a></li>\n",
       "<li><a href=\"#market-intelligence\">MARKET INTELLIGENCE</a><ul>\n",
       "<li><a href=\"#openai\">OpenAI</a></li>\n",
       "<li><a href=\"#google\">Google</a></li>\n",
       "<li><a href=\"#amazon\">Amazon</a></li>\n",
       "<li><a href=\"#perplexity\">Perplexity</a></li>\n",
       "<li><a href=\"#microsoft\">Microsoft</a></li>\n",
       "<li><a href=\"#mistral-ai\">Mistral AI</a></li>\n",
       "<li><a href=\"#twitter\">Twitter</a></li>\n",
       "<li><a href=\"#nvidia\">NVIDIA</a></li>\n",
       "<li><a href=\"#anthropic\">Anthropic</a></li>\n",
       "<li><a href=\"#meta\">Meta</a></li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><a href=\"#technology-themes\">TECHNOLOGY THEMES</a><ul>\n",
       "<li><a href=\"#agents\">Agents</a></li>\n",
       "<li><a href=\"#model\">Model</a></li>\n",
       "<li><a href=\"#model-training\">Model Training</a></li>\n",
       "<li><a href=\"#funding\">Funding</a></li>\n",
       "<li><a href=\"#legal\">Legal</a></li>\n",
       "<li><a href=\"#multimodal\">Multimodal</a></li>\n",
       "<li><a href=\"#evaluation\">Evaluation</a></li>\n",
       "<li><a href=\"#mldl\">ML&amp;DL</a></li>\n",
       "<li><a href=\"#reasoning-models\">Reasoning models</a></li>\n",
       "<li><a href=\"#robotics\">Robotics</a></li>\n",
       "<li><a href=\"#rai\">RAI</a></li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><a href=\"#parser-report\">PARSER REPORT</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "<h1 id=\"competitive-intelligence\">COMPETITIVE INTELLIGENCE</h1>\n",
       "<p>Nothing to see here!</p>\n",
       "<hr />\n",
       "<h1 id=\"market-intelligence\">MARKET INTELLIGENCE</h1>\n",
       "<h2 id=\"openai\">OpenAI</h2>\n",
       "<p>OpenAI is exploring new revenue streams, including a potential ad-based model for ChatGPT, despite CEO Sam Altman's reservations [1, 8]. The company is also venturing into AI shopping agents, aiming to reach 1 billion users through new AI agents and infrastructure investments, while navigating political complexities and competition from Google and Microsoft [2, 3]. Legal challenges persist, with Elon Musk attempting to block OpenAI's for-profit conversion and potential implications from Google's search monopoly trial [4, 6]. Meanwhile, OpenAI secured a $1.5 billion investment from SoftBank, allowing employees to sell shares [5]. The company faces significant challenges in the AI market, including high costs and competition, and recently cut off access to its Sora video generation platform after a leak [9]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://techcrunch.com/2024/12/02/ads-might-be-coming-to-chatgpt-despite-sam-altman-not-being-a-fan/\">ADS MIGHT BE COMING TO CHATGPT</a></li>\n",
       "<li><a href=\"https://techcrunch.com/2024/12/02/the-race-is-on-to-make-ai-agents-do-your-online-shopping-for-you/\">THE RACE IS ON TO MAKE AI AGENTS DO YOUR ONLINE SHOPPING FOR YOU</a></li>\n",
       "<li><a href=\"https://www.ft.com/content/e91cb018-873c-4388-84c0-46e9f82146b4\">OPENAI TARGETS 1BN USERS IN NEXT PHASE OF GROWTH</a></li>\n",
       "<li><a href=\"https://arstechnica.com/tech-policy/2024/11/google-drags-ai-rivals-into-search-trial-as-judge-entertains-ai-remedies/\">GOOGLE'S PLAN TO KEEP AI OUT OF SEARCH TRIAL REMEDIES ISN'T GOING\n",
       "VERY WELL</a></li>\n",
       "<li><a href=\"https://www.cnbc.com/2024/11/26/openai-gets-1point5-billion-investment-from-softbank-in-tender-offer.html\">OPENAI GETS NEW $1.5 BILLION INVESTMENT FROM SOFTBANK, ALLOWING\n",
       "EMPLOYEES TO SELL SHARES IN A TENDER OFFER</a></li>\n",
       "<li><a href=\"https://www.theverge.com/2024/11/30/24309697/elon-musk-openai-lawsuit-for-profit-transition-preliminary-injunction\">ELON MUSK SEEKS TO BLOCK OPENAI'S FOR-PROFIT CONVERSION AGAIN</a></li>\n",
       "<li><a href=\"https://calpaterson.com/porter.html\">BUILDING LLMS IS PROBABLY NOT GOING BE A BRILLIANT BUSINESS</a></li>\n",
       "<li><a href=\"https://www.pcmag.com/news/chatgpt-with-ads-openai-explores-new-business-model-to-cover-its-high-costs\">CHATGPT WITH ADS? OPENAI EXPLORES NEW BUSINESS MODEL TO COVER ITS\n",
       "HIGH COSTS</a></li>\n",
       "<li><a href=\"https://arstechnica.com/ai/2024/11/openai-is-at-war-with-its-own-sora-video-testers-following-brief-public-leak/\">OPENAI IS AT WAR WITH ITS OWN SORA VIDEO TESTERS FOLLOWING BRIEF\n",
       "PUBLIC LEAK</a></li>\n",
       "</ol>\n",
       "<h2 id=\"google\">Google</h2>\n",
       "<p>OpenAI is aggressively expanding its AI capabilities, aiming to reach 1 billion users through new AI agents and infrastructure investments, while competing with Google and Microsoft and navigating political dynamics [2]. Meanwhile, Google is facing regulatory challenges, with the DOJ proposing remedies to prevent Google from using AI to maintain its search monopoly, which could impact its AI products and potentially involve OpenAI [3]. Additionally, Google is innovating in climate solutions by mapping the ionosphere using Android technology, though it has canceled plans for a Pixel Tablet 2 upgrade [4, 5]. The tech industry is also focusing on developing AI shopping agents to automate online purchases, with companies like OpenAI, Google, and Amazon leading the effort despite operational challenges and privacy concerns [1, 2]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://techcrunch.com/2024/12/02/the-race-is-on-to-make-ai-agents-do-your-online-shopping-for-you/\">THE RACE IS ON TO MAKE AI AGENTS DO YOUR ONLINE SHOPPING FOR YOU</a></li>\n",
       "<li><a href=\"https://www.ft.com/content/e91cb018-873c-4388-84c0-46e9f82146b4\">OPENAI TARGETS 1BN USERS IN NEXT PHASE OF GROWTH</a></li>\n",
       "<li><a href=\"https://arstechnica.com/tech-policy/2024/11/google-drags-ai-rivals-into-search-trial-as-judge-entertains-ai-remedies/\">GOOGLE'S PLAN TO KEEP AI OUT OF SEARCH TRIAL REMEDIES ISN'T GOING\n",
       "VERY WELL</a></li>\n",
       "<li><a href=\"https://research.google/blog/mapping-the-ionosphere-with-the-power-of-android/\">MAPPING THE IONOSPHERE WITH THE POWER OF ANDROID</a></li>\n",
       "<li><a href=\"https://www.androidauthority.com/exclusive-canceled-pixel-tablet-2-3504100/\">EXCLUSIVE: HERE'S WHAT THE CANCELED PIXEL TABLET 2 WOULD HAVE BEEN</a></li>\n",
       "<li><a href=\"https://www.theverge.com/2024/11/27/24302415/doj-google-search-antitrust-remedies-chrome-android\">BREAKING DOWN THE DOJ'S PLAN TO END GOOGLE'S SEARCH MONOPOLY</a></li>\n",
       "</ol>\n",
       "<h2 id=\"amazon\">Amazon</h2>\n",
       "<p><strong>Perplexity AI</strong> is developing AI shopping agents despite operational challenges, competing with OpenAI, Google, and Amazon to automate online purchases and reshape retail, while raising privacy and interaction concerns [1]. Meanwhile, <strong>The Browser Company</strong> announced Dia, an AI-focused browser launching in 2025, capable of completing user tasks like adding items to an Amazon cart [2]. Additionally, <strong>Uber</strong> is under FTC investigation for its subscription practices, as part of a broader probe including Amazon and Adobe, following the 'click-to-cancel' rule and rising consumer complaints about subscriptions [3]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://techcrunch.com/2024/12/02/the-race-is-on-to-make-ai-agents-do-your-online-shopping-for-you/\">THE RACE IS ON TO MAKE AI AGENTS DO YOUR ONLINE SHOPPING FOR YOU</a></li>\n",
       "<li><a href=\"https://techcrunch.com/2024/12/02/the-browser-company-teases-dia-its-new-ai-browser/\">THE BROWSER COMPANY TEASES DIA, ITS NEW AI BROWSER</a></li>\n",
       "<li><a href=\"https://sherwood.news/business/ftc-investigating-uber-subscription-service-cancel/\">THE FTC IS INVESTIGATING UBER OVER ITS SUBSCRIPTION SERVICE</a></li>\n",
       "</ol>\n",
       "<h2 id=\"perplexity\">Perplexity</h2>\n",
       "<p>Perplexity is exploring new avenues in AI, including developing AI shopping agents to automate online purchases, a trend also pursued by competitors like OpenAI, Google, and Amazon, which could transform the retail industry despite operational challenges and privacy concerns [1]. Additionally, Perplexity is considering expanding into hardware, with plans for a simple, affordable AI device for voice-to-voice interactions, following a broader trend among AI startups [2]. While this venture holds potential, it also comes with risks, as historically, AI hardware has faced numerous challenges [2]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://techcrunch.com/2024/12/02/the-race-is-on-to-make-ai-agents-do-your-online-shopping-for-you/\">THE RACE IS ON TO MAKE AI AGENTS DO YOUR ONLINE SHOPPING FOR YOU</a></li>\n",
       "<li><a href=\"https://techcrunch.com/2024/11/26/perplexity-mulls-getting-into-hardware/\">PERPLEXITY MULLS GETTING INTO HARDWARE</a></li>\n",
       "</ol>\n",
       "<h2 id=\"microsoft\">Microsoft</h2>\n",
       "<p>OpenAI aims to reach 1 billion users by launching new AI agents, building AI infrastructure, and integrating ChatGPT with Apple devices, while navigating political complexities and competing with Google and Microsoft [1]. Meanwhile, Google faces potential AI restrictions following the search monopoly trial, with the DOJ proposing remedies to prevent Google from using AI to maintain market dominance [2]. Microsoft resists sharing confidential AI deal details, and OpenAI may be compelled to share data amidst these considerations [2]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://www.ft.com/content/e91cb018-873c-4388-84c0-46e9f82146b4\">OPENAI TARGETS 1BN USERS IN NEXT PHASE OF GROWTH</a></li>\n",
       "<li><a href=\"https://arstechnica.com/tech-policy/2024/11/google-drags-ai-rivals-into-search-trial-as-judge-entertains-ai-remedies/\">GOOGLE'S PLAN TO KEEP AI OUT OF SEARCH TRIAL REMEDIES ISN'T GOING\n",
       "VERY WELL</a></li>\n",
       "</ol>\n",
       "<h2 id=\"mistral-ai\">Mistral AI</h2>\n",
       "<p>European AI startup Mistral, known for open-weight LLMs, is expanding to Palo Alto to access Silicon Valley talent and resources [1].</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://www.semafor.com/article/11/27/2024/ai-company-mistral-is-latest-european-startup-to-eye-expansion-in-silicon-valley\">AI COMPANY MISTRAL IS LATEST EUROPEAN STARTUP TO EYE EXPANSION IN\n",
       "SILICON VALLEY</a></li>\n",
       "</ol>\n",
       "<h2 id=\"twitter\">Twitter</h2>\n",
       "<p>Elon Musk faces legal challenges, including his attempt to block OpenAI's for-profit conversion and a failed bid to reinstate his $56 billion Tesla pay package [1, 2]. Meanwhile, Tesla has initiated the rollout of its Full Self-Driving (FSD) V13, with high expectations for its performance [3]. The company's Optimus robot has been upgraded with new hands, potentially compatible with Neuralink's N1 implant, and Tesla is seeking a team to remotely control its 'self-driving' robotaxis [4, 5]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://www.theverge.com/2024/11/30/24309697/elon-musk-openai-lawsuit-for-profit-transition-preliminary-injunction\">ELON MUSK SEEKS TO BLOCK OPENAI'S FOR-PROFIT CONVERSION AGAIN</a></li>\n",
       "<li><a href=\"https://www.cnbc.com/2024/12/02/tesla-ceo-elon-musk-loses-bid-to-get-56-billion-pay-package-reinstated.html\">TESLA CEO ELON MUSK LOSES BID TO GET $56 BILLION PAY PACKAGE\n",
       "REINSTATED</a></li>\n",
       "<li><a href=\"https://www.teslarati.com/tesla-full-self-driving-fsd-v13-rollout-release-notes/\">TESLA FULL SELF-DRIVING (FSD) V13 STARTS INITIAL ROLLOUT</a></li>\n",
       "<li><a href=\"https://www.teslarati.com/tesla-optimus-new-hand-upgrade/\">TESLA OPTIMUS SHOWS OFF NEW UPGRADE</a></li>\n",
       "<li><a href=\"https://gizmodo.com/tesla-is-looking-to-hire-a-team-to-remotely-control-its-self-driving-robotaxis-2000530600\">TESLA IS LOOKING TO HIRE A TEAM TO REMOTELY CONTROL ITS\n",
       "‚ÄòSELF-DRIVING' ROBOTAXIS</a></li>\n",
       "</ol>\n",
       "<h2 id=\"nvidia\">NVIDIA</h2>\n",
       "<p>LLM makers like OpenAI face significant challenges due to industry structure, notably NVIDIA's pricing power as the key chip supplier and high buyer price sensitivity and competition [1]. Many AI companies raise substantial capital but struggle with profitability, similar to historical tech firms like Netscape [1]. Despite this, the technology itself may still advance [1]. AI businesses might succeed by leveraging existing models rather than developing new ones [1].</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://calpaterson.com/porter.html\">BUILDING LLMS IS PROBABLY NOT GOING BE A BRILLIANT BUSINESS</a></li>\n",
       "</ol>\n",
       "<h2 id=\"anthropic\">Anthropic</h2>\n",
       "<p>Anthropic's Claude AI now offers customizable response styles, allowing users to adjust tone and length for different writing tasks [1].</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://www.theverge.com/2024/11/26/24306575/anthropic-claude-ai-custom-style-presets\">ANTHROPIC SAYS CLAUDE AI CAN MATCH YOUR UNIQUE WRITING STYLE</a></li>\n",
       "</ol>\n",
       "<h2 id=\"meta\">Meta</h2>\n",
       "<p>Meta is advancing its infrastructure and AI capabilities despite setbacks . The company is planning a global 40,000+ kilometer fiber-optic subsea cable, potentially a $10 billion project, to support its significant internet traffic share [2]. Meanwhile, Meta's open-source contributions include code for converting GPT models to its Llama framework [1]. However, its ambitious Libra cryptocurrency project was shut down due to political pressure, despite Meta's efforts to address regulatory concerns [3]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/07_gpt_to_llama\">CONVERTING GPT TO LLAMA</a></li>\n",
       "<li><a href=\"https://techcrunch.com/2024/11/29/meta-plans-to-build-a-10b-subsea-cable-spanning-the-world-sources-say/\">META PLANS TO BUILD A $10B SUBSEA CABLE SPANNING THE WORLD, SOURCES\n",
       "SAY</a></li>\n",
       "<li><a href=\"https://threadreaderapp.com/thread/1862654506774810641.html\">HOW LIBRA WAS KILLED</a></li>\n",
       "</ol>\n",
       "<hr />\n",
       "<h1 id=\"technology-themes\">TECHNOLOGY THEMES</h1>\n",
       "<h2 id=\"agents\">Agents</h2>\n",
       "<p>Tech companies are developing AI shopping agents to automate online purchases, with players like OpenAI, Google, and Amazon leading the way, despite operational challenges and privacy concerns [1, 3]. Salesforce CEO Marc Benioff foresees companies creating AI agents for customer service and sales, boosting productivity and reshaping workflows without eliminating jobs [2, 5]. OpenAI aims to reach 1 billion users by launching new AI agents and integrating ChatGPT with Apple devices, while navigating political complexities and competing with rivals [3]. In a notable event, an AI agent, Freysa, was convinced to release its funds after 483 attempts, highlighting the potential and challenges of AI interaction [4]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://techcrunch.com/2024/12/02/the-race-is-on-to-make-ai-agents-do-your-online-shopping-for-you/\">THE RACE IS ON TO MAKE AI AGENTS DO YOUR ONLINE SHOPPING FOR YOU</a></li>\n",
       "<li><a href=\"https://www.bigtechnology.com/p/salesforce-ceo-marc-benioff-has-thoughts\">SALESFORCE CEO MARC BENIOFF HAS THOUGHTS ON AI AGENTS, AUTOMATION,\n",
       "AND THE FUTURE OF YOUR JOB</a></li>\n",
       "<li><a href=\"https://www.ft.com/content/e91cb018-873c-4388-84c0-46e9f82146b4\">OPENAI TARGETS 1BN USERS IN NEXT PHASE OF GROWTH</a></li>\n",
       "<li><a href=\"https://threadreaderapp.com/thread/1862299845710757980.html\">SOMEONE JUST WON $50,000 BY CONVINCING AN AI AGENT TO SEND ALL OF ITS\n",
       "FUNDS TO THEM</a></li>\n",
       "<li><a href=\"https://www.bigtechnology.com/p/salesforce-ceo-marc-benioff-has-thoughts\">SALESFORCE CEO MARC BENIOFF HAS THOUGHTS ON AI AGENTS, AUTOMATION,\n",
       "AND THE FUTURE OF YOUR JOB</a></li>\n",
       "</ol>\n",
       "<h2 id=\"model\">Model</h2>\n",
       "<p><strong>Technological Trends</strong>: New optimizers like Decoupled Momentum Optimization are enhancing language model training, while frameworks such as ProX refine pretraining data programmatically, improving corpus quality [4]. <strong>Challenges</strong> include reward hacking, which hinders model deployment, and industry structural issues that may impact the profitability of AI companies [2, 5]. <strong>Business Shifts</strong>: Inflection AI has moved from developing new AI models to providing enterprise AI tools, highlighting a trend towards practical applications and leveraging existing models [3, 5]. <strong>Product Management</strong> in AI involves close collaboration with data scientists, complex testing, and continuous experimentation for rapid model refinement [6]. <strong>Fundraising &amp; Competition</strong>: Despite significant capital raised, AI companies face challenges due to high buyer price sensitivity, competition, and supplier pricing power [5]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://arxiv.org/abs/2411.19870\">DEMO: DECOUPLED MOMENTUM OPTIMIZATION</a></li>\n",
       "<li><a href=\"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\">REWARD HACKING</a></li>\n",
       "<li><a href=\"https://techcrunch.com/2024/11/26/inflection-ceo-says-its-done-competing-to-make-next-generation-ai-models/\">INFLECTION AI CEO SAYS IT'S DONE TRYING TO MAKE NEXT-GENERATION AI\n",
       "MODELS</a></li>\n",
       "<li><a href=\"https://gair-nlp.github.io/ProX/homepage.html\">REFINING PRETRAINING DATA PROGRAMMATICALLY</a></li>\n",
       "<li><a href=\"https://calpaterson.com/porter.html\">BUILDING LLMS IS PROBABLY NOT GOING BE A BRILLIANT BUSINESS</a></li>\n",
       "<li><a href=\"https://productify.substack.com/p/how-to-experiment-in-the-world-of\">HOW TO EXPERIMENT IN THE WORLD OF AI</a></li>\n",
       "</ol>\n",
       "<h2 id=\"model-training\">Model Training</h2>\n",
       "<p><strong>AI Technology Trends:</strong> Significant advancements include the introduction of Demo, a new optimizer that speeds up language model training by 2.5x with 100x less communication volume, and ProX, a framework that programmatically refines pretraining data, enhancing the quality of pre-training corpora [1, 4]. Additionally, the release of INTELLECT-1, a globally trained 10B parameter model, showcases improvements in decentralized large model training with a notable MFU of 30%+ [3]. <strong>AI Challenges:</strong> Lilian Weng highlights reward hacking as a major obstacle in deploying language models to production [2]. *<em>AI Strategy:</em> * In business strategy, the decision stack and product strategy stack offer different approaches to connecting strategy to actionable goals, with the former focusing on broad objectives and the latter on measurable product goals [5]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://arxiv.org/abs/2411.19870\">DEMO: DECOUPLED MOMENTUM OPTIMIZATION</a></li>\n",
       "<li><a href=\"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\">REWARD HACKING</a></li>\n",
       "<li><a href=\"https://www.primeintellect.ai/blog/intellect-1-release\">INTELLECT-1 RELEASE: THE FIRST GLOBALLY TRAINED 10B PARAMETER MODEL</a></li>\n",
       "<li><a href=\"https://gair-nlp.github.io/ProX/homepage.html\">REFINING PRETRAINING DATA PROGRAMMATICALLY</a></li>\n",
       "<li><a href=\"https://newsletter.herbig.co/posts/344\">PRODUCT STRATEGY STACK VS. DECISION STACK: WHAT'S THE DIFFERENCE</a></li>\n",
       "</ol>\n",
       "<h2 id=\"funding\">Funding</h2>\n",
       "<p>Google faces challenges in keeping AI out of search trial remedies, with Judge Amit Mehta suggesting AI's role in developing remedies and the DOJ proposing restrictions on Google's AI use [1]. Meanwhile, OpenAI secures $1.5 billion from SoftBank, allowing employees to sell shares [2]. Perplexity AI is exploring hardware with a planned affordable AI device for voice interactions [3]. Inflection AI has shifted from developing next-gen AI models to providing enterprise AI tools, acquiring three startups to bolster its offerings [4]. Rox, a new AI-native CRM, raised $50M to disrupt Salesforce with AI-powered agents [5]. PlayAI raised $21M and released a new voice model, Play Dialog [6]. SpaceX is considering a tender offer at a $350 billion valuation, potentially becoming one of the world's largest companies [7]. Additionally, a tutorial on building a distributed log using S3 in under 150 lines of Go code was published, focusing on durability, high availability, and open-source contributions [8]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://arstechnica.com/tech-policy/2024/11/google-drags-ai-rivals-into-search-trial-as-judge-entertains-ai-remedies/\">GOOGLE'S PLAN TO KEEP AI OUT OF SEARCH TRIAL REMEDIES ISN'T GOING\n",
       "VERY WELL</a></li>\n",
       "<li><a href=\"https://www.cnbc.com/2024/11/26/openai-gets-1point5-billion-investment-from-softbank-in-tender-offer.html\">OPENAI GETS NEW $1.5 BILLION INVESTMENT FROM SOFTBANK, ALLOWING\n",
       "EMPLOYEES TO SELL SHARES IN A TENDER OFFER</a></li>\n",
       "<li><a href=\"https://techcrunch.com/2024/11/26/perplexity-mulls-getting-into-hardware/\">PERPLEXITY MULLS GETTING INTO HARDWARE</a></li>\n",
       "<li><a href=\"https://techcrunch.com/2024/11/26/inflection-ceo-says-its-done-competing-to-make-next-generation-ai-models/\">INFLECTION AI CEO SAYS IT'S DONE TRYING TO MAKE NEXT-GENERATION AI\n",
       "MODELS</a></li>\n",
       "<li><a href=\"https://www.notboring.co/p/rox\">ROX</a></li>\n",
       "<li><a href=\"https://blog.play.ai/blog/21m-funding\">PLAYAI RAISES $21M FUNDING AND RELEASES A NEW VOICE MODEL</a></li>\n",
       "<li><a href=\"https://www.bloomberg.com/news/articles/2024-12-02/spacex-discusses-tender-offer-at-roughly-350-billion-valuation?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTczMzIwNTY1OCwiZXhwIjoxNzMzODEwNDU4LCJhcnRpY2xlSWQiOiJTTlZZVENEV1gyUFMwMCIsImJjb25uZWN0SWQiOiJFQTExNDNDNTM4NEE0RUY5QTg5RjJEN0IxMTg2MzcwOSJ9.R6eDk0jJH-FGho8vFKblCL5_2PQdEc6xGMQytDr4ZWg\">SPACEX WEIGHS TENDER OFFER AT ROUGHLY $350 BILLION VALUATION</a></li>\n",
       "<li><a href=\"https://avi.im/blag/2024/s3-log/\">BUILDING A DISTRIBUTED LOG USING S3 (UNDER 150 LINES OF GO)</a></li>\n",
       "</ol>\n",
       "<h2 id=\"legal\">Legal</h2>\n",
       "<p>Elon Musk faces legal challenges with OpenAI and Tesla, having filed a motion to block OpenAI's for-profit conversion due to alleged antitrust violations and financial risks, while also losing an appeal to reinstate his $56 billion Tesla pay package, which was deemed improperly granted [1, 2]. Meanwhile, Musk's net worth has surged by over $43 billion in recent weeks [2]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://www.theverge.com/2024/11/30/24309697/elon-musk-openai-lawsuit-for-profit-transition-preliminary-injunction\">ELON MUSK SEEKS TO BLOCK OPENAI'S FOR-PROFIT CONVERSION AGAIN</a></li>\n",
       "<li><a href=\"https://www.cnbc.com/2024/12/02/tesla-ceo-elon-musk-loses-bid-to-get-56-billion-pay-package-reinstated.html\">TESLA CEO ELON MUSK LOSES BID TO GET $56 BILLION PAY PACKAGE\n",
       "REINSTATED</a></li>\n",
       "</ol>\n",
       "<h2 id=\"multimodal\">Multimodal</h2>\n",
       "<p>Perplexity is eyeing hardware development with plans for a low-cost AI device for voice interactions, joining a broader trend of AI startups venturing into hardware despite historical challenges [1]. Meanwhile, advancements in AI capabilities include a new framework enabling object detection in open-world scenarios and HUPE, an AI method enhancing underwater image clarity [2, 3]. In funding news, PlayAI secured $21 million to advance voice-first AI interfaces and launched Play Dialog, a multi-turn speech model [4]. Additionally, OpenAI faced internal conflict after artists publicly leaked access to its Sora video generation platform, leading to a halt in their testing program [5]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://techcrunch.com/2024/11/26/perplexity-mulls-getting-into-hardware/\">PERPLEXITY MULLS GETTING INTO HARDWARE</a></li>\n",
       "<li><a href=\"https://arxiv.org/abs/2411.18207v1\">DETECT AND LEARN UNSEEN OBJECTS</a></li>\n",
       "<li><a href=\"https://arxiv.org/abs/2411.18296v1\">MAKING UNDERWATER IMAGES CLEAR</a></li>\n",
       "<li><a href=\"https://blog.play.ai/blog/21m-funding\">PLAYAI RAISES $21M FUNDING AND RELEASES A NEW VOICE MODEL</a></li>\n",
       "<li><a href=\"https://arstechnica.com/ai/2024/11/openai-is-at-war-with-its-own-sora-video-testers-following-brief-public-leak/\">OPENAI IS AT WAR WITH ITS OWN SORA VIDEO TESTERS FOLLOWING BRIEF\n",
       "PUBLIC LEAK</a></li>\n",
       "</ol>\n",
       "<h2 id=\"evaluation\">Evaluation</h2>\n",
       "<p><strong>Summary:</strong></p>\n",
       "<p>Advancements in AI include Intellect-1, a 10B parameter model trained globally with impressive efficiency, and OrioleDB Beta7, a storage extension for PostgreSQL [1, 3]. In AI-driven hardware, Near Space Labs has deployed balloon robots to swiftly map disaster areas, aiding various sectors in risk assessment [2]. Meanwhile, AI product management is evolving, with increased collaboration among data scientists and engineers, complex testing, and continuous experimentation for rapid model improvement [4]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://www.primeintellect.ai/blog/intellect-1-release\">INTELLECT-1 RELEASE: THE FIRST GLOBALLY TRAINED 10B PARAMETER MODEL</a></li>\n",
       "<li><a href=\"https://interestingengineering.com/innovation/robot-balloons-capture-nyc-sized-areas\">12-POUND US ROBOT BALLOONS CAPTURE NYC-SIZED AREAS IN JAW-DROPPING\n",
       "DETAILS</a></li>\n",
       "<li><a href=\"https://www.orioledb.com/blog/orioledb-beta7-benchmarks\">ORIOLEDB BETA7: BENCHMARKS</a></li>\n",
       "<li><a href=\"https://productify.substack.com/p/how-to-experiment-in-the-world-of\">HOW TO EXPERIMENT IN THE WORLD OF AI</a></li>\n",
       "</ol>\n",
       "<h2 id=\"mldl\">ML&amp;DL</h2>\n",
       "<p>The AI landscape is seeing notable advancements with Logic Tensor Networks (LTN) emerging as a significant trend, merging deep learning with logical reasoning to enable neural models to learn via optimization of logical formulas [1]. Meanwhile, AI product management is evolving, with increased collaboration between data scientists and ML engineers, complex multivariate testing, and iterative experiments for rapid model refinement becoming standard practices [2]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://arxiv.org/abs/2409.16045v1\">INTRODUCING LTNTORCH</a></li>\n",
       "<li><a href=\"https://productify.substack.com/p/how-to-experiment-in-the-world-of\">HOW TO EXPERIMENT IN THE WORLD OF AI</a></li>\n",
       "</ol>\n",
       "<h2 id=\"reasoning-models\">Reasoning models</h2>\n",
       "<p>Logic Tensor Networks (LTN) merge deep learning with logical reasoning, allowing neural models to learn by optimizing a knowledge base of logical formulas [1].</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://arxiv.org/abs/2409.16045v1\">INTRODUCING LTNTORCH</a></li>\n",
       "</ol>\n",
       "<h2 id=\"robotics\">Robotics</h2>\n",
       "<p>Tesla has initiated the rollout of its Full Self-Driving (FSD) V13, with high expectations for real-world performance following promising demonstrations at the 'We, Robot' event [2]. Meanwhile, Tesla's Optimus robot has been upgraded with new hands, potentially compatible with Neuralink's N1 implant, as part of a new feasibility trial [3]. In other developments, Near Space Labs has deployed a fleet of balloon robots equipped with AI-powered Swifts to capture high-resolution aerial images, aiming to expedite disaster zone mapping and assist various stakeholders in risk assessment and property evaluation [1]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://interestingengineering.com/innovation/robot-balloons-capture-nyc-sized-areas\">12-POUND US ROBOT BALLOONS CAPTURE NYC-SIZED AREAS IN JAW-DROPPING\n",
       "DETAILS</a></li>\n",
       "<li><a href=\"https://www.teslarati.com/tesla-full-self-driving-fsd-v13-rollout-release-notes/\">TESLA FULL SELF-DRIVING (FSD) V13 STARTS INITIAL ROLLOUT</a></li>\n",
       "<li><a href=\"https://www.teslarati.com/tesla-optimus-new-hand-upgrade/\">TESLA OPTIMUS SHOWS OFF NEW UPGRADE</a></li>\n",
       "</ol>\n",
       "<h2 id=\"rai\">RAI</h2>\n",
       "<p><strong>AI Trends and Management Practices</strong></p>\n",
       "<p>Performance improvement plans (PIPs) are increasingly used to manage employee performance despite being unpopular among both workers and managers [1]. PIPs set challenging goals to be completed within a short timeframe, with failure often resulting in termination [1]. Meanwhile, data proficiency remains crucial for product companies, with top firms incorporating systematic, data-driven approaches and AI to enhance decision-making while mitigating biases and selective data usage [2]. (summary provided by mistral/mistral-large-2411 )</p>\n",
       "<p>Sources:</p>\n",
       "<ol>\n",
       "<li><a href=\"https://www.wsj.com/business/firing-someone-performance-improvement-plans-more-popular-the-pip-7cac7062?st=7tqjn5&amp;reflink=desktopwebshare_permalink\">THE MOST HATED WAY OF FIRING SOMEONE IS MORE POPULAR THAN EVER. IT'S\n",
       "THE AGE OF THE PIP</a></li>\n",
       "<li><a href=\"https://itamargilad.com/data-driven/\">4 LEVELS OF DATA PROFICIENCY</a></li>\n",
       "</ol>\n",
       "<hr />\n",
       "<h1 id=\"parser-report\">PARSER REPORT</h1>\n",
       "<p>All emails could be parsed!</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(report_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be16ad08-32a5-4bcd-8966-2117ee64e8d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:55:12.414791Z",
     "iopub.status.busy": "2025-06-20T02:55:12.414441Z",
     "iopub.status.idle": "2025-06-20T02:55:15.218166Z",
     "shell.execute_reply": "2025-06-20T02:55:15.217352Z",
     "shell.execute_reply.started": "2025-06-20T02:55:12.414764Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.genai_model.production_editor import ProductionEditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87b7275-80ea-4f67-9ea0-746192c5e97a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:55:15.219523Z",
     "iopub.status.busy": "2025-06-20T02:55:15.219031Z",
     "iopub.status.idle": "2025-06-20T02:55:15.229704Z",
     "shell.execute_reply": "2025-06-20T02:55:15.228527Z",
     "shell.execute_reply.started": "2025-06-20T02:55:15.219500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenAI is exploring new revenue streams, including a potential ad-based model, to support its costly AI development, although CEO Sam Altman views this as a last resort [1, 8].'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProductionEditor._join_cite(\n",
    "    \"OpenAI is exploring new revenue streams, including a potential ad-based model, to support its costly AI development, although CEO Sam Altman views this as a last resort.\", \n",
    "    [1, 8]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b66c8f-a02b-43ad-a4c5-844b84bab9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42681d2-c80c-4585-b12d-4b9df5311517",
   "metadata": {},
   "source": [
    "# Run through the steps of Report.create_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed379c4-37bb-42af-ae7b-c7477bb76a3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T04:35:01.897813Z",
     "iopub.status.busy": "2025-02-05T04:35:01.896348Z",
     "iopub.status.idle": "2025-02-05T04:35:01.953697Z",
     "shell.execute_reply": "2025-02-05T04:35:01.951848Z",
     "shell.execute_reply.started": "2025-02-05T04:35:01.897750Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['COMPET', 'FUNDING', 'EVALUATION', 'THEMES-OpenAI', 'THEMES-Google', 'THEMES-Agents', 'THEMES-Other'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_stories_for_report = report._filtered_news_stories(min_score_threshold=3, min_nb_entries=5, min_pct_entries=0.1)\n",
    "news_stories_for_report.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea624b5-e3aa-4bc0-9f66-90652ab4b665",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T04:35:01.960595Z",
     "iopub.status.busy": "2025-02-05T04:35:01.960181Z",
     "iopub.status.idle": "2025-02-05T04:35:02.419668Z",
     "shell.execute_reply": "2025-02-05T04:35:02.417979Z",
     "shell.execute_reply.started": "2025-02-05T04:35:01.960565Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Report' object has no attribute '_create_report_from_news_stories'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m report_str \u001b[38;5;241m=\u001b[39m \u001b[43mreport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_report_from_news_stories\u001b[49m(news_stories_for_report)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Report' object has no attribute '_create_report_from_news_stories'"
     ]
    }
   ],
   "source": [
    "report_str = report._create_report_from_news_stories(news_stories_for_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a94caa-4b70-4fad-ae3c-f7c6f90d4101",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T04:35:02.420419Z",
     "iopub.status.idle": "2025-02-05T04:35:02.420842Z",
     "shell.execute_reply": "2025-02-05T04:35:02.420638Z",
     "shell.execute_reply.started": "2025-02-05T04:35:02.420623Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"test.txt\", \"w\") as f:\n",
    "    f.write(report_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bf20f-6bf1-4f28-a71f-0f4525c7bdbd",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T04:35:02.422066Z",
     "iopub.status.idle": "2025-02-05T04:35:02.422528Z",
     "shell.execute_reply": "2025-02-05T04:35:02.422253Z",
     "shell.execute_reply.started": "2025-02-05T04:35:02.422240Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report.df_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa6505-32eb-439d-b8f7-1990efbe8a81",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T04:35:02.423675Z",
     "iopub.status.idle": "2025-02-05T04:35:02.424115Z",
     "shell.execute_reply": "2025-02-05T04:35:02.423948Z",
     "shell.execute_reply.started": "2025-02-05T04:35:02.423934Z"
    }
   },
   "outputs": [],
   "source": [
    "df_selected = report.df_ns[report.df_ns[\"score_category_count\"] >= 2]\n",
    "print(f\"Selected {len(df_selected)} out of {len(df_news_stories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbddb10b-ae6a-4c3b-b3ee-65a83e8d6442",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T04:35:02.426358Z",
     "iopub.status.idle": "2025-02-05T04:35:02.427709Z",
     "shell.execute_reply": "2025-02-05T04:35:02.427098Z",
     "shell.execute_reply.started": "2025-02-05T04:35:02.427004Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89972204-c2ad-4a5b-b52a-722185af6f15",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4f04f-ffc9-43d6-bd92-5d363a24233d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T04:35:02.429280Z",
     "iopub.status.idle": "2025-02-05T04:35:02.429816Z",
     "shell.execute_reply": "2025-02-05T04:35:02.429560Z",
     "shell.execute_reply.started": "2025-02-05T04:35:02.429544Z"
    }
   },
   "outputs": [],
   "source": [
    "report._get_df_from_tag(df=report.df_ns, tag=\"Perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908827e-8b3a-43a6-afb1-5cf1be1e579b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T04:35:02.432578Z",
     "iopub.status.idle": "2025-02-05T04:35:02.433006Z",
     "shell.execute_reply": "2025-02-05T04:35:02.432797Z",
     "shell.execute_reply.started": "2025-02-05T04:35:02.432783Z"
    }
   },
   "outputs": [],
   "source": [
    "report._get_df_from_tag(df=report.df_ns, tag=\"Big Bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4a193-117a-42b3-9a5a-b4a302a21f70",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T04:35:02.436003Z",
     "iopub.status.idle": "2025-02-05T04:35:02.436662Z",
     "shell.execute_reply": "2025-02-05T04:35:02.436434Z",
     "shell.execute_reply.started": "2025-02-05T04:35:02.436392Z"
    }
   },
   "outputs": [],
   "source": [
    "themes_count = report.df_ns[\"themes\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbbaf3b-fd81-4147-8fe3-7729e3146d8a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T04:35:02.437723Z",
     "iopub.status.idle": "2025-02-05T04:35:02.438280Z",
     "shell.execute_reply": "2025-02-05T04:35:02.438022Z",
     "shell.execute_reply.started": "2025-02-05T04:35:02.438006Z"
    }
   },
   "outputs": [],
   "source": [
    "themes_count[:int(len(themes_count)*.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b343f6-cdc1-4f19-85a9-f86c765bfd34",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T04:35:02.440442Z",
     "iopub.status.idle": "2025-02-05T04:35:02.441052Z",
     "shell.execute_reply": "2025-02-05T04:35:02.440894Z",
     "shell.execute_reply.started": "2025-02-05T04:35:02.440878Z"
    }
   },
   "outputs": [],
   "source": [
    "report.df_ns[\"competitive_intelligence\"].explode().dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c4fb6-89e7-4e27-91f3-a1ce63655111",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T04:35:02.443021Z",
     "iopub.status.idle": "2025-02-05T04:35:02.444218Z",
     "shell.execute_reply": "2025-02-05T04:35:02.443860Z",
     "shell.execute_reply.started": "2025-02-05T04:35:02.443710Z"
    }
   },
   "outputs": [],
   "source": [
    "report._get_df_from_tag(report.df_ns, tag=\"Big Bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976a74a-5665-48e0-a5ff-8eaab28090f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
